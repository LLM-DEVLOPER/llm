{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88cbb58646de4eb8809af64fadac2363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63c0fa7865c14421a951f7276bbe8134",
              "IPY_MODEL_82b89cbe049646df817ce09f5d0bf06a",
              "IPY_MODEL_49fae0384f5d4005954e4bad7a4b23b7"
            ],
            "layout": "IPY_MODEL_351e025fc08b46828e13b610a62a44a1"
          }
        },
        "63c0fa7865c14421a951f7276bbe8134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a20944fcfef4381ab7b27ef6eb161fb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f4b9036aba5545b596346a3dcfda6458",
            "value": "README.md:â€‡100%"
          }
        },
        "82b89cbe049646df817ce09f5d0bf06a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_367634585ad24e92986be0ee398339fe",
            "max": 27,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80d077418cdf48bc9de7fe077811538b",
            "value": 27
          }
        },
        "49fae0384f5d4005954e4bad7a4b23b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f63af71d4416475b9831660aa9c59c66",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d6d43d3310fe47f4b8f68f99cad803ce",
            "value": "â€‡27.0/27.0â€‡[00:00&lt;00:00,â€‡2.29kB/s]"
          }
        },
        "351e025fc08b46828e13b610a62a44a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a20944fcfef4381ab7b27ef6eb161fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b9036aba5545b596346a3dcfda6458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "367634585ad24e92986be0ee398339fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d077418cdf48bc9de7fe077811538b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f63af71d4416475b9831660aa9c59c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6d43d3310fe47f4b8f68f99cad803ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59f86f2fa3b940b3a4a687829467126a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b8996e673f148e8a2dbf6a028d3be1a",
              "IPY_MODEL_3cb3823108044a3784d7bf25ba833374",
              "IPY_MODEL_8a5511f15a4b469494f3c7eba9540ae6"
            ],
            "layout": "IPY_MODEL_8c76ba3cebb24d3ba110c78794a6e1da"
          }
        },
        "6b8996e673f148e8a2dbf6a028d3be1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c67452736747dd8db5fae3f53646d8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fd10d4fd2fb3464dab51ac40d8b4b4ca",
            "value": "zhihu_3k_rlfh.tsv:â€‡100%"
          }
        },
        "3cb3823108044a3784d7bf25ba833374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc5129ca5e0b4af0ad73dbbc68429dcf",
            "max": 15557370,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56713605bd7e4ff39f67036e36399cc8",
            "value": 15557370
          }
        },
        "8a5511f15a4b469494f3c7eba9540ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80b2e935174543099a1d09b7e039b31a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b28896743cba454fad6a6a94e9a63e56",
            "value": "â€‡15.6M/15.6Mâ€‡[00:00&lt;00:00,â€‡75.1MB/s]"
          }
        },
        "8c76ba3cebb24d3ba110c78794a6e1da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4c67452736747dd8db5fae3f53646d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd10d4fd2fb3464dab51ac40d8b4b4ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc5129ca5e0b4af0ad73dbbc68429dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56713605bd7e4ff39f67036e36399cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80b2e935174543099a1d09b7e039b31a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b28896743cba454fad6a6a94e9a63e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c571c18ae4df4a1a90f2819fc5d9803f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7e8a60dc39a434a8d97f98580ff201f",
              "IPY_MODEL_98449b50d77043e7bba60e6f2e0d5b18",
              "IPY_MODEL_4931472f477d4573bf8413f2e629303d"
            ],
            "layout": "IPY_MODEL_a512cbdf8201442488b1b603459d3f3c"
          }
        },
        "b7e8a60dc39a434a8d97f98580ff201f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f714606ca0c4bd5ac0642d7defeca87",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_896eb5662e3046529d24b3a1d0d72a38",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "98449b50d77043e7bba60e6f2e0d5b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0ed61da0cd845c288cbdf14c7ee278e",
            "max": 3460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b64256559f241f5ac1075da9237f11b",
            "value": 3460
          }
        },
        "4931472f477d4573bf8413f2e629303d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8318f3eb4d044269003276083e3e9c5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_307f549df91f4f1fbc4c80721d72f6c5",
            "value": "â€‡3460/3460â€‡[00:00&lt;00:00,â€‡8714.74â€‡examples/s]"
          }
        },
        "a512cbdf8201442488b1b603459d3f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f714606ca0c4bd5ac0642d7defeca87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896eb5662e3046529d24b3a1d0d72a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0ed61da0cd845c288cbdf14c7ee278e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b64256559f241f5ac1075da9237f11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8318f3eb4d044269003276083e3e9c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "307f549df91f4f1fbc4c80721d72f6c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# å¾®è°ƒæ¨¡åž‹æ‰‹å†Œ\n",
        "\n",
        "ä»¥è½»é‡çº§çš„ Qwen2.5-0.5B-Instruct æ¨¡åž‹ä¸ºä¾‹ã€‚"
      ],
      "metadata": {
        "id": "FctiHqA-SlAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç¬¬ä¸€éƒ¨åˆ†ï¼šé€šç”¨å‡†å¤‡å·¥ä½œ\n",
        "\n",
        "\n",
        "åœ¨å¼€å§‹ä»»ä½•å¾®è°ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå®ŒæˆçŽ¯å¢ƒå’Œæ•°æ®çš„å‡†å¤‡ã€‚"
      ],
      "metadata": {
        "id": "_KuY0G7xTpA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ç¬¬ 1 æ­¥ï¼šç»Ÿä¸€çŽ¯å¢ƒæ­å»º\n",
        "ä¸€ä¸ªç¨³å®šä¸”éš”ç¦»çš„PythonçŽ¯å¢ƒæ˜¯æˆåŠŸçš„ç¬¬ä¸€æ­¥ã€‚"
      ],
      "metadata": {
        "id": "oeH29lY7T1Qb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä½¿ç”¨ Conda å¯ä»¥åˆ›å»ºä¸€ä¸ªéš”ç¦»çš„ Python çŽ¯å¢ƒï¼Œé˜²æ­¢åŒ…ç‰ˆæœ¬å†²çªã€‚"
      ],
      "metadata": {
        "id": "OZkl-OXQT1Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ä½¿ç”¨ Conda\"\n",
        "# conda create -n finetune-env python=3.12\n",
        "# conda activate finetune-env"
      ],
      "metadata": {
        "id": "RmAlBKD7T7F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ç¬¬ 2 æ­¥ï¼šLLaMA-Factory éƒ¨ç½²ä¸Žä¾èµ–å®‰è£…"
      ],
      "metadata": {
        "id": "9cUn5EisUCiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 è®¾ç½®å·¥ä½œç›®å½•"
      ],
      "metadata": {
        "id": "YB-7vDHoqRu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. è®¾ç½®ä½ è¦ç”¨çš„å­ç›®å½•å\n",
        "project_dir_name = \"Workspace\"\n",
        "\n",
        "# 2. å°è¯•æŒ‚è½½ Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    base_dir = Path(\"/content/drive/MyDrive\")  # äº‘ç›˜æ ¹ç›®å½•\n",
        "    print(\"âœ… æˆåŠŸæŒ‚è½½ Google Drive\")\n",
        "except Exception as e:\n",
        "    base_dir = Path(\"/content\")  # å›žé€€åˆ°æœ¬åœ° Colab ç©ºé—´\n",
        "    print(\"âš ï¸ æ— æ³•æŒ‚è½½ Google Driveï¼Œä½¿ç”¨æœ¬åœ°ç›®å½•\")\n",
        "\n",
        "# 3. è®¾ç½®å·¥ä½œç›®å½•\n",
        "work_dir = base_dir / project_dir_name\n",
        "work_dir.mkdir(parents=True, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰\n",
        "os.chdir(work_dir)\n",
        "print(f\"ðŸ“‚ å½“å‰å·¥ä½œç›®å½•å·²åˆ‡æ¢åˆ°: {work_dir.resolve()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PGAKR9VqUSi",
        "outputId": "3c0d6c30-c394-44fc-b7b8-ec9e049ba798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ æ— æ³•æŒ‚è½½ Google Driveï¼Œä½¿ç”¨æœ¬åœ°ç›®å½•\n",
            "ðŸ“‚ å½“å‰å·¥ä½œç›®å½•å·²åˆ‡æ¢åˆ°: /content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 å…‹éš† LLaMA-Factory ä»“åº“"
      ],
      "metadata": {
        "id": "0Qja7C7AjcJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF6PY3_tUEFE",
        "outputId": "03269646-5f92-4ac5-b348-42b26f9d722f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkrByNhOGvhf",
        "outputId": "74ec64a2-8316-4b70-fa4d-c86bd0e9f022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'LLaMA-Factory'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 å®‰è£…æ ¸å¿ƒä¸Žé«˜æ€§èƒ½ä¾èµ–"
      ],
      "metadata": {
        "id": "Q3YtyV0TUHoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMOUCpkaimz6",
        "outputId": "2aacaaa8-131b-4a46-b09d-505c44a214ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%nvidia-smi` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å¸è½½æ—§ç‰ˆæœ¬åŒ…ï¼ˆé˜²æ­¢å†²çªï¼‰\n",
        "# pip uninstall -y torch torchvision torchaudio flash-attn deepspeed llamafactory tensorflow tensorflow-decision-forests dopamine-rl opencv-python opencv-python-headless opencv-contrib-python thinc\n",
        "\n",
        "# æ ¸å¿ƒåº“å®‰è£… (è¯·æ ¹æ®æ‚¨çš„ CUDA ç‰ˆæœ¬é€‰æ‹©åˆé€‚çš„ PyTorch å®‰è£…å‘½ä»¤)\n",
        "!uv pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "# Hugging Face ç”Ÿæ€ç³»ç»Ÿå’Œå¾®è°ƒå·¥å…·\n",
        "!uv pip install \"transformers>=4.41.0,<5.0.0\"\n",
        "!uv pip install sentence-transformers\n",
        "!uv pip install -q \"accelerate>=0.30.1\"\n",
        "!uv pip install -q \"peft>=0.10.0\"\n",
        "!uv pip install -q \"datasets>=2.19.1\"\n",
        "!uv pip install -q \"deepspeed>=0.14.2\"\n",
        "!uv pip install -q \"sentence-transformers==4.1.0\"\n",
        "\n",
        "# Qwen-VL ç‰¹å®šåº“å’Œæ€§èƒ½ä¼˜åŒ–åº“\n",
        "!uv pip install -q \"bitsandbytes>=0.43.1\"\n",
        "!uv pip install -q \"qwen-vl-utils>=0.0.11\"\n",
        "# !pip install -q \"flash-attn==2.5.8\" --no-build-isolation\n",
        "\n",
        "# å¯è§†åŒ–å’Œç›‘æŽ§å·¥å…·\n",
        "!uv pip install -q \"supervision>=0.20.0\"\n",
        "!uv pip install -q \"tensorboard==2.18\"\n",
        "!uv pip install -q \"wandb>=0.17.0\"\n",
        "!uv pip install -q peft trl\n",
        "\n",
        "\n",
        "# å®‰è£… Flash Attentionï¼ˆå¯èƒ½éœ€è¦ç¼–è¯‘å·¥å…· cmakeã€ninjaï¼‰\n",
        "!uv pip install flash-attn --no-build-isolation\n",
        "\n",
        "# å®‰è£… DeepSpeedï¼ˆä½¿ç”¨é˜¿é‡Œé•œåƒåŠ é€Ÿï¼‰\n",
        "!uv pip install deepspeed -i https://mirrors.aliyun.com/pypi/simple/\n",
        "\n",
        "\n",
        "# å®‰è£…å…¶ä»–ä¾èµ–\n",
        "!uv pip install trl modelscope addict\n",
        "\n",
        "# å¯é€‰ï¼šéªŒè¯ GPU ä¸Ž PyTorch\n",
        "!python -c \"import torch; print(torch.cuda.is_available(), torch.version.cuda, torch.backends.cudnn.version())\"\n",
        "\n",
        "# !pip install -e .[torch,metrics]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bOmWW1lYUJhz",
        "outputId": "b6e68c05-5623-422e-dafd-36e748c08352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping flash-attn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping deepspeed as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping llamafactory as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-decision-forests as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dopamine-rl as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "peft 0.17.0 requires torch>=1.13.0, which is not installed.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
            "accelerate 1.10.0 requires torch>=2.0.0, which is not installed.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4def949152d04bc99fe247cd5800f053"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torch-2.8.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torchvision-0.23.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torchaudio-2.8.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.4.0 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu126/torch-2.8.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (821.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.9/821.9 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached https://download.pytorch.org/whl/cu126/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.whl (8.9 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (897 kB)\n",
            "Using cached https://download.pytorch.org/whl/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "Downloading https://download.pytorch.org/whl/cu126/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchvision-0.23.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchaudio-2.8.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0+cu126 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.13.3 torch-2.8.0+cu126 torchaudio-2.8.0+cu126 torchvision-0.23.0+cu126 triton-3.4.0\n",
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp311-cp311-linux_x86_64.whl size=256043372 sha256=3d41b2fc55753faa7f45d6568ea73a96b96afb48b82994ab9b49bcbcb6c87588\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/31/1f/4b22dd7295b3cb064b8fa9038f6d58fb15c9571555b2d7c39c\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n",
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
            "Collecting deepspeed\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/84/b3/02d000bfd0d6f0ba3f5dac487e4151b3e9b4b3ee71336882102dc9ad79ad/deepspeed-0.17.4.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/1f/7f/13cd798d180af4bf4c0ceddeefba2b864a63c71645abc0308b768d67bb81/hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.1)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/ed/de/0e6edf44d6a04dabd0318a519125ed0415ce437ad5a1ec9b9be03d9048cf/ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch->deepspeed) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.17.4-py3-none-any.whl size=1705588 sha256=582d72088a8edec20fd169bade1f31bfdde2c771ffaac533e6bd1996241ef998\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/63/7d/deeb3ed72a58c02e6de3948fe2cf127aa2f28cbe34586155fe\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.17.4 hjson-3.1.0 ninja-1.13.0\n",
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==3.6.0\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate==1.7.0\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.8.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=2.0.0->accelerate==1.7.0) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.7.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.7.0) (3.0.2)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, datasets, accelerate\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.4\n",
            "    Uninstalling tokenizers-0.21.4:\n",
            "      Successfully uninstalled tokenizers-0.21.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.55.1\n",
            "    Uninstalling transformers-4.55.1:\n",
            "      Successfully uninstalled transformers-4.55.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.10.0\n",
            "    Uninstalling accelerate-1.10.0:\n",
            "      Successfully uninstalled accelerate-1.10.0\n",
            "Successfully installed accelerate-1.7.0 datasets-3.6.0 tokenizers-0.19.1 transformers-4.41.2\n",
            "Collecting trl\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting modelscope\n",
            "  Downloading modelscope-1.29.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.7.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.6.0)\n",
            "Collecting transformers>=4.55.0 (from trl)\n",
            "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from modelscope) (3.18.0)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.11/dist-packages (from modelscope) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from modelscope) (75.2.0)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from modelscope) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from modelscope) (2.5.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->modelscope) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->modelscope) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->modelscope) (2025.8.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.55.0->trl) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.55.0->trl)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=1.4.0->trl) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.7)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
            "Downloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.29.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, modelscope, tokenizers, transformers, trl\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed addict-2.4.0 modelscope-1.29.0 tokenizers-0.21.4 transformers-4.55.2 trl-0.21.0\n",
            "Obtaining file:///content\n",
            "\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 éªŒè¯å®‰è£… LLaMA-Factory\n",
        "\n",
        "è¿è¡Œ LLaMA-Factory çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œæ£€æŸ¥æ˜¯å¦èƒ½æˆåŠŸæ‰“å°ç‰ˆæœ¬å·ã€‚"
      ],
      "metadata": {
        "id": "zGktbtKZUgN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVTjtOsCUmor",
        "outputId": "ca9bcd42-905b-43b6-b6b8-8617f4578ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-16 03:37:15,067] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
            "[2025-08-16 03:37:15,069] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
            "[2025-08-16 03:37:19,988] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
            "----------------------------------------------------------\n",
            "| Welcome to LLaMA Factory, version 0.9.4.dev0           |\n",
            "|                                                        |\n",
            "| Project page: https://github.com/hiyouga/LLaMA-Factory |\n",
            "----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "å¦‚æžœå®‰è£…æˆåŠŸï¼Œåº”è¯¥èƒ½çœ‹åˆ° LLaMA-Factory çš„æ¬¢è¿Žä¿¡æ¯å’Œç‰ˆæœ¬å·ã€‚"
      ],
      "metadata": {
        "id": "4gvhVWqZjqq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ç¬¬ 3 æ­¥ï¼šæ•°æ®å‡†å¤‡ä¸Žè‡ªåŠ¨æ³¨å†Œ\n",
        "é«˜è´¨é‡çš„æ•°æ®æ˜¯æ¨¡åž‹â€œå­¦å¾—å¥½â€çš„å…³é”®ã€‚ä¸åŒçš„å¾®è°ƒé˜¶æ®µéœ€è¦ä¸åŒæ ¼å¼çš„æ•°æ®ã€‚"
      ],
      "metadata": {
        "id": "ZGAm5YcGVpvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1 æ•°æ®æ ¼å¼è¯´æ˜Ž**\n",
        "\n",
        "\n",
        "| æ•°æ®æ ¼å¼ / å­—æ®µç»“æž„                                          | LLaMA-Factory            | Hugging Face Trainer / PEFT   | Hugging Face TRLï¼ˆDPO/PPOï¼‰    | è¯´æ˜Ž                              |\n",
        "| ------------------------------------------------------------ | ------------------------ | ----------------------------- | ------------------------------ | --------------------------------- |\n",
        "| âœ… `conversations` åˆ—è¡¨ç»“æž„ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰                   | âœ… åŽŸç”Ÿæ”¯æŒ               | âŒ ä¸æ”¯æŒï¼Œéœ€è¦æ‰‹åŠ¨æ‹¼æŽ¥        | âŒ ä¸æ”¯æŒï¼Œéœ€è¦æ‰‹åŠ¨æ‹¼æŽ¥         | åª LLaMA-Factory ä¼šè§£æžæˆå¯¹è¯æ¨¡æ¿ |\n",
        "| âœ… æ”¯æŒ `system` æ¶ˆæ¯                                         | âœ… æ”¯æŒï¼ˆå‚ä¸Žä¸Šä¸‹æ–‡æ‹¼æŽ¥ï¼‰ | âŒ ä¸æ”¯æŒï¼ˆéœ€æ‰‹åŠ¨æ‹¼å…¥ promptï¼‰ | âŒ ä¸æ”¯æŒï¼ˆéœ€æ‹¼å…¥ promptï¼‰      | Hugging Face ä¸è§£æžè§’è‰²å­—æ®µ       |\n",
        "| âœ… å•è½®é—®ç­”æ ¼å¼ï¼š `{\"input\": \"prompt\", \"output\": \"response\"}` | âŒ ä¸æŽ¨èç”¨æ­¤æ ¼å¼         | âœ… æ ‡å‡†æ”¯æŒæ ¼å¼                | âš ï¸ æ”¯æŒï¼ˆä½†å­—æ®µåå¯èƒ½éœ€è‡ªå®šä¹‰ï¼‰ | Hugging Face æŽ¨èçš„ç›‘ç£å¾®è°ƒæ ¼å¼   |\n",
        "| âœ… DPO æ ¼å¼ï¼š `{\"instruction\": \"...\", \"output\": [chosen, rejected]}` | âœ… æ”¯æŒï¼ˆtrain_dpo.pyï¼‰   | âŒ ä¸æ”¯æŒ                      | âœ… æ”¯æŒï¼ˆ`DPOTrainer`ï¼‰         | LLaMA å’Œ TRL éƒ½æ”¯æŒ               |\n",
        "| âœ… DPO æ ¼å¼å˜ä½“ï¼š `{\"prompt\": \"...\", \"chosen\": \"...\", \"rejected\": \"...\"}` | âœ… æ”¯æŒ                   | âŒ ä¸æ”¯æŒ                      | âœ… æ”¯æŒ                         | TRL æŽ¨èæ ¼å¼                      |\n",
        "| âœ… RM è®­ç»ƒæ ¼å¼ï¼š `{\"prompt\": \"...\", \"chosen\": \"...\", \"rejected\": \"...\"}` | âœ… æ”¯æŒ                   | âŒ ä¸æ”¯æŒ                      | âš ï¸ éƒ¨åˆ†æ”¯æŒï¼ˆéœ€è‡ªå®šä¹‰ RM æž„é€ ï¼‰ | ç”¨äºŽå¥–åŠ±æ¨¡åž‹è®­ç»ƒ                  |\n",
        "| âœ… GRPO / PPOï¼š `{\"prompt\": \"...\"} æˆ–å« history`              | âœ… æ”¯æŒ                   | âŒ ä¸æ”¯æŒ                      | âœ… æ”¯æŒï¼ˆ`PPOTrainer`ï¼‰         | history ä¸€èˆ¬éœ€æ‰‹åŠ¨æ‹¼å…¥ prompt     |\n",
        "| âœ… `.json` æ–‡ä»¶ï¼ˆæ•°ç»„å½¢å¼ï¼‰                                   | âœ… æ”¯æŒ                   | âœ… æ”¯æŒ                        | âœ… æ”¯æŒ                         | JSON æ–‡ä»¶å‡æ”¯æŒ                   |\n",
        "| âœ… `.jsonl` æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ª JSONï¼‰                             | âœ… æŽ¨èæ ¼å¼               | âœ… æ”¯æŒ                        | âœ… æ”¯æŒ                         | æ›´é€‚åˆå¤§è§„æ¨¡è®­ç»ƒ                  |"
      ],
      "metadata": {
        "id": "U6jv8aSWVz9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **3.2 å…¬å¼€æ•°æ®é›†æŽ¨è**\n",
        "\n",
        "å¯¹äºŽè®¸å¤šåœºæ™¯ï¼Œç¤¾åŒºå·²ç»è´¡çŒ®äº†é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "rCKpzJAAYTIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **SFT (ç›‘ç£å¾®è°ƒ) æ•°æ®é›†**\n",
        "\n",
        "\n",
        "SFT é˜¶æ®µçš„ç›®æ ‡æ˜¯æ•™ä¼šæ¨¡åž‹åŸºç¡€çš„çŸ¥è¯†å’Œå¯¹è¯é£Žæ ¼ã€‚æ­¤é˜¶æ®µæœ€å¸¸ç”¨çš„æ ¼å¼æ˜¯ `instruction`/`output` æ ¼å¼ï¼Œç‰¹åˆ«é€‚åˆæŒ‡ä»¤è·Ÿéšå’Œå•è½®é—®ç­”ã€‚\n",
        "\n",
        "- **ç¬¬ä¸€æ­¥ï¼šä¸‹è½½æ•°æ®é›†** æˆ‘ä»¬å°†ä½¿ç”¨ä¼˜è´¨æ•°æ®é›† `CFYuan/Chat-huanhuan`ï¼Œå®ƒçš„æ ¼å¼å°±æ˜¯æ ‡å‡†çš„ `instruction`/`input`/`output`ã€‚åˆ›å»ºä¸€ä¸ª `download_sft_dataset.py` è„šæœ¬æ¥ä¸‹è½½å¹¶ä¿å­˜å®ƒ\n",
        "\n"
      ],
      "metadata": {
        "id": "8ERYRAiBb2T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download_sft_dataset.py\n",
        "from modelscope.msdatasets import MsDataset\n",
        "import json\n",
        "\n",
        "def save_dataset_to_json(dataset, output_path):\n",
        "    \"\"\" å°†æ•°æ®é›†ä¿å­˜ä¸º JSON Lines æ ¼å¼ (.jsonl) \"\"\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for item in dataset:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    print(f\"æ•°æ®é›†å·²æˆåŠŸä¿å­˜åˆ°: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ä»Ž ModelScope åŠ è½½æ•°æ®é›†\n",
        "    dataset_name = 'CFYuan/Chat-huanhuan'\n",
        "    ms_dataset = MsDataset.load(dataset_name, subset_name='default', split='train', trust_remote_code=True)\n",
        "\n",
        "    # å®šä¹‰æœ¬åœ°ä¿å­˜è·¯å¾„\n",
        "    output_file_path = \"data/huanhuan_sft_local.json\"\n",
        "\n",
        "    # ç›´æŽ¥ä¿å­˜\n",
        "    save_dataset_to_json(ms_dataset, output_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "451AGnf1b8QV",
        "outputId": "ce547fe9-a83e-4906-f41f-db0468bc49a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-15 09:29:06,489 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from Chat-huanhuan. Please make sure that you can trust the external codes.\n",
            "2025-08-15 09:29:08,324 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from CFYuan/Chat-huanhuan. Please make sure that you can trust the external codes.\n",
            "2025-08-15 09:29:08,325 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from CFYuan/Chat-huanhuan. Please make sure that you can trust the external codes.\n",
            "2025-08-15 09:29:08,326 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from CFYuan/Chat-huanhuan. Please make sure that you can trust the external codes.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ•°æ®é›†å·²æˆåŠŸä¿å­˜åˆ°: data/huanhuan_sft_local.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¿è¡Œè„šæœ¬ python download_sft_dataset.pyï¼Œæ•°æ®å°†è¢«ä¿å­˜åˆ° data/huanhuan_sft_local.jsonã€‚\n"
      ],
      "metadata": {
        "id": "HFmKEq4vc_Fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **é€šç”¨æŒ‡ä»¤åž‹ï¼š`BelleGroup/train_2M_CN`**\n",
        "  - **ç®€ä»‹**ï¼šç”± BELLE é¡¹ç›®ç”Ÿæˆçš„ã€åŒ…å«çº¦200ä¸‡æ¡å¤šæ ·åŒ–æŒ‡ä»¤çš„ä¸­æ–‡æ•°æ®é›†ã€‚\n",
        "  - **é€‚ç”¨åœºæ™¯**ï¼šå…¨é¢æå‡æ¨¡åž‹åœ¨ä¸­æ–‡çŽ¯å¢ƒä¸‹çš„é€šç”¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ˆå¦‚é—®ç­”ã€ç¿»è¯‘ã€å†™ä½œç­‰ï¼‰ã€‚\n",
        "  - **LLaMA-Factory åç§°**: `belle_2m`"
      ],
      "metadata": {
        "id": "z9CBeBnnb7Zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DPO/ORPO æ•°æ®å‡†å¤‡\n",
        "\n",
        "- **ç¬¬ä¸€æ­¥ï¼šä¸‹è½½æ•°æ®é›†** æˆ‘ä»¬å°†ä½¿ç”¨é«˜è´¨é‡çš„ä¸­æ–‡é—®ç­”åå¥½æ•°æ®é›† `liyucheng/zhihu_rlhf_3k`ã€‚åˆ›å»ºä¸€ä¸ª `download_dpo_dataset.py` è„šæœ¬æ¥ä¸‹è½½å¹¶ä¿å­˜å®ƒã€‚"
      ],
      "metadata": {
        "id": "q_QVffFDoz85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download_dpo_dataset.py\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "def save_dataset_to_json(dataset, output_path):\n",
        "    \"\"\" å°†æ•°æ®é›†ä¿å­˜ä¸º JSON Lines æ ¼å¼ (.jsonl) \"\"\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for item in dataset:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    print(f\"æ•°æ®é›†å·²æˆåŠŸä¿å­˜åˆ°: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ä»Ž Hugging Face Hub åŠ è½½æ•°æ®é›†\n",
        "    dataset_name = 'liyucheng/zhihu_rlhf_3k'\n",
        "    dataset = load_dataset(dataset_name, split='train')\n",
        "\n",
        "    # å®šä¹‰æœ¬åœ°ä¿å­˜è·¯å¾„\n",
        "    output_file_path = \"data/zhihu_rlhf_local.json\"\n",
        "\n",
        "    # ç›´æŽ¥ä¿å­˜\n",
        "    save_dataset_to_json(dataset, output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "88cbb58646de4eb8809af64fadac2363",
            "63c0fa7865c14421a951f7276bbe8134",
            "82b89cbe049646df817ce09f5d0bf06a",
            "49fae0384f5d4005954e4bad7a4b23b7",
            "351e025fc08b46828e13b610a62a44a1",
            "7a20944fcfef4381ab7b27ef6eb161fb",
            "f4b9036aba5545b596346a3dcfda6458",
            "367634585ad24e92986be0ee398339fe",
            "80d077418cdf48bc9de7fe077811538b",
            "f63af71d4416475b9831660aa9c59c66",
            "d6d43d3310fe47f4b8f68f99cad803ce",
            "59f86f2fa3b940b3a4a687829467126a",
            "6b8996e673f148e8a2dbf6a028d3be1a",
            "3cb3823108044a3784d7bf25ba833374",
            "8a5511f15a4b469494f3c7eba9540ae6",
            "8c76ba3cebb24d3ba110c78794a6e1da",
            "b4c67452736747dd8db5fae3f53646d8",
            "fd10d4fd2fb3464dab51ac40d8b4b4ca",
            "dc5129ca5e0b4af0ad73dbbc68429dcf",
            "56713605bd7e4ff39f67036e36399cc8",
            "80b2e935174543099a1d09b7e039b31a",
            "b28896743cba454fad6a6a94e9a63e56",
            "c571c18ae4df4a1a90f2819fc5d9803f",
            "b7e8a60dc39a434a8d97f98580ff201f",
            "98449b50d77043e7bba60e6f2e0d5b18",
            "4931472f477d4573bf8413f2e629303d",
            "a512cbdf8201442488b1b603459d3f3c",
            "3f714606ca0c4bd5ac0642d7defeca87",
            "896eb5662e3046529d24b3a1d0d72a38",
            "b0ed61da0cd845c288cbdf14c7ee278e",
            "3b64256559f241f5ac1075da9237f11b",
            "e8318f3eb4d044269003276083e3e9c5",
            "307f549df91f4f1fbc4c80721d72f6c5"
          ]
        },
        "id": "L5FqvnH2o9Pp",
        "outputId": "a6c8e22c-ebe6-43ac-f997-260d6a369914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88cbb58646de4eb8809af64fadac2363"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "zhihu_3k_rlfh.tsv:   0%|          | 0.00/15.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59f86f2fa3b940b3a4a687829467126a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/3460 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c571c18ae4df4a1a90f2819fc5d9803f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ•°æ®é›†å·²æˆåŠŸä¿å­˜åˆ°: data/zhihu_rlhf_local.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¿è¡Œè„šæœ¬ python download_dpo_dataset.pyï¼Œæ•°æ®å°†è¢«ä¿å­˜åˆ° data/zhihu_rlhf_local.jsonã€‚"
      ],
      "metadata": {
        "id": "EMqxCth5o_4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä¸ºäº†è®©æ¨¡åž‹å…ˆå­¦ä¹ â€œçŸ¥ä¹Žé—®ç­”â€çš„åŸºæœ¬æ¨¡å¼ï¼Œæˆ‘ä»¬ä»Ž DPO æ•°æ®ä¸­æå–é«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œä½œä¸º SFT æ•°æ®ã€‚åˆ›å»ºä¸€ä¸ª create_sft_from_dpo.py è„šæœ¬ã€‚"
      ],
      "metadata": {
        "id": "xjbsdBm8pRIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create_sft_from_dpo.py\n",
        "import json\n",
        "\n",
        "def convert_dpo_to_sft(dpo_path, sft_path):\n",
        "    sft_data = []\n",
        "    with open(dpo_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            item = json.loads(line)\n",
        "            sft_item = {\n",
        "                \"instruction\": item[\"prompt\"],\n",
        "                \"input\": \"\",\n",
        "                \"output\": item[\"chosen\"] # ä½¿ç”¨é«˜è´¨é‡å›žç­”ä½œä¸ºSFTçš„ç­”æ¡ˆ\n",
        "            }\n",
        "            sft_data.append(sft_item)\n",
        "\n",
        "    with open(sft_path, 'w', encoding='utf-8') as f:\n",
        "        for item in sft_data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    print(f\"å·²ä»Ž {dpo_path} æå–å¹¶åˆ›å»º SFT æ•°æ®é›†äºŽ: {sft_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    convert_dpo_to_sft(\"data/zhihu_rlhf_local.json\", \"data/zhihu_sft_local.json\")"
      ],
      "metadata": {
        "id": "r23Pz4mYqDdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¿è¡Œ python create_sft_from_dpo.pyã€‚"
      ],
      "metadata": {
        "id": "ODKvnkdnqHJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### é€šè¿‡ä»£ç æ³¨å†Œæ•°æ®é›† (æŽ¨è)"
      ],
      "metadata": {
        "id": "bIceZUDbpDZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä¸‹è½½å®Œæ•°æ®é›†åŽï¼Œæ‰‹åŠ¨ç¼–è¾‘ `dataset_info.json` è¾ƒä¸ºç¹çã€‚æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªè„šæœ¬æ¥è‡ªåŠ¨å®Œæˆæ³¨å†Œã€‚\n",
        "\n",
        "- **åˆ›å»º `register_datasets.py` è„šæœ¬** åœ¨ `LLaMA-Factory` æ ¹ç›®å½•ä¸‹åˆ›å»ºè¯¥æ–‡ä»¶ã€‚"
      ],
      "metadata": {
        "id": "d9K_Z04UpIwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register_datasets.py\n",
        "import json\n",
        "import os\n",
        "\n",
        "def register_dataset(dataset_info_path, dataset_name, file_name, columns, stage):\n",
        "    \"\"\"\n",
        "    ä»¥ç¼–ç¨‹æ–¹å¼è¯»å–ã€æ›´æ–°å¹¶ä¿å­˜ dataset_info.json æ–‡ä»¶ï¼Œå¹¶åŒ…å« stage ä¿¡æ¯ã€‚\n",
        "    \"\"\"\n",
        "    if os.path.exists(dataset_info_path):\n",
        "        with open(dataset_info_path, 'r', encoding='utf-8') as f:\n",
        "            all_datasets = json.load(f)\n",
        "    else:\n",
        "        all_datasets = {}\n",
        "\n",
        "    all_datasets[dataset_name] = {\n",
        "        \"file_name\": file_name,\n",
        "        \"stage\": stage, # æ·»åŠ æ•°æ®é›†é€‚ç”¨çš„é˜¶æ®µ\n",
        "        \"columns\": columns,\n",
        "        \"ranking\": stage==\"rm\",\n",
        "    }\n",
        "\n",
        "    with open(dataset_info_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_datasets, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"æˆåŠŸæ³¨å†Œæ•°æ®é›† '{dataset_name}' (é˜¶æ®µ: {stage})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    info_path = \"data/dataset_info.json\"\n",
        "    # æ³¨å†Œ SFT ç« èŠ‚çš„æ•°æ®é›†\n",
        "    register_dataset(info_path, \"huanhuan_sft_local\", \"huanhuan_sft_local.json\",\n",
        "                     {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}, stage=\"sft\")\n",
        "    # æ³¨å†Œ DPO ç« èŠ‚å¯é€‰ SFT æ­¥éª¤çš„æ•°æ®é›†\n",
        "    register_dataset(info_path, \"zhihu_sft_local\", \"zhihu_sft_local.json\",\n",
        "                     {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}, stage=\"sft\")\n",
        "    # æ³¨å†Œ DPO ç« èŠ‚çš„æ•°æ®é›†\n",
        "    # DPO é˜¶æ®µåœ¨ LLaMA-Factory ä¸­å†…éƒ¨ä½¿ç”¨ \"rm\" (Reward Modeling) çš„æ•°æ®åŠ è½½é€»è¾‘\n",
        "    register_dataset(info_path, \"zhihu_dpo_local\", \"zhihu_rlhf_local.json\",\n",
        "                     {\"prompt\": \"prompt\", \"chosen\": \"chosen\", \"rejected\": \"rejected\"}, stage=\"rm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "rBENvHappE2J",
        "outputId": "7d6eaf75-8a90-4945-b6b4-269e3353b8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/dataset_info.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2582901332.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0minfo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/dataset_info.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# æ³¨å†Œ SFT ç« èŠ‚çš„æ•°æ®é›†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     register_dataset(info_path, \"huanhuan_sft_local\", \"huanhuan_sft_local.json\", \n\u001b[0m\u001b[1;32m     29\u001b[0m                      {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}, stage=\"sft\")\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# æ³¨å†Œ DPO ç« èŠ‚å¯é€‰ SFT æ­¥éª¤çš„æ•°æ®é›†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2582901332.py\u001b[0m in \u001b[0;36mregister_dataset\u001b[0;34m(dataset_info_path, dataset_name, file_name, columns, stage)\u001b[0m\n\u001b[1;32m     19\u001b[0m     }\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_info_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"æˆåŠŸæ³¨å†Œæ•°æ®é›† '{dataset_name}' (é˜¶æ®µ: {stage})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/dataset_info.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- è¿è¡Œè„šæœ¬\n",
        "\n",
        "æ‰§è¡Œ python register_datasets.pyï¼Œå³å¯ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰æœ¬åœ°æ•°æ®é›†çš„æ³¨å†Œã€‚"
      ],
      "metadata": {
        "id": "zUnoh9-ppOGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### GRPO (ç”Ÿæˆå¼é‡æ”¾ç­–ç•¥ä¼˜åŒ–) æ•°æ®å‡†å¤‡\n",
        "GRPO åªéœ€è¦ä¸€ç³»åˆ—â€œæç¤ºâ€(Prompt)å³å¯ï¼Œéžå¸¸é€‚åˆæ•°å­¦ã€ä»£ç ç­‰ç”Ÿæˆä»»åŠ¡ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ç»å…¸çš„ GSM8K æ•°å­¦æŽ¨ç†æ•°æ®é›†ã€‚è¯¦è§åŽç»­ç« èŠ‚ã€‚"
      ],
      "metadata": {
        "id": "WFlb8SUzpToc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç¬¬äºŒéƒ¨åˆ†ï¼šSFT å¾®è°ƒç« èŠ‚"
      ],
      "metadata": {
        "id": "_16afEM4WRwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç›®æ ‡**ï¼šä¸ºæ¨¡åž‹æ³¨å…¥åŸºç¡€çŸ¥è¯†æˆ–ç‰¹å®šé¢†åŸŸçš„è¯´è¯é£Žæ ¼ã€‚SFT æ˜¯æœ€åŸºç¡€ä¹Ÿæ˜¯æœ€é‡è¦çš„ä¸€æ­¥ï¼Œå®ƒä¸ºåŽç»­æ›´é«˜çº§çš„å¯¹é½æŠ€æœ¯ï¼ˆå¦‚ DPOï¼‰æ‰“ä¸‹åšå®žçš„åŸºç¡€ã€‚\n",
        "\n",
        "**ç¤ºä¾‹**ï¼šåœ¨æœ¬ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åœ¨ç¬¬ä¸€éƒ¨åˆ†å‡†å¤‡å¥½çš„â€œç”„å¬›ä¼ â€é£Žæ ¼æ•°æ®é›† (`huanhuan_sft_local`)ï¼Œé€šè¿‡ SFT-LoRA æŠ€æœ¯ï¼Œè®­ç»ƒä¸€ä¸ªå…·æœ‰ç‰¹å®šè¯´è¯é£Žæ ¼çš„æ¨¡åž‹ã€‚"
      ],
      "metadata": {
        "id": "QLh2cJQ-pnAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨ LLaMA-Factory è¿›è¡Œ SFT\n",
        "\n",
        "LLaMA-Factory æä¾›äº†é«˜åº¦é›†æˆçš„å‘½ä»¤è¡Œå·¥å…·ï¼Œæ˜¯è¿›è¡Œ SFT å¾®è°ƒçš„æœ€é«˜æ•ˆé€‰æ‹©ã€‚"
      ],
      "metadata": {
        "id": "UMMQX1Gyp_cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SFT è®­ç»ƒå‘½ä»¤\n",
        "ä¸‹é¢æ˜¯ç”¨äºŽå¯åŠ¨ SFT è®­ç»ƒçš„ bash è„šæœ¬ã€‚\n",
        "\n",
        "```bash\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "export FORCE_TORCHRUN=1\n",
        "\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "OUTPUT_PATH_SFT='./output/Zhenhuan_Style_SFT_LLF'\n",
        "DATASET_NAME_SFT='huanhuan_sft_local'\n",
        "\n",
        "llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --dataset $DATASET_NAME_SFT \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --output_dir $OUTPUT_PATH_SFT \\\n",
        "    --overwrite_cache \\\n",
        "    --overwrite_output_dir \\\n",
        "    --cutoff_len 4096 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 \\\n",
        "    \\\n",
        "    # --- QLoRA é‡åŒ–å‚æ•° ---\n",
        "    --quantization_bit 4 \\\n",
        "    --double_quantization True \\\n",
        "    --quantization_type nf4 \\\n",
        "    \\\n",
        "    # --- LoRA å‚æ•° ---\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --lora_target q_proj,v_proj\n",
        "```"
      ],
      "metadata": {
        "id": "T2aduvhk690L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### å‚æ•°æ·±åº¦è§£æž\n",
        "\n",
        "è¿™é‡Œå¯¹ä¸Šè¿°å‘½ä»¤ä¸­çš„å…³é”®å‚æ•°è¿›è¡Œè¯¦ç»†è¯´æ˜Žï¼š\n",
        "\n",
        "| åˆ†ç±»           | å‚æ•°                              | è¯´æ˜Ž                                                         |\n",
        "| -------------- | --------------------------------- | ------------------------------------------------------------ |\n",
        "| **æ ¸å¿ƒå‚æ•°**   | `--stage sft`                     | æŒ‡å®šå½“å‰ä»»åŠ¡ä¸º **ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning)** é˜¶æ®µã€‚  |\n",
        "|                | `--do_train`                      | æ˜Žç¡®æŒ‡ç¤ºè„šæœ¬æ‰§è¡Œè®­ç»ƒæµç¨‹ã€‚                                   |\n",
        "|                | `--model_name_or_path`            | æŒ‡å®šåŸºç¡€æ¨¡åž‹ã€‚å¯ä»¥æ˜¯ Hugging Face Hub IDã€ModelScope ID æˆ–æœ¬åœ°è·¯å¾„ã€‚ |\n",
        "|                | `--dataset`                       | æŒ‡å®šåœ¨ `dataset_info.json` ä¸­æ³¨å†Œçš„æ•°æ®é›†åç§°ã€‚              |\n",
        "|                | `--template qwen`                 | **æžå…¶é‡è¦**ã€‚æŒ‡å®šå¯¹è¯æ¨¡æ¿ï¼Œå¿…é¡»ä¸ŽåŸºç¡€æ¨¡åž‹ï¼ˆå¦‚æ­¤å¤„çš„ Qwenï¼‰ä¸¥æ ¼åŒ¹é…ï¼Œå¦åˆ™æ¨¡åž‹æ— æ³•æ­£ç¡®ç†è§£è¾“å…¥ã€‚ |\n",
        "|                | `--finetuning_type lora`          | æŒ‡å®šå¾®è°ƒæ–¹æ³•ä¸º LoRAï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæŠ€æœ¯ã€‚            |\n",
        "|                | `--output_dir`                    | æŒ‡å®šæ‰€æœ‰è®­ç»ƒäº§ç‰©ï¼ˆæ¨¡åž‹æƒé‡ã€æ—¥å¿—ã€æ£€æŸ¥ç‚¹ï¼‰çš„ä¿å­˜ç›®å½•ã€‚       |\n",
        "| **è®­ç»ƒæŽ§åˆ¶**   | `--overwrite_cache`               | è¦†ç›–é¢„å¤„ç†åŽçš„æ•°æ®ç¼“å­˜ã€‚å½“æ‚¨ä¿®æ”¹äº†æ•°æ®é›†æˆ–æ•°æ®å¤„ç†æ–¹å¼æ—¶ï¼Œå»ºè®®å¼€å¯ã€‚ |\n",
        "|                | `--overwrite_output_dir`          | å…è®¸è¦†ç›–è¾“å‡ºç›®å½•ä¸­å·²æœ‰çš„å†…å®¹ï¼Œæ–¹ä¾¿é‡å¤å®žéªŒã€‚                 |\n",
        "|                | `--cutoff_len 4096`               | è®¾ç½®æ¨¡åž‹å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆtokensï¼‰ã€‚éœ€è¦æ ¹æ®æ‚¨çš„ä»»åŠ¡å’Œæ˜¾å­˜å¤§å°è¿›è¡Œè°ƒæ•´ã€‚ |\n",
        "|                | `--per_device_train_batch_size 4` | æ¯å— GPU åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­å¤„ç†çš„æ ·æœ¬æ•°é‡ã€‚                    |\n",
        "|                | `--gradient_accumulation_steps 4` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚`æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = batch_size * ç´¯ç§¯æ­¥æ•°`ã€‚è¿™æ˜¯åœ¨æ˜¾å­˜æœ‰é™æ—¶æ‰©å¤§æ‰¹æ¬¡å¤§å°çš„å¸¸ç”¨æŠ€å·§ã€‚ |\n",
        "|                | `--learning_rate 1e-4`            | å­¦ä¹ çŽ‡ã€‚å¯¹äºŽ LoRA å¾®è°ƒï¼Œ`1e-4` æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„ã€æ•ˆæžœä¸é”™çš„åˆå§‹å€¼ã€‚ |\n",
        "|                | `--num_train_epochs 3`            | è®­ç»ƒçš„æ€»è½®æ•°ã€‚å¯¹äºŽ SFTï¼Œé€šå¸¸ 1-3 è½®å³å¯èŽ·å¾—ä¸é”™çš„æ•ˆæžœã€‚      |\n",
        "|                | `--plot_loss`                     | åœ¨è®­ç»ƒç»“æŸåŽï¼Œåœ¨è¾“å‡ºç›®å½•ä¸­ç”Ÿæˆä¸€å¼ è®­ç»ƒæŸå¤±æ›²çº¿å›¾ `training_loss.png`ã€‚ |\n",
        "|                | `--fp16`                          | å¯ç”¨åŠç²¾åº¦ï¼ˆ16-bitï¼‰æµ®ç‚¹æ•°è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥å¤§å¹…èŠ‚çœæ˜¾å­˜å¹¶æå‡è®­ç»ƒé€Ÿåº¦ã€‚ |\n",
        "| **QLoRA é‡åŒ–** | `--quantization_bit 4`            | **QLoRA æ ¸å¿ƒ**ã€‚æŒ‡å®šä½¿ç”¨ 4-bit å¯¹æ¨¡åž‹åŸºåº§è¿›è¡Œé‡åŒ–ï¼Œæžå¤§é™ä½Žäº†æ˜¾å­˜å ç”¨ã€‚ |\n",
        "|                | `--double_quantization True`      | å¯ç”¨åŒé‡é‡åŒ–ï¼Œå¯ä»¥è¿›ä¸€æ­¥èŠ‚çœå°‘é‡æ˜¾å­˜ï¼ŒæŽ¨èå¼€å¯ã€‚             |\n",
        "|                | `--quantization_type nf4`         | æŒ‡å®šé‡åŒ–ç±»åž‹ä¸º `nf4` (Normal Float 4)ï¼Œè¿™æ˜¯ QLoRA æŽ¨èçš„ã€ç†è®ºä¸Šæ›´ä¼˜çš„ 4-bit æ•°æ®ç±»åž‹ã€‚ |\n",
        "| **LoRA å‚æ•°**  | `--lora_rank 8`                   | LoRA çŸ©é˜µçš„ç§© (r)ã€‚å†³å®šäº† LoRA é€‚é…å™¨çš„å‚æ•°é‡å¤§å°ã€‚å¸¸ç”¨å€¼ä¸º 8, 16, 32, 64ã€‚å€¼è¶Šå¤§ï¼Œå¯è®­ç»ƒå‚æ•°è¶Šå¤šï¼Œæ‹Ÿåˆèƒ½åŠ›è¶Šå¼ºï¼Œä½†è¿‡å¤§ä¹Ÿå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚ |\n",
        "|                | `--lora_alpha 16`                 | LoRA çš„ç¼©æ”¾å› å­ã€‚é€šå¸¸è®¾ç½®ä¸º `lora_rank` çš„ 2 å€ï¼Œè¿™æ˜¯ä¸€ä¸ªç»éªŒæ€§çš„æœ€ä½³å®žè·µã€‚ |\n",
        "|                | `--lora_dropout 0.05`             | åœ¨ LoRA çŸ©é˜µä¸Šåº”ç”¨çš„ Dropout æ¯”çŽ‡ï¼Œç”¨äºŽé˜²æ­¢è¿‡æ‹Ÿåˆã€‚          |\n",
        "|                | `--lora_target all`               | **æŽ¨èè®¾ç½®**ã€‚æŒ‡å®šå°† LoRA åº”ç”¨åˆ°æ¨¡åž‹ä¸­çš„å“ªäº›æ¨¡å—ã€‚è®¾ç½®ä¸º `all` åŽï¼ŒLLaMA-Factory ä¼šè‡ªåŠ¨è¯†åˆ«æ‰€æœ‰å¯åº”ç”¨çš„çº¿æ€§å±‚ï¼ˆå¦‚ `q_proj`, `v_proj` ç­‰ï¼‰ï¼ŒçœåŽ»æ‰‹åŠ¨æŒ‡å®šçš„éº»çƒ¦ã€‚ |\n",
        "\n"
      ],
      "metadata": {
        "id": "RJzKVZoUqRTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### æ‰§è¡Œè®­ç»ƒ"
      ],
      "metadata": {
        "id": "ZW2H--mqqcqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "# os.environ[\"WANDB_MODE\"] = \"offline\"  # ç¦»çº¿æ¨¡å¼\n",
        "\n",
        "# --- è¯·åœ¨æ­¤å¤„ä¿®æ”¹ä½ çš„è·¯å¾„å’Œåç§° ---\n",
        "# æ¨¡åž‹IDï¼Œå¯ä»¥æ˜¯ Hugging Face Hub ID æˆ– ModelScope ID\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "OUTPUT_PATH='./output/Zhenhuan_Style_SFT'\n",
        "# ä½¿ç”¨æ‚¨åœ¨æ­¥éª¤2.1å’Œ2.3ä¸­å‡†å¤‡å¥½çš„æœ¬åœ°æ•°æ®é›†\n",
        "DATASET_NAME='huanhuan_sft_local'\n",
        "# ---------------------------------\n",
        "DS_CONFIG_PATH='examples/deepspeed/ds_z3_config.json'\n",
        "\n",
        "!FORCE_TORCHRUN=1 llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --dataset $DATASET_NAME_SFT \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --output_dir $OUTPUT_PATH_SFT \\\n",
        "    --overwrite_cache \\\n",
        "    --overwrite_output_dir \\\n",
        "    --cutoff_len 4096 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 \\\n",
        "    \\\n",
        "    --quantization_bit 4 \\\n",
        "    --double_quantization True \\\n",
        "    --quantization_type nf4 \\\n",
        "    \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --lora_target all \\\n",
        "    --report_to none"
      ],
      "metadata": {
        "id": "4Ymp281MVpBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### æ–¹æ¡ˆäºŒï¼šä½¿ç”¨ Hugging Face TRL è¿›è¡Œ SFT\n",
        "\n",
        "å¯¹äºŽå¸Œæœ›æ›´æ·±å…¥å®šåˆ¶è®­ç»ƒé€»è¾‘çš„ç”¨æˆ·ï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨ Hugging Face çš„ TRL åº“ã€‚"
      ],
      "metadata": {
        "id": "g87HMcZo7QXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_sft_with_trl.py\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def train_sft():\n",
        "    # --- 1. å®šä¹‰æ¨¡åž‹ã€æ•°æ®é›†å’Œè¾“å‡ºè·¯å¾„ ---\n",
        "    model_id = 'qwen/Qwen1.5-0.5B-Chat'\n",
        "    dataset_name = \"huanhuan_sft_local\"\n",
        "    output_dir = \"./output/Zhenhuan_Style_SFT_TRL\"\n",
        "\n",
        "    # --- 2. é…ç½® QLoRA é‡åŒ– ---\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,                      # æ¿€æ´»4-bité‡åŒ–\n",
        "        bnb_4bit_quant_type=\"nf4\",              # é‡åŒ–ç±»åž‹ (nf4, fp4)\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,  # è®¡ç®—æ•°æ®ç±»åž‹\n",
        "        bnb_4bit_use_double_quant=True,         # æ¿€æ´»åµŒå¥—é‡åŒ–\n",
        "    )\n",
        "\n",
        "    # --- 3. åŠ è½½æ¨¡åž‹å’Œåˆ†è¯å™¨ ---\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map={\"\": 0} # æŒ‡å®šGPU\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # --- 4. åŠ è½½æ•°æ®é›† ---\n",
        "    dataset = load_dataset(\"json\", data_files=f\"data/{dataset_name}.json\", split=\"train\")\n",
        "\n",
        "    # --- 5. é…ç½®è®­ç»ƒå‚æ•° ---\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=1e-4,\n",
        "        num_train_epochs=3,\n",
        "        logging_steps=10,\n",
        "        fp16=True,\n",
        "        save_strategy=\"epoch\",\n",
        "    )\n",
        "\n",
        "    # --- 6. é…ç½® LoRA ---\n",
        "    peft_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        # æŒ‡å®šè¦åº”ç”¨ LoRA çš„æ¨¡å—ã€‚è¿™æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # --- 7. åˆ›å»ºå¹¶å¯åŠ¨ SFTTrainer ---\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"instruction\", # æŒ‡å®šåŒ…å«æ–‡æœ¬çš„åˆ—\n",
        "        max_seq_length=1024,\n",
        "        peft_config=peft_config,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    print(f\"SFT è®­ç»ƒå®Œæˆï¼Œæ¨¡åž‹å·²ä¿å­˜è‡³ {output_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_sft()"
      ],
      "metadata": {
        "id": "zylT8PeY7Uqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¿è¡Œè„šæœ¬ python train_sft_with_trl.pyã€‚"
      ],
      "metadata": {
        "id": "XIHSaZZZ7ffy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SFT æ¨¡åž‹åˆå¹¶ä¸Žæµ‹è¯•\n",
        "\n",
        "æ— è®ºä½¿ç”¨å“ªç§æ–¹æ¡ˆï¼Œè®­ç»ƒå®ŒæˆåŽå¾—åˆ°çš„éƒ½åªæ˜¯ä¸€ä¸ªè½»é‡çš„ LoRA é€‚é…å™¨ï¼ˆadapterï¼‰ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡åž‹ã€‚æˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªé€‚é…å™¨ä¸ŽåŽŸå§‹çš„åŸºç¡€æ¨¡åž‹åˆå¹¶ï¼Œæ‰èƒ½å¾—åˆ°ä¸€ä¸ªå¯ä»¥ç‹¬ç«‹éƒ¨ç½²å’Œä½¿ç”¨çš„æ¨¡åž‹ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "aa2MwLTjX1do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### åˆå¹¶æ¨¡åž‹\n",
        "\n",
        "```bash\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "BASE_MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "ADAPTER_PATH='./output/Zhenhuan_Style_SFT'\n",
        "EXPORT_PATH='./output/Zhenhuan_SFT_Merged'\n",
        "\n",
        "llamafactory-cli export \\\n",
        "    --model_name_or_path $BASE_MODEL_PATH \\\n",
        "    --adapter_name_or_path $ADAPTER_PATH \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --export_dir $EXPORT_PATH \\\n",
        "    --export_size 2 \\\n",
        "    --export_legacy_format False\n",
        "```"
      ],
      "metadata": {
        "id": "_zGQuolEqsTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "ADAPTER_PATH='./output/Zhenhuan_Style_SFT'\n",
        "EXPORT_PATH='./output/Zhenhuan_SFT_Merged'\n",
        "\n",
        "!FORCE_TORCHRUN=1 llamafactory-cli export \\\n",
        "    --model_name_or_path $BASE_MODEL_PATH \\\n",
        "    --adapter_name_or_path $ADAPTER_PATH \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --export_dir $EXPORT_PATH \\\n",
        "    --export_size 2 \\\n",
        "    --export_legacy_format False"
      ],
      "metadata": {
        "id": "h5DcpK_QZnuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### å¯åŠ¨ Web UI èŠå¤©æµ‹è¯•"
      ],
      "metadata": {
        "id": "tydGwxbwZicA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "MERGED_MODEL_PATH='./output/Zhenhuan_SFT_Merged'\n",
        "\n",
        "!llamafactory-cli webchat \\\n",
        "    --model_name_or_path $MERGED_MODEL_PATH \\\n",
        "    --template qwen"
      ],
      "metadata": {
        "id": "pgXFHyaQbDE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### é€šè¿‡ä»£ç è¿›è¡Œäº¤äº’å¼æŽ¨ç†\n",
        "\n",
        "é€šè¿‡ Python è„šæœ¬æ¥è°ƒç”¨æ¨¡åž‹ï¼Œè¿›è¡Œæ›´çµæ´»çš„æµ‹è¯•ã€‚"
      ],
      "metadata": {
        "id": "fnVo6Tv7cTua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# --- è¯·ç¡®ä¿è¿™é‡Œçš„è·¯å¾„æŒ‡å‘æ‚¨åˆå¹¶åŽçš„æ¨¡åž‹ ---\n",
        "model_path = './output/Zhenhuan_SFT_Merged'\n",
        "\n",
        "print(f\"æ­£åœ¨ä»Ž '{model_path}' åŠ è½½æ¨¡åž‹å’Œåˆ†è¯å™¨...\")\n",
        "\n",
        "# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡åž‹\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16, # æŽ¨èä½¿ç”¨ bfloat16 ä»¥èŽ·å¾—æ›´å¥½çš„æ€§èƒ½å’Œå…¼å®¹æ€§\n",
        "    device_map=\"auto\" # è‡ªåŠ¨å°†æ¨¡åž‹åŠ è½½åˆ°å¯ç”¨çš„ GPU\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"æ¨¡åž‹åŠ è½½å®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆå›žç­”ã€‚\")\n",
        "\n",
        "# --- å…³é”®æ­¥éª¤ï¼šæž„å»ºç¬¦åˆæ¨¡æ¿çš„å¯¹è¯ ---\n",
        "\n",
        "# 1. å°†ä½ çš„é—®é¢˜æž„é€ æˆä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«å­—å…¸ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªè§’è‰²å’Œå…¶å†…å®¹\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"ä½ å¥½ï¼Œå¯ä»¥ä»‹ç»ä¸€ä¸‹ä½ æ˜¯è°å—ï¼Ÿ\"}\n",
        "]\n",
        "\n",
        "# 2. ä½¿ç”¨ tokenizer.apply_chat_template æ¥æ ¼å¼åŒ–è¾“å…¥\n",
        "#    è¿™ä¸ªå‡½æ•°ä¼šè‡ªåŠ¨æ·»åŠ  <|im_start|>user\\n...<|im_end|>\\n<|im_start|>assistant\\n ç­‰ç‰¹æ®Šæ ‡è®°\n",
        "#    add_generation_prompt=True ä¼šåœ¨æœ€åŽåŠ ä¸Š assistant çš„èµ·å§‹æ ‡è®°ï¼Œå¼•å¯¼æ¨¡åž‹å¼€å§‹ç”Ÿæˆ\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "# --- ç”Ÿæˆå›žç­” ---\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "# --- è§£ç å¹¶æ‰“å° ---\n",
        "# outputs[0] åŒ…å«äº†è¾“å…¥çš„ prompt å’Œæ–°ç”Ÿæˆçš„å›žç­”\n",
        "# æˆ‘ä»¬éœ€è¦ä»Ž outputs ä¸­å‰¥ç¦»æŽ‰è¾“å…¥çš„ prompt éƒ¨åˆ†ï¼Œåªè§£ç æ–°ç”Ÿæˆçš„å†…å®¹\n",
        "response_ids = outputs[0][inputs.shape[-1]:]\n",
        "response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- å¯¹è¯ ---\")\n",
        "print(\"User:\", messages[0][\"content\"])\n",
        "print(\"Assistant:\", response)"
      ],
      "metadata": {
        "id": "co0WF9GLcPNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šDPO å¾®è°ƒç« èŠ‚\n",
        "\n",
        "**ç›®æ ‡**ï¼šåœ¨æ¨¡åž‹å…·å¤‡åŸºç¡€èƒ½åŠ›åŽï¼Œé€šè¿‡â€œå¥½/åâ€å›žç­”å¯¹æ¯”ï¼Œè®©å…¶å­¦ä¹ äººç±»çš„åå¥½ï¼Œç”Ÿæˆæ›´ä¼˜è´¨çš„å›žç­”ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "PPUO5HuKo2uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (å¯é€‰) é¢„å¤‡æ­¥éª¤ï¼šSFT\n",
        "\n",
        "**ä¸ºä»€ä¹ˆéœ€è¦ SFTï¼Ÿ** ç›´æŽ¥å¯¹åŸºç¡€æ¨¡åž‹è¿›è¡Œ DPO ä¹Ÿå¯ä»¥ï¼Œä½†é€šå¸¸æ•ˆæžœä¸å¦‚å…ˆè¿›è¡Œ SFTã€‚SFT é˜¶æ®µå¯ä»¥è®©æ¨¡åž‹å…ˆå­¦ä¹ ç›®æ ‡é¢†åŸŸçš„åŸºæœ¬çŸ¥è¯†å’Œæ•°æ®åˆ†å¸ƒï¼ˆä¾‹å¦‚ï¼Œå…ˆå­¦ä¼šå¦‚ä½•å›žç­”çŸ¥ä¹Žé£Žæ ¼çš„é—®é¢˜ï¼‰ï¼Œä¸ºåŽç»­çš„åå¥½å­¦ä¹ æä¾›ä¸€ä¸ªæ›´å¥½çš„èµ·ç‚¹ï¼Œä»Žè€Œè®© DPO è®­ç»ƒæ›´ç¨³å®šã€æ•ˆæžœæ›´å¥½ã€‚\n",
        "\n",
        "**ç¤ºä¾‹**ï¼šä½¿ç”¨æˆ‘ä»¬ä»ŽçŸ¥ä¹Žåå¥½æ•°æ®ä¸­æå–çš„ SFT æ•°æ®é›†ï¼Œå…ˆè¿›è¡Œä¸€è½® SFTã€‚\n",
        "```bash\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "export FORCE_TORCHRUN=1\n",
        "\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "OUTPUT_PATH_SFT='./output/ZhihuQA_SFT'\n",
        "DATASET_NAME_SFT='zhihu_sft_local'\n",
        "\n",
        "llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --dataset $DATASET_NAME_SFT \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --output_dir $OUTPUT_PATH_SFT \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-5 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --plot_loss \\\n",
        "    --fp16\n",
        "    \\\n",
        "    # --- QLoRA é‡åŒ–å‚æ•° ---\n",
        "    --quantization_bit 4 \\\n",
        "    --double_quantization True \\\n",
        "    --quantization_type nf4 \\\n",
        "    \\\n",
        "    # --- LoRA å‚æ•° ---\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --lora_target all \\\n",
        "    --report_to none\n"
      ],
      "metadata": {
        "id": "k5GaV1ou2lHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "# os.environ[\"WANDB_MODE\"] = \"offline\"  # ç¦»çº¿æ¨¡å¼\n",
        "os.environ[\"FORCE_TORCHRUN\"] = \"1\"\n",
        "\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "OUTPUT_PATH_SFT='./output/ZhihuQA_SFT'\n",
        "DATASET_NAME_SFT='zhihu_sft_local'\n",
        "\n",
        "!llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --dataset $DATASET_NAME_SFT \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --output_dir $OUTPUT_PATH_SFT \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-5 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 \\\n",
        "    --quantization_bit 4 \\\n",
        "    --double_quantization True \\\n",
        "    --quantization_type nf4 \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --lora_target all"
      ],
      "metadata": {
        "id": "4i49LsIx2y6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DPO è®­ç»ƒ\n",
        "\n",
        "çŽ°åœ¨ï¼Œæˆ‘ä»¬åœ¨ SFT é¢„è®­ç»ƒè¿‡çš„æ¨¡åž‹åŸºç¡€ä¸Šï¼Œä½¿ç”¨å®Œæ•´çš„ DPO æ•°æ®é›†è¿›è¡Œåå¥½å¯¹é½ã€‚\n",
        "\n",
        "```bash\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "export FORCE_TORCHRUN=1\n",
        "# æ¶ˆé™¤æ—¥å¿—ä¸­çš„å¹¶è¡ŒåŒ–è­¦å‘Šï¼Œä½¿è¾“å‡ºæ›´å¹²å‡€\n",
        "export TOKENIZERS_PARALLELISM=false\n",
        "\n",
        "# DPOæ—¶ï¼Œ--model_name_or_path åº”æŒ‡å‘åŽŸå§‹åŸºç¡€æ¨¡åž‹\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "# ä½¿ç”¨ --adapter_name_or_path åŠ è½½ SFT é˜¶æ®µçš„ LoRA æƒé‡\n",
        "SFT_ADAPTER_PATH='./output/ZhihuQA_SFT'\n",
        "OUTPUT_PATH_DPO='./output/ZhihuQA_DPO'\n",
        "DATASET_NAME_DPO='zhihu_dpo_local'\n",
        "\n",
        "llamafactory-cli train \\\n",
        "    --stage dpo \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --adapter_name_or_path $SFT_ADAPTER_PATH \\\n",
        "    --dataset $DATASET_NAME_DPO \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target all \\\n",
        "    --output_dir $OUTPUT_PATH_DPO \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --learning_rate 1e-6 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 \\\n",
        "    --quantization_bit 4\n",
        "```"
      ],
      "metadata": {
        "id": "kzLioDtx2-Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DPOæ—¶ï¼Œ--model_name_or_path åº”æŒ‡å‘åŽŸå§‹åŸºç¡€æ¨¡åž‹\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "# ä½¿ç”¨ --adapter_name_or_path åŠ è½½ SFT é˜¶æ®µçš„ LoRA æƒé‡\n",
        "SFT_ADAPTER_PATH='./output/ZhihuQA_SFT'\n",
        "OUTPUT_PATH_DPO='./output/ZhihuQA_DPO'\n",
        "DATASET_NAME_DPO='zhihu_dpo_local'\n",
        "\n",
        "!FORCE_TORCHRUN=1 llamafactory-cli train \\\n",
        "    --stage dpo \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --adapter_name_or_path $SFT_ADAPTER_PATH \\\n",
        "    --dataset $DATASET_NAME_DPO \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target all \\\n",
        "    --output_dir $OUTPUT_PATH_DPO \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-6 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --fp16 \\\n",
        "    --quantization_bit 4 \\\n",
        "    --gradient_checkpointing"
      ],
      "metadata": {
        "id": "xBx3-tF52_Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hugging Face TRL"
      ],
      "metadata": {
        "id": "2BhC_J_sTg6z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtsCoTBYRntE"
      },
      "outputs": [],
      "source": [
        "# train_dpo_with_trl.py\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import DPOTrainer\n",
        "\n",
        "def train_dpo():\n",
        "    # --- 1. å®šä¹‰æ¨¡åž‹ã€æ•°æ®é›†å’Œè¾“å‡ºè·¯å¾„ ---\n",
        "    sft_model_id = \"./output/ZhihuQA_SFT\"\n",
        "    dataset_name = \"zhihu_dpo_local\"\n",
        "    output_dir = \"./output/ZhihuQA_DPO_TRL\"\n",
        "\n",
        "    # --- 2. é…ç½® QLoRA é‡åŒ– ---\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # --- 3. åŠ è½½æ¨¡åž‹å’Œåˆ†è¯å™¨ ---\n",
        "    model = AutoModelForCausalLM.from_pretrained(sft_model_id, quantization_config=bnb_config, device_map={\"\": 0})\n",
        "    tokenizer = AutoTokenizer.from_pretrained(sft_model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # --- 4. åŠ è½½æ•°æ®é›† ---\n",
        "    dataset = load_dataset(\"json\", data_files=f\"data/{dataset_name}.json\", split=\"train\")\n",
        "\n",
        "    # --- 5. é…ç½®è®­ç»ƒå‚æ•° ---\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=1e-6,\n",
        "        num_train_epochs=1,\n",
        "        logging_steps=5,\n",
        "        fp16=True,\n",
        "        save_strategy=\"epoch\",\n",
        "    )\n",
        "\n",
        "    # --- 6. é…ç½® LoRA ---\n",
        "    peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, target_modules=[\"q_proj\", \"v_proj\"], task_type=\"CAUSAL_LM\")\n",
        "\n",
        "    # --- 7. åˆ›å»ºå¹¶å¯åŠ¨ DPOTrainer ---\n",
        "    dpo_trainer = DPOTrainer(\n",
        "        model,\n",
        "        args=training_args,\n",
        "        beta=0.1,\n",
        "        train_dataset=dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        peft_config=peft_config,\n",
        "        dataset_map_kwargs={\"prompt\": \"prompt\", \"chosen\": \"chosen\", \"rejected\": \"rejected\"}\n",
        "    )\n",
        "\n",
        "    dpo_trainer.train()\n",
        "    print(f\"DPO è®­ç»ƒå®Œæˆï¼Œæ¨¡åž‹å·²ä¿å­˜è‡³ {output_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_dpo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¿è¡Œè„šæœ¬ python train_dpo_with_trl.pyã€‚"
      ],
      "metadata": {
        "id": "mbAyETAQ_jWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DPO æ¨¡åž‹åˆå¹¶ä¸Žæµ‹è¯•\n",
        "\n",
        "- **åˆå¹¶æ¨¡åž‹**\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "BASE_MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "ADAPTER_PATH='./output/ZhihuQA_DPO'\n",
        "EXPORT_PATH='./output/ZhihuQA_DPO_Merged'\n",
        "\n",
        "llamafactory-cli export \\\n",
        "    --model_name_or_path $BASE_MODEL_PATH \\\n",
        "    --adapter_name_or_path $ADAPTER_PATH \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --export_dir $EXPORT_PATH \\\n",
        "    --export_size 2 \\\n",
        "    --export_legacy_format False\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "B2aURA-sm9mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "ADAPTER_PATH='./output/ZhihuQA_DPO'\n",
        "EXPORT_PATH='./output/ZhihuQA_DPO_Merged'\n",
        "\n",
        "!llamafactory-cli export \\\n",
        "    --model_name_or_path $BASE_MODEL_PATH \\\n",
        "    --adapter_name_or_path $ADAPTER_PATH \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --export_dir $EXPORT_PATH \\\n",
        "    --export_size 2 \\\n",
        "    --export_legacy_format False"
      ],
      "metadata": {
        "id": "LIzhOfZVnLqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **å¯åŠ¨ Web UI èŠå¤©æµ‹è¯•**\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "MERGED_MODEL_PATH='./output/ZhihuQA_DPO_Merged'\n",
        "\n",
        "llamafactory-cli webchat \\\n",
        "    --model_name_or_path $MERGED_MODEL_PATH \\\n",
        "    --template qwen\n",
        "```"
      ],
      "metadata": {
        "id": "lFORrB8BnLEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ä»£ç æŽ¨ç†\n",
        "\n",
        "å¦‚æžœä½ æ— æ³•å¯åŠ¨ Web UIï¼Œæˆ–è€…å¸Œæœ›åœ¨ä»£ç ä¸­ç›´æŽ¥è°ƒç”¨æ¨¡åž‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ Python è„šæœ¬è¿›è¡Œäº¤äº’å¼æŽ¨ç†ã€‚"
      ],
      "metadata": {
        "id": "149px7VtoF3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inference.py\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# --- è¯·ç¡®ä¿è¿™é‡Œçš„è·¯å¾„æŒ‡å‘æ‚¨åˆå¹¶åŽçš„DPOæ¨¡åž‹ ---\n",
        "model_path = './output/ZhihuQA_DPO_Merged'\n",
        "\n",
        "print(f\"æ­£åœ¨ä»Ž '{model_path}' åŠ è½½æ¨¡åž‹å’Œåˆ†è¯å™¨...\")\n",
        "\n",
        "# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡åž‹\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"æ¨¡åž‹åŠ è½½å®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆå›žç­”ã€‚\")\n",
        "\n",
        "# æž„é€ å¯¹è¯\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"è¯·é—®ï¼Œä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ\"}\n",
        "]\n",
        "\n",
        "# ä½¿ç”¨ apply_chat_template æ ¼å¼åŒ–è¾“å…¥\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "# ç”Ÿæˆå›žç­”\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "# è§£ç å¹¶æ‰“å°\n",
        "response_ids = outputs[0][inputs.shape[-1]:]\n",
        "response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- å¯¹è¯ ---\")\n",
        "print(\"User:\", messages[0][\"content\"])\n",
        "print(\"Assistant:\", response)"
      ],
      "metadata": {
        "id": "-7r3oFy2oIeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç¬¬å››éƒ¨åˆ†ï¼šDeepSpeed ä¸Žå¤šå¡è®­ç»ƒé…ç½®"
      ],
      "metadata": {
        "id": "IA_I9ln5kD6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSeedé…ç½®"
      ],
      "metadata": {
        "id": "s9Uu9cGgoWTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DeepSpeed é…ç½®**: LLaMA-Factory åœ¨ `examples/deepspeed` ç›®å½•ä¸‹æä¾›äº†å¤šç§é…ç½®æ–‡ä»¶ï¼Œå¦‚ `ds_z3_config.json` (ZeRO Stage 3)ã€‚è¿™äº›é…ç½®é€šè¿‡ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œä½¿å¾—åœ¨æœ‰é™çš„ç¡¬ä»¶ä¸Šè®­ç»ƒæ›´å¤§çš„æ¨¡åž‹æˆä¸ºå¯èƒ½ã€‚\n",
        "\n",
        "**å¤šå¡/å¤šæœºè®­ç»ƒ**:\n",
        "\n",
        "- **å•æœºå¤šå¡**: åªéœ€ä¿®æ”¹ `CUDA_VISIBLE_DEVICES` çŽ¯å¢ƒå˜é‡ï¼Œä¾‹å¦‚ `export CUDA_VISIBLE_DEVICES=0,1`ï¼Œç„¶åŽä½¿ç”¨ `torchrun` å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚\n",
        "- **å¤šæœºå¤šå¡**: éœ€è¦åˆ›å»ºä¸€ä¸ª `hostfile` æ–‡ä»¶ï¼Œåˆ—å‡ºæ‰€æœ‰æœºå™¨çš„ IP åœ°å€å’Œ GPU æ•°é‡ï¼Œå¹¶åœ¨ `torchrun` å‘½ä»¤ä¸­æŒ‡å®šè¯¥æ–‡ä»¶ã€‚æ‰€æœ‰æœºå™¨éœ€è¦èƒ½å¤Ÿå…å¯† SSH è®¿é—®ã€‚"
      ],
      "metadata": {
        "id": "T_jEWs_Iki4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### å‡†å¤‡ DeepSpeed é…ç½®æ–‡ä»¶\n",
        "\n",
        "DeepSpeed å®˜æ–¹ä»“åº“æä¾›ç¤ºä¾‹æ–‡ä»¶ï¼š\n",
        "- GitHubï¼šhttps://github.com/microsoft/DeepSpeed\n",
        "- å¸¸è§ä½ç½®ï¼šexamples æ–‡ä»¶å¤¹é‡Œï¼Œä¾‹å¦‚ ds_config.json\n",
        "\n",
        "LLaMA-Factory å®˜æ–¹æ–‡æ¡£ä¹Ÿæœ‰æŽ¨èé…ç½®ï¼š\n",
        "- https://github.com/liuhaotian/LLaMA-Factory\n",
        "- DeepSpeed é…ç½®: LLaMA-Factory åœ¨ examples/deepspeed ç›®å½•ä¸‹æä¾›äº†å¤šç§é…ç½®æ–‡ä»¶ï¼Œå¦‚ ds_z3_config.json (ZeRO Stage 3)ã€‚è¿™äº›é…ç½®é€šè¿‡ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œä½¿å¾—åœ¨æœ‰é™çš„ç¡¬ä»¶ä¸Šè®­ç»ƒæ›´å¤§çš„æ¨¡åž‹æˆä¸ºå¯èƒ½ã€‚"
      ],
      "metadata": {
        "id": "fPr_xXeK-TXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DeepSpeed å•æœºå•å¡é…ç½®\n",
        "```json\n",
        "{\n",
        "  \"train_batch_size\": 4,\n",
        "  \"train_micro_batch_size_per_gpu\": 4,\n",
        "  \"gradient_accumulation_steps\": 1,\n",
        "  \"steps_per_print\": 50,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": 5e-5,\n",
        "      \"betas\": [0.9, 0.999],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\": 0.01\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 5e-5,\n",
        "      \"warmup_num_steps\": 100\n",
        "    }\n",
        "  },\n",
        "  \"fp16\": {\n",
        "    \"enabled\": true,\n",
        "    \"loss_scale\": 0\n",
        "  },\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 1,\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"none\"\n",
        "    },\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"none\"\n",
        "    },\n",
        "    \"overlap_comm\": false,\n",
        "    \"contiguous_gradients\": true\n",
        "  },\n",
        "  \"gradient_clipping\": 1.0,\n",
        "  \"wall_clock_breakdown\": false,\n",
        "  \"steps_per_checkpoint\": 200,\n",
        "  \"activation_checkpointing\": {\n",
        "    \"partition_activations\": false,\n",
        "    \"contiguous_memory_optimization\": true\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "è¯´æ˜Žï¼šé€‚åˆæœ¬åœ°å•å¡å¿«é€Ÿè°ƒè¯•å’Œå°æ¨¡åž‹è®­ç»ƒï¼Œä¸éœ€è¦ offloadï¼Œä¹Ÿä¸å¼€æ¿€æ´»åˆ†åŒºã€‚"
      ],
      "metadata": {
        "id": "uHVeaP_LCFav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DeepSpeed å•æœºå¤šå¡é…ç½®\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"train_batch_size\": 32,\n",
        "  \"train_micro_batch_size_per_gpu\": 4,\n",
        "  \"gradient_accumulation_steps\": 2,\n",
        "  \"steps_per_print\": 50,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": 5e-5,\n",
        "      \"betas\": [0.9, 0.999],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\": 0.01\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 5e-5,\n",
        "      \"warmup_num_steps\": 500\n",
        "    }\n",
        "  },\n",
        "  \"fp16\": {\n",
        "    \"enabled\": true,\n",
        "    \"loss_scale\": 0\n",
        "  },\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 2,\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"none\"\n",
        "    },\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"none\"\n",
        "    },\n",
        "    \"overlap_comm\": true,\n",
        "    \"contiguous_gradients\": true\n",
        "  },\n",
        "  \"gradient_clipping\": 1.0,\n",
        "  \"wall_clock_breakdown\": false,\n",
        "  \"steps_per_checkpoint\": 500,\n",
        "  \"activation_checkpointing\": {\n",
        "    \"partition_activations\": true,\n",
        "    \"contiguous_memory_optimization\": true\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "è¯´æ˜Žï¼šé€‚åˆå•æœºå¤šå¡è®­ç»ƒï¼Œå¼€å¯ ZeRO stage 2ï¼Œæ”¯æŒæ¢¯åº¦ç´¯ç§¯å’Œæ¿€æ´»æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜ã€‚\n"
      ],
      "metadata": {
        "id": "JUSqLQqnCFeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DeepSpeed å¤šæœºå¤šå¡é…ç½®\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"train_batch_size\": 128,\n",
        "  \"train_micro_batch_size_per_gpu\": 4,\n",
        "  \"gradient_accumulation_steps\": 4,\n",
        "  \"steps_per_print\": 50,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": 5e-5,\n",
        "      \"betas\": [0.9, 0.999],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\": 0.01\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 5e-5,\n",
        "      \"warmup_num_steps\": 1000\n",
        "    }\n",
        "  },\n",
        "  \"fp16\": {\n",
        "    \"enabled\": true,\n",
        "    \"loss_scale\": 0\n",
        "  },\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 3,\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"cpu\"\n",
        "    },\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"cpu\"\n",
        "    },\n",
        "    \"overlap_comm\": true,\n",
        "    \"contiguous_gradients\": true\n",
        "  },\n",
        "  \"gradient_clipping\": 1.0,\n",
        "  \"wall_clock_breakdown\": true,\n",
        "  \"steps_per_checkpoint\": 1000,\n",
        "  \"activation_checkpointing\": {\n",
        "    \"partition_activations\": true,\n",
        "    \"contiguous_memory_optimization\": true\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "è¯´æ˜Žï¼šé€‚åˆå¤šæœºåˆ†å¸ƒå¼å¤§æ¨¡åž‹è®­ç»ƒï¼Œå¼€å¯ ZeRO stage 3ï¼Œå‚æ•°å’Œä¼˜åŒ–å™¨ offload åˆ° CPU/NVMeï¼Œå¼€å¯é€šä¿¡é‡å å’Œè¯¦ç»†æ—¶é—´ç»Ÿè®¡ã€‚\n",
        "\n",
        "\n",
        "| é…ç½®   | train\\_batch\\_size | micro\\_batch | ZeRO stage | Offload | æ¿€æ´»æ£€æŸ¥ç‚¹ | å•æœºå•å¡å¯ç”¨  |\n",
        "| ---- | ------------------ | ------------ | ---------- | ------- | ----- | ------- |\n",
        "| å•æœºå•å¡ | 4                  | 4            | 1          | none    | false | âœ…       |\n",
        "| å•æœºå¤šå¡ | 32                 | 4            | 2          | none    | true  | âœ…       |\n",
        "| å¤šæœºå¤šå¡ | 128                | 4            | 3          | cpu     | true  | âŒï¼ˆå¤šæœºå¿…ç”¨ï¼‰ |\n"
      ],
      "metadata": {
        "id": "sCvvaKvCCFg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DeepSeed é…ç½®å‚æ•°"
      ],
      "metadata": {
        "id": "c2eYijhqCk8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| å­—æ®µ                                                        | é»˜è®¤å€¼          | å¯é€‰å€¼             | è¯´æ˜Ž                  | é€šç”¨å»ºè®®                         |\n",
        "| --------------------------------------------------------- | ------------ | --------------- | ------------------- | ---------------------------- |\n",
        "| `train_batch_size`                                        | 16           | æ­£æ•´æ•°             | æ‰€æœ‰ GPU ç´¯ç§¯ batch     | æŒ‰æ˜¾å­˜å’Œå¡æ•°è°ƒèŠ‚ï¼Œå•æœºå•å¡å¯ä»¥å°ä¸€ç‚¹           |\n",
        "| `train_micro_batch_size_per_gpu`                          | 4            | 1\\~16           | æ¯å¼  GPU çš„ batch size | æ˜¾å­˜å¤Ÿå¤§å¯å¢žåŠ ï¼ŒæŽ¨è 1\\~4              |\n",
        "| `gradient_accumulation_steps`                             | 4            | 1\\~16           | æ¢¯åº¦ç´¯ç§¯æ­¥æ•°              | ç”¨äºŽå¢žåŠ æœ‰æ•ˆ batch size            |\n",
        "| `steps_per_print`                                         | 50           | 10\\~100         | æ—¥å¿—æ‰“å°é¢‘çŽ‡              | å•æœº 10\\~50 è¶³å¤Ÿ                 |\n",
        "| `optimizer.type`                                          | AdamW        | Adam, SGD       | ä¼˜åŒ–å™¨ç±»åž‹               | AdamW é€šç”¨å¾®è°ƒ                   |\n",
        "| `optimizer.params.lr`                                     | 5e-5         | 1e-6\\~5e-4      | å­¦ä¹ çŽ‡                 | å°æ¨¡åž‹å¯ç•¥å¤§ï¼Œå¤§æ¨¡åž‹ä¿æŒ 1e-5\\~5e-5      |\n",
        "| `optimizer.params.betas`                                  | \\[0.9,0.999] | -               | AdamW å‚æ•°            | é€šç”¨é»˜è®¤                         |\n",
        "| `optimizer.params.eps`                                    | 1e-8         | -               | AdamW å‚æ•°            | é€šç”¨é»˜è®¤                         |\n",
        "| `optimizer.params.weight_decay`                           | 0.01         | 0\\~0.1          | AdamW å‚æ•°            | é¿å…è¿‡æ‹Ÿåˆ                        |\n",
        "| `scheduler.type`                                          | WarmupLR     | Cosine, Linear  | å­¦ä¹ çŽ‡è°ƒåº¦               | WarmupLR é€šç”¨                  |\n",
        "| `scheduler.params.warmup_min_lr`                          | 0            | â‰¥0              | åˆå§‹å­¦ä¹ çŽ‡               | é»˜è®¤ 0                         |\n",
        "| `scheduler.params.warmup_max_lr`                          | 5e-5         | 0\\~1e-3         | æœ€å¤§å­¦ä¹ çŽ‡               | æŒ‰ lr è°ƒæ•´                      |\n",
        "| `scheduler.params.warmup_num_steps`                       | 500          | 100\\~2000       | Warmup æ­¥æ•°           | å°æ•°æ®é›† 100~~500ï¼Œå¤§æ•°æ®é›† 500~~2000 |\n",
        "| `fp16.enabled`                                            | true         | true/false      | æ˜¯å¦å¼€å¯æ··åˆç²¾åº¦            | èŠ‚çœæ˜¾å­˜ï¼ŒåŠ é€Ÿè®­ç»ƒ                    |\n",
        "| `fp16.loss_scale`                                         | 0            | æ­£æ•´æ•°             | åŠ¨æ€ loss scale       | 0 è‡ªåŠ¨å³å¯                       |\n",
        "| `fp16.loss_scale_window`                                  | 1000         | -               | è°ƒæ•´çª—å£                | é€šç”¨é»˜è®¤                         |\n",
        "| `fp16.hysteresis`                                         | 2            | -               | åŠ¨æ€ loss scale       | é€šç”¨é»˜è®¤                         |\n",
        "| `fp16.min_loss_scale`                                     | 1            | -               | æœ€å° loss scale       | é€šç”¨é»˜è®¤                         |\n",
        "| `zero_optimization.stage`                                 | 2            | 0\\~3            | ZeRO ä¼˜åŒ–é˜¶æ®µ           | å•æœºå•å¡å¯ 1 æˆ– 2ï¼Œå¤šå¡å¯ 3            |\n",
        "| `zero_optimization.offload_optimizer.device`              | none         | cpu, nvme, none | ä¼˜åŒ–å™¨ offload         | å¤šæœºå¤§æ¨¡åž‹å¿…å¼€                      |\n",
        "| `zero_optimization.offload_param.device`                  | none         | cpu, nvme, none | å‚æ•° offload          | å¤šæœºå¤šå¡å¯å¼€                       |\n",
        "| `zero_optimization.overlap_comm`                          | true         | true/false      | é€šä¿¡å’Œè®¡ç®—é‡å              | å¤šæœºå¿…å¼€ï¼Œå•æœºå¯å…³                    |\n",
        "| `zero_optimization.contiguous_gradients`                  | true         | true/false      | æ¢¯åº¦è¿žç»­å†…å­˜              | é€šç”¨ä¿æŒ true                    |\n",
        "| `gradient_clipping`                                       | 1.0          | â‰¥0              | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸              | 1.0 è¶³å¤Ÿ                       |\n",
        "| `wall_clock_breakdown`                                    | false        | true/false      | æ‰“å°è¯¦ç»†æ—¶é—´              | å•æœº False å°±å¤Ÿ                  |\n",
        "| `steps_per_checkpoint`                                    | 500          | æ­£æ•´æ•°             | æ¯å¤šå°‘æ­¥ä¿å­˜ checkpoint   | æ•°æ®é‡å°å¯è°ƒå°                      |\n",
        "| `activation_checkpointing.partition_activations`          | true         | true/false      | æ¿€æ´»æ£€æŸ¥ç‚¹               | å¤§æ¨¡åž‹å¿…å¼€                        |\n",
        "| `activation_checkpointing.contiguous_memory_optimization` | true         | true/false      | å†…å­˜ä¼˜åŒ–                | é€šç”¨ä¿æŒ True                    |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "å•æœºå•å¡ï¼šåªä¼šåœ¨ä¸€å¼  GPU ä¸Šè®­ç»ƒï¼Œmicro batch å°ï¼ŒZeRO stage å¯é™ä¸º 1ã€‚\n",
        "\n",
        "å•æœºå¤šå¡ï¼štrain_micro_batch_size_per_gpu Ã— GPU æ•° = train_batch_sizeã€‚å¼€å¯ ZeRO stage 2 æˆ– 3ï¼Œoffload å¯é€‰ CPU/NVMeã€‚\n",
        "\n",
        "å¤šæœºå¤šå¡ï¼šDeepSpeed å¯åŠ¨å‘½ä»¤åŠ  --num_nodesã€--num_gpusï¼ŒZeRO stage 2~3ï¼Œoffload å‚æ•°å¿…å¼€ã€‚"
      ],
      "metadata": {
        "id": "j8iii0NU_OzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA-Factory + DeepSpeed å¯åŠ¨å‘½ä»¤æ¨¡æ¿"
      ],
      "metadata": {
        "id": "DZduKbVh-VyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepSpeed + PyTorch + LLaMA-Factory éƒ½æ”¯æŒ TensorBoard æœ¬åœ°å¯è§†åŒ–ï¼š\n",
        "\n",
        "```bash\n",
        "pip install tensorboard\n",
        "```\n",
        "\n",
        "- è®­ç»ƒè„šæœ¬ä¸­åŠ å‚æ•°ï¼š\n",
        "\n",
        "```bash\n",
        "--report_to tensorboard\n",
        "```\n",
        "\n",
        "ç„¶åŽå¯åŠ¨ TensorBoardï¼š\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir ./output/Zhenhuan_Style_SFT\n",
        "```\n",
        "\n",
        "\n",
        "åœ¨æµè§ˆå™¨ä¸­è®¿é—® http://localhost:6006 å³å¯çœ‹åˆ°è®­ç»ƒæ›²çº¿ã€lossã€lr ç­‰æŒ‡æ ‡ã€‚\n"
      ],
      "metadata": {
        "id": "1ZfU6BtPFSao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### è®­ç»ƒå‚æ•°"
      ],
      "metadata": {
        "id": "WSEakS3-8oPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### çŽ¯å¢ƒå˜é‡å‚æ•°\n",
        "\n",
        "| å‚æ•°/å˜é‡å            | ä½œç”¨                   | å¤‡æ³¨                                             |\n",
        "| ---------------------- | ---------------------- | ------------------------------------------------ |\n",
        "| `CUDA_VISIBLE_DEVICES` | æŒ‡å®šä½¿ç”¨çš„ GPU ç´¢å¼•    | ä»Ž 0 å¼€å§‹ã€‚0 è¡¨ç¤ºä½¿ç”¨ç¬¬ä¸€å¼ å¡ã€‚                  |\n",
        "| `FORCE_TORCHRUN`       | å¼ºåˆ¶ä½¿ç”¨ torchrun å¯åŠ¨ | è§£å†³ LLaMA-Factory ä¸­ DeepSpeed çš„å¯åŠ¨æ£€æŸ¥é—®é¢˜ã€‚ |\n",
        "| `MODEL_PATH`           | åŸºç¡€æ¨¡åž‹çš„è·¯å¾„æˆ– ID    | å¯ä¸º HuggingFace Hubã€ModelScope ID æˆ–æœ¬åœ°è·¯å¾„ã€‚ |\n",
        "| `OUTPUT_PATH_SFT`      | è®­ç»ƒè¾“å‡ºç›®å½•           | å­˜æ”¾ checkpointsã€é€‚é…å™¨æƒé‡å’Œæ—¥å¿—ã€‚             |\n",
        "| `DATASET_NAME_SFT`     | ä½¿ç”¨çš„æ•°æ®é›†åç§°       | å¿…é¡»æ˜¯ `dataset_info.json` ä¸­æ³¨å†Œè¿‡çš„åç§°ã€‚      |\n",
        "\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "F1DyncvzAIcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### æ ¸å¿ƒå‚æ•°\n",
        "\n",
        "| å‚æ•°                     | ä½œç”¨             | å¤‡æ³¨                                  |\n",
        "| ------------------------ | ---------------- | ------------------------------------- |\n",
        "| `--stage sft`            | æŒ‡å®šè®­ç»ƒé˜¶æ®µ     | `sft` è¡¨ç¤ºç›‘ç£å¾®è°ƒã€‚                  |\n",
        "| `--do_train`             | æ‰§è¡Œè®­ç»ƒ         | å¿…é¡»æ·»åŠ æ­¤å‚æ•°æ‰ä¼šå¼€å§‹è®­ç»ƒã€‚          |\n",
        "| `--model_name_or_path`   | æ¨¡åž‹åç§°æˆ–è·¯å¾„   | å¼•ç”¨ `$MODEL_PATH`ã€‚                  |\n",
        "| `--dataset`              | æ•°æ®é›†åç§°æˆ–è·¯å¾„ | å¼•ç”¨ `$DATASET_NAME_SFT`ã€‚            |\n",
        "| `--template qwen`        | å¯¹è¯æ¨¡æ¿         | ç¡®ä¿ä¸ŽåŸºç¡€æ¨¡åž‹ï¼ˆå¦‚ Qwenï¼‰çš„æ ¼å¼ä¸€è‡´ã€‚ |\n",
        "| `--finetuning_type lora` | å¾®è°ƒç±»åž‹         | `lora` æ˜¯æœ€å¸¸ç”¨çš„é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚       |\n",
        "| `--output_dir`           | è¾“å‡ºç›®å½•         | å¼•ç”¨ `$OUTPUT_PATH_SFT`ã€‚             |\n",
        "\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "p_oqgtit8adK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### è®­ç»ƒæŽ§åˆ¶å‚æ•°\n",
        "\n",
        "| å‚æ•°                            | ä½œç”¨             | å¤‡æ³¨                                             |\n",
        "| ------------------------------- | ---------------- | ------------------------------------------------ |\n",
        "| `--overwrite_cache`             | è¦†ç›–æ•°æ®ç¼“å­˜     | æ›´æ¢æ•°æ®é›†æˆ–æ ¼å¼åŽå»ºè®®ä½¿ç”¨ã€‚                     |\n",
        "| `--overwrite_output_dir`        | è¦†ç›–è¾“å‡ºç›®å½•     | å…è®¸è¦†ç›–ä¹‹å‰è®­ç»ƒäº§ç”Ÿçš„æ–‡ä»¶ã€‚                     |\n",
        "| `--cutoff_len`                  | æœ€å¤§åºåˆ—é•¿åº¦     | å•ä½ä¸º tokenï¼Œä¾æ®æ˜¾å­˜ä¸Žä»»åŠ¡è®¾ç½®ã€‚               |\n",
        "| `--per_device_train_batch_size` | å•å¡ batch size  | è¶Šå¤§é€šå¸¸è®­ç»ƒæ›´ç¨³å®šï¼Œå—é™äºŽæ˜¾å­˜ã€‚                 |\n",
        "| `--gradient_accumulation_steps` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•°     | æå‡ç­‰æ•ˆ batch sizeï¼Œé€‚ç”¨äºŽæ˜¾å­˜æœ‰é™æƒ…å†µã€‚        |\n",
        "| `--learning_rate`               | å­¦ä¹ çŽ‡           | QLoRA æŽ¨èèµ·ç‚¹ä¸º `1e-4`ã€‚                        |\n",
        "| `--num_train_epochs`            | è®­ç»ƒè½®æ•°         | SFT é˜¶æ®µé€šå¸¸ 1~3 è½®å³å¯ã€‚                        |\n",
        "| `--plot_loss`                   | ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿ | ä¼šç”Ÿæˆ `training_loss.png`ï¼Œä¿å­˜äºŽè¾“å‡ºç›®å½•ã€‚     |\n",
        "| `--fp16`                        | å¯ç”¨åŠç²¾åº¦è®­ç»ƒ   | ä½¿ç”¨ 16-bit æµ®ç‚¹æ•°è®­ç»ƒï¼ŒèŠ‚çœæ˜¾å­˜ã€æå‡è®­ç»ƒé€Ÿåº¦ã€‚ |\n",
        "\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "vVICdNou8ag5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### QLoRA é‡åŒ–å‚æ•°\n",
        "\n",
        "| å‚æ•°                      | ä½œç”¨     | å¤‡æ³¨                                         |\n",
        "| ------------------------- | -------- | -------------------------------------------- |\n",
        "| `--quantization_bit 4`    | é‡åŒ–ä½æ•° | 4-bit æ˜¯ QLoRA çš„æ ¸å¿ƒè®¾å®šã€‚                  |\n",
        "| `--double_quantization`   | åŒé‡é‡åŒ– | èŠ‚çœæ˜¾å­˜ï¼Œæ€§èƒ½å½±å“å°ï¼ŒæŽ¨èå¼€å¯ã€‚             |\n",
        "| `--quantization_type nf4` | é‡åŒ–ç±»åž‹ | `nf4 (Normal Float 4)` æ˜¯æŽ¨èçš„ 4-bit ç±»åž‹ã€‚ |\n",
        "\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "DJ1B-dih8aj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### LoRA å‚æ•°\n",
        "\n",
        "| å‚æ•°             | ä½œç”¨              | å¤‡æ³¨                                              |\n",
        "| ---------------- | ----------------- | ------------------------------------------------- |\n",
        "| `--lora_rank`    | LoRA çŸ©é˜µç§© (r)   | å†³å®šå¯è®­ç»ƒå‚æ•°é‡ï¼Œå¸¸ç”¨å€¼ï¼š8, 16, 32, 64ã€‚         |\n",
        "| `--lora_alpha`   | LoRA ç¼©æ”¾å› å­     | é€šå¸¸è®¾ç½®ä¸º `lora_rank` çš„ 2 å€ã€‚                  |\n",
        "| `--lora_dropout` | LoRA Dropout æ¯”ä¾‹ | å¸¸è®¾ä¸º 0.05 æˆ– 0.1ï¼Œç”¨äºŽé˜²æ­¢è¿‡æ‹Ÿåˆã€‚              |\n",
        "| `--lora_target`  | LoRA ç›®æ ‡å±‚       | **æŽ¨èåšæ³•**ï¼šå¯¹äºŽ LLaMA-Factoryï¼Œå¯ä»¥ç›´æŽ¥è®¾ç½®ä¸º allï¼Œæ¡†æž¶ä¼šè‡ªåŠ¨è¯†åˆ«å¹¶é€‰æ‹©æ¨¡åž‹ä¸­æ‰€æœ‰å¯ç”¨çš„çº¿æ€§å±‚ï¼ˆå¦‚ q_proj, v_proj, k_proj, o_proj, gate_proj ç­‰ï¼‰ï¼Œè¿™æ˜¯æœ€çœå¿ƒä¸”æ•ˆæžœé€šå¸¸æœ€å¥½çš„é€‰æ‹©ã€‚ |"
      ],
      "metadata": {
        "id": "YhBTgwDP8amg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å•æœºå•å¡å¯ä»¥ç›´æŽ¥è¿è¡Œä¸Šé¢çš„è„šæœ¬ï¼ŒDeepSpeed ä¼šè‡ªåŠ¨ç®¡ç†å†…å­˜å’Œä¼˜åŒ–ã€‚\n",
        "\n",
        "å¦‚æžœä»¥åŽæƒ³å¤šæœºï¼Œåªéœ€åœ¨å‘½ä»¤é‡ŒåŠ ä¸Šï¼š\n",
        "\n",
        "```bash\n",
        "--hostfile ./my_hostfile\n",
        "```\n",
        "\n",
        "å¹¶åœ¨ hostfile ä¸­åˆ—å‡ºå„æœºå™¨ IP å’Œ GPU æ•°é‡ï¼ŒDeepSpeed ä¼šè‡ªåŠ¨åˆ†å¸ƒå¼è®­ç»ƒã€‚\n"
      ],
      "metadata": {
        "id": "p1EaFGvwANM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### hostfile.txt ç¤ºä¾‹ï¼ˆå•æœºå•å¡ï¼‰\n",
        "\n",
        "```\n",
        "localhost slots=1\n",
        "```\n",
        "\n",
        "- **localhost**ï¼šæœ¬æœºåç§°æˆ– IP åœ°å€\n",
        "- **slots=1**ï¼šæœ¬æœºå¯ç”¨ GPU æ•°é‡ï¼Œè¿™é‡Œæ˜¯ 1 å¼  GPU\n",
        "\n",
        "å¦‚æžœä½ æœ‰å¤šå¼  GPUï¼Œå¯ä»¥å†™æˆï¼š\n",
        "\n",
        "```\n",
        "localhost slots=4\n",
        "```\n",
        "\n",
        "è¡¨ç¤ºæœ¬æœºæœ‰ 4 å¼  GPU å¯ç”¨ï¼ŒDeepSpeed ä¼šåœ¨ 4 å¼ å¡ä¸Šåˆ†å¸ƒè®­ç»ƒä»»åŠ¡ã€‚\n"
      ],
      "metadata": {
        "id": "Uo_tQNgGAxry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### hostfile.txt ç¤ºä¾‹ï¼ˆå¤šæœºå¤šå¡ï¼‰\n",
        "\n",
        "å‡è®¾æœ‰ä¸¤å°æœºå™¨ï¼š\n",
        "\n",
        "- æœºå™¨ Aï¼ŒIP: 192.168.1.10ï¼Œæœ‰ 4 å¼  GPU\n",
        "- æœºå™¨ Bï¼ŒIP: 192.168.1.11ï¼Œæœ‰ 2 å¼  GPU\n",
        "\n",
        "hostfile å¯ä»¥å†™æˆï¼š\n",
        "\n",
        "```\n",
        "192.168.1.10 slots=4\n",
        "192.168.1.11 slots=2\n",
        "```\n",
        "\n",
        "- DeepSpeed ä¼šè‡ªåŠ¨æŠŠ batch å’Œè®¡ç®—ä»»åŠ¡æŒ‰ GPU æ•°é‡åˆ†é…\n",
        "- åªéœ€åœ¨ **ä¸€å°æœºå™¨ä¸Šè¿è¡Œè®­ç»ƒå‘½ä»¤**ï¼ŒDeepSpeed ä¼šé€šè¿‡ SSH è°ƒç”¨å…¶ä»–æœºå™¨"
      ],
      "metadata": {
        "id": "kAimIuSQAxuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### æ³¨æ„äº‹é¡¹\n",
        "\n",
        "1. **å•æœºå•å¡**ï¼šhostfile å¯ä»¥çœç•¥ï¼Œç›´æŽ¥åœ¨æœ¬æœºè·‘å³å¯\n",
        "2. **å¤šæœºè®­ç»ƒ**ï¼š\n",
        "   - ç¡®ä¿æ‰€æœ‰æœºå™¨èƒ½é€šè¿‡ SSH å…å¯†è®¿é—®\n",
        "   - æ‰€æœ‰æœºå™¨éƒ½å®‰è£…å¥½ç›¸åŒç‰ˆæœ¬çš„ Pythonã€ä¾èµ–å’Œ PyTorch/DeepSpeed\n",
        "3. **slots å’Œ batch size**ï¼šæ¯å¼  GPU batch size * slots = å…¨å±€ batch size\n",
        "4. **å•æœºå¤šå¡**ï¼šhostfile ä¹Ÿå¯ä»¥å†™ localhost slots=4 è¿™æ · DeepSpeed ä¼šæŒ‰ GPU åˆ†é…"
      ],
      "metadata": {
        "id": "2wL7wcjeAPJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç¬¬äº”éƒ¨åˆ†ï¼šGRPO å¾®è°ƒ\n",
        "\n"
      ],
      "metadata": {
        "id": "FACMjWXFgQs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### GRPO (Generative Replay Policy Optimization)\n",
        "\n",
        "**ç›®æ ‡**ï¼šæå‡æ¨¡åž‹çš„æŽ¨ç†èƒ½åŠ›å’Œäº‹å®žå‡†ç¡®æ€§ã€‚GRPO æ˜¯ä¸€ç§æ–°é¢–çš„å¯¹é½ç®—æ³•ï¼Œå®ƒé€šè¿‡è®©æ¨¡åž‹é‡æ–°ç”Ÿæˆå¹¶è¯„ä¼°è‡ªå·±çš„ç­”æ¡ˆæ¥è¿›è¡Œå­¦ä¹ å’Œä¼˜åŒ–ï¼Œç‰¹åˆ«é€‚åˆæ•°å­¦è®¡ç®—ã€ä»£ç ç”Ÿæˆã€äº‹å®žé—®ç­”ç­‰éœ€è¦é«˜ç²¾åº¦çš„ä»»åŠ¡ã€‚\n",
        "\n",
        "**æœ¬ç« æ–¹æ¡ˆ**ï¼šæˆ‘ä»¬å°†é‡‡ç”¨ `TRL` å’Œ `EasyR1` çš„åŽŸç”Ÿå·¥ä½œæµã€‚é¦–å…ˆåœ¨æœ¬ç« å†…å‡†å¤‡ `GSM8K` æ•°æ®æ–‡ä»¶ï¼Œç„¶åŽä½¿ç”¨ `TRL` åº“ç¼–å†™è„šæœ¬å®Œæˆ SFT é¢„å¤‡è®­ç»ƒï¼Œæœ€åŽåˆ‡æ¢åˆ° `EasyR1` æ¡†æž¶æ¥æ‰§è¡Œä¸“ä¸šçš„ GRPO è®­ç»ƒã€‚"
      ],
      "metadata": {
        "id": "sQoC-MMtrUH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GSM8K æ•°æ®é›†å‡†å¤‡\n",
        "\n",
        "åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸º GRPO å‡†å¤‡ `GSM8K` æ•°æ®é›†æ–‡ä»¶ã€‚"
      ],
      "metadata": {
        "id": "ocrwZvQWzF1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file: prepare_gsm8k_for_grpo.py\n",
        "import json\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. é…ç½®ä¸Žè¾…åŠ©å‡½æ•° ---\n",
        "\n",
        "# å®šä¹‰ç³»ç»Ÿæç¤ºï¼Œè¿™æ˜¯æˆ‘ä»¬å¸Œæœ›æ¨¡åž‹éµå¾ªçš„æŒ‡ä»¤ï¼Œç¡®ä¿è¾“å‡ºæ ¼å¼ç»Ÿä¸€\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "# å®šä¹‰ä¸€ä¸ªâ€œæ€ç»´é“¾â€(Chain-of-Thought)ç¤ºä¾‹ï¼Œç”¨äºŽç»™æ¨¡åž‹ä¸€ä¸ªæ¸…æ™°çš„æ¨¡ä»¿æ ·æœ¬\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "The single-digit numbers are 1, 2, 3, 4, 5, 6, 7, 8, 9. A prime number is a number greater than 1 that has no positive divisors other than 1 and itself. 7 is a prime number. 9 is divisible by 3. 8 is divisible by 2.\n",
        "</reasoning>\n",
        "<answer>\n",
        "7\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def ensure_dir_exists(path):\n",
        "    \"\"\"ç¡®ä¿æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•å­˜åœ¨ã€‚\"\"\"\n",
        "    dir_name = os.path.dirname(path)\n",
        "    if dir_name and not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "        print(f\"åˆ›å»ºç›®å½•: {dir_name}\")\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    \"\"\"\n",
        "    ä»ŽåŽŸå§‹ç­”æ¡ˆæ–‡æœ¬ï¼ˆä¾‹å¦‚ '...è¿‡ç¨‹... #### 7'ï¼‰ä¸­ç¨³å¥åœ°æå–æœ€ç»ˆçš„çº¯æ•°å­—ç­”æ¡ˆã€‚\n",
        "    \"\"\"\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    # åˆ†å‰²å­—ç¬¦ä¸²å¹¶å–æœ€åŽä¸€éƒ¨åˆ†ï¼ŒåŽ»é™¤å¯èƒ½çš„å‰åŽç©ºæ ¼\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "# --- 2. æ ¸å¿ƒæ ¼å¼åŒ–å‡½æ•° ---\n",
        "\n",
        "def format_gsm8k_for_grpo(dataset, output_path, use_one_shot=True):\n",
        "    \"\"\"\n",
        "    åŠ è½½åŽŸå§‹ GSM8K æ•°æ®é›†ï¼Œå°†å…¶å®Œå…¨æ ¼å¼åŒ–ä¸º GRPO è®­ç»ƒæ‰€éœ€çš„æœ€ç»ˆæ ¼å¼ï¼Œå¹¶ä¿å­˜åˆ° JSONL æ–‡ä»¶ã€‚\n",
        "\n",
        "    æœ€ç»ˆæ ¼å¼: {\"prompt\": [{\"role\": \"system\", ...}, {\"role\": \"user\", ...}], \"answer\": \"...\"}\n",
        "    \"\"\"\n",
        "    ensure_dir_exists(output_path)\n",
        "\n",
        "    print(f\"å¼€å§‹è¿›è¡Œå®Œæ•´çš„æ ¼å¼åŒ–å¤„ç†å¹¶ä¿å­˜åˆ° {output_path}...\")\n",
        "\n",
        "    processed_count = 0\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡\n",
        "        for item in tqdm(dataset, desc=\"æ ¼å¼åŒ– GSM8K æ•°æ®\"):\n",
        "            # 1. æž„å»ºèŠå¤©æ ¼å¼çš„ prompt åˆ—è¡¨ï¼Œé¦–å…ˆæ˜¯ç³»ç»Ÿæç¤º\n",
        "            prompt_list = [{'role': 'system', 'content': SYSTEM_PROMPT}]\n",
        "\n",
        "            # 2. (å¯é€‰) æ·»åŠ  one-shot ç¤ºä¾‹æ¥å¼•å¯¼æ¨¡åž‹å­¦ä¹ æ ¼å¼\n",
        "            if use_one_shot:\n",
        "                prompt_list.extend([\n",
        "                    {'role': 'user', 'content': 'What is the largest single-digit prime number?'},\n",
        "                    {'role': 'assistant', 'content': XML_COT_FORMAT}\n",
        "                ])\n",
        "\n",
        "            # 3. æ·»åŠ å½“å‰æ ·æœ¬çš„å®žé™…é—®é¢˜ (åŽŸå§‹é”®åä¸º 'question')\n",
        "            prompt_list.append({'role': 'user', 'content': item['question']})\n",
        "\n",
        "            # 4. æå–çº¯å‡€ç­”æ¡ˆ (åŽŸå§‹é”®åä¸º 'answer')\n",
        "            answer_text = item.get('answer', '')\n",
        "            extracted_answer = extract_hash_answer(answer_text)\n",
        "\n",
        "            # 5. åªå†™å…¥åŒ…å«æœ‰æ•ˆç­”æ¡ˆçš„æ ·æœ¬ï¼Œç¡®ä¿æ•°æ®è´¨é‡\n",
        "            if extracted_answer is not None:\n",
        "                formatted_item = {\n",
        "                    \"prompt\": prompt_list,\n",
        "                    \"answer\": extracted_answer\n",
        "                }\n",
        "                f.write(json.dumps(formatted_item, ensure_ascii=False) + '\\n')\n",
        "                processed_count += 1\n",
        "\n",
        "    print(f\"âœ… å¤„ç†å®Œæˆï¼å…± {processed_count} æ¡æœ‰æ•ˆæ ·æœ¬å·²ä¿å­˜åˆ°: {output_path}\")\n",
        "\n",
        "\n",
        "print(\"æ­£åœ¨ä»Ž Hugging Face Hub åŠ è½½ gsm8k æ•°æ®é›†...\")\n",
        "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "format_gsm8k_for_grpo(gsm8k_dataset, \"data/gsm8k_grpo_local.jsonl\")\n"
      ],
      "metadata": {
        "id": "6N3oZ1kpzW8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¿è¡Œ python prepare_gsm8k_for_grpo.py æ¥ç”Ÿæˆ GRPO è®­ç»ƒæ‰€éœ€çš„æ•°æ®æ–‡ä»¶ã€‚"
      ],
      "metadata": {
        "id": "rNXkYR9vzeYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ä½¿ç”¨ EasyR1 æ‰§è¡Œ GRPO è®­ç»ƒ\n",
        "\n",
        "çŽ°åœ¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ `EasyR1` æ¡†æž¶åœ¨ SFT çš„åŸºç¡€ä¸Šè¿›è¡Œ GRPO è®­ç»ƒã€‚"
      ],
      "metadata": {
        "id": "GedCp_DvzxF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### å®‰è£… EasyR1"
      ],
      "metadata": {
        "id": "oaWDo4gVz0cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(work_dir)\n",
        "print(f\"ðŸ“‚ å½“å‰å·¥ä½œç›®å½•å·²åˆ‡æ¢åˆ°: {work_dir.resolve()}\")"
      ],
      "metadata": {
        "id": "L-XYPTufzuUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # å…‹éš† EasyR1 ä»“åº“\n",
        "# !git clone https://github.com/hiyouga/EasyR1.git\n",
        "\n",
        "# # å®‰è£… EasyR1 åŠå…¶ä¾èµ–\n",
        "# %cd EasyR1\n",
        "# !pip install -e ."
      ],
      "metadata": {
        "id": "Z2tHr0ja0T1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_PATH=\"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "\n",
        "# !python -m verl.trainer.main \\\n",
        "#     config=examples/config.yaml \\\n",
        "#     worker.actor.model.model_path=${MODEL_PATH} \\\n",
        "#     dataset.train_path=data/gsm8k_grpo_local.jsonl\n"
      ],
      "metadata": {
        "id": "ryke00pBQXW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ‰§è¡Œ GRPO è®­ç»ƒ (ä½¿ç”¨ TRL è‡ªå®šä¹‰è„šæœ¬)\n",
        "\n",
        "åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„ Python è„šæœ¬ `train_grpo.py`ï¼Œå®ƒå°†å®žçŽ° GRPO çš„è®­ç»ƒå¾ªçŽ¯ã€‚"
      ],
      "metadata": {
        "id": "IHPgxTfJRDdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file: train_grpo.py\n",
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# --- 1. åŸºæœ¬è®¾ç½®å’Œæ—¥å¿—è®°å½• ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- æ–°å¢žï¼šçŽ¯å¢ƒè¯Šæ–­æ£€æŸ¥ ---\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"âœ… CUDA is available. Found {torch.cuda.device_count()} GPU(s).\")\n",
        "    logger.info(f\"Current device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    logger.warning(\"âš ï¸ CUDA is not available. Training will run on CPU. Please check your NVIDIA driver and PyTorch installation.\")\n",
        "\n",
        "# --- 2. é€šè¿‡çŽ¯å¢ƒå˜é‡è¿›è¡Œé…ç½® ---\n",
        "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen/Qwen1.5-0.5B-Chat\")\n",
        "# æŒ‡å‘ç”± prepare_gsm8k_for_grpo.py ç”Ÿæˆçš„ã€å·²å®Œå…¨æ ¼å¼åŒ–å¥½çš„æ•°æ®é›†\n",
        "DATASET_PATH = os.getenv(\"DATASET_PATH\", \"data/gsm8k_grpo_local.jsonl\")\n",
        "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"outputs/GSM8K_GRPO_Final\")\n",
        "RUN_NAME = os.getenv(\"RUN_NAME\", \"grpo-qwen1.5-0.5b-gsm8k-final\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "logger.info(f\"æ¨¡åž‹ ID: {MODEL_NAME}\")\n",
        "logger.info(f\"æ•°æ®é›†è·¯å¾„: {DATASET_PATH}\")\n",
        "logger.info(f\"è¾“å‡ºç›®å½•: {OUTPUT_DIR}\")\n",
        "logger.info(f\"è¿è¡Œåç§°: {RUN_NAME}\")\n",
        "\n",
        "# --- 3. æ•°æ®é›†åŠ è½½ ---\n",
        "# æ•°æ®å·²ç”±é¢„å¤„ç†è„šæœ¬å®Œå…¨æ ¼å¼åŒ–ï¼Œæ­¤å¤„ç›´æŽ¥åŠ è½½å³å¯ã€‚\n",
        "try:\n",
        "    logger.info(f\"æ­£åœ¨ä»Žé¢„å¤„ç†æ–‡ä»¶åŠ è½½æ•°æ®é›†: {DATASET_PATH}\")\n",
        "    dataset = load_dataset('json', data_files=DATASET_PATH, split='train')\n",
        "    logger.info(f\"æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œæ ·æœ¬æ•°é‡: {len(dataset)}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"åŠ è½½é¢„å¤„ç†æ•°æ®é›†å¤±è´¥: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 4. å¥–åŠ±å‡½æ•°å®šä¹‰ ---\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"ä»Žæ¨¡åž‹ç”Ÿæˆçš„ XML æ ¼å¼æ–‡æœ¬ä¸­æå– <answer> æ ‡ç­¾å†…çš„å†…å®¹ã€‚\"\"\"\n",
        "    try:\n",
        "        answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
        "        return answer\n",
        "    except IndexError:\n",
        "        logger.warning(\"Failed to extract answer from XML format.\")\n",
        "        return \"\"\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    \"\"\"ä¸»è¦å¥–åŠ±ï¼šæ ¹æ®ç­”æ¡ˆæ­£ç¡®æ€§ç»™äºˆé«˜åˆ†ã€‚\"\"\"\n",
        "    responses = [comp[0]['content'] for comp in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    if prompts and answer and responses and extracted_responses:\n",
        "        logger.info(f\"é—®é¢˜:\\n{prompts[0][-1]['content']}\\næ ‡å‡†ç­”æ¡ˆ:\\n{answer[0]}\\næ¨¡åž‹è¾“å‡º:\\n{responses[0]}\\næå–çš„ç­”æ¡ˆ:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if resp == ans else 0.0 for resp, ans in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"è¾…åŠ©å¥–åŠ±ï¼šå¦‚æžœç­”æ¡ˆæ˜¯æ•°å­—ï¼Œç»™äºˆå°‘é‡åˆ†æ•°ã€‚\"\"\"\n",
        "    responses = [comp[0]['content'] for comp in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"è¾…åŠ©å¥–åŠ±ï¼šå¦‚æžœæ ¼å¼åŸºæœ¬æ­£ç¡®ï¼Œç»™äºˆå°‘é‡åˆ†æ•°ã€‚\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [comp[0][\"content\"] for comp in completions]\n",
        "    return [0.5 if re.search(pattern, r, re.DOTALL) else 0.0 for r in responses]\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"è¾…åŠ©å¥–åŠ±ï¼šæ ¹æ® XML æ ‡ç­¾çš„å®Œæ•´æ€§ç»™äºˆç²¾ç»†åˆ†æ•°ã€‚\"\"\"\n",
        "    def count_xml(text):\n",
        "        score = 0.0\n",
        "        if \"<reasoning>\" in text: score += 0.125\n",
        "        if \"</reasoning>\" in text: score += 0.125\n",
        "        if \"<answer>\" in text: score += 0.125\n",
        "        if \"</answer>\" in text: score += 0.125\n",
        "        return score\n",
        "    contents = [comp[0][\"content\"] for comp in completions]\n",
        "    return [count_xml(c) for c in contents]\n",
        "\n",
        "# --- 5. æ¨¡åž‹å’Œ Tokenizer åŠ è½½ ---\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\" # è‡ªåŠ¨å°†æ¨¡åž‹åŠ è½½åˆ° GPU\n",
        "    )\n",
        "    logger.info(\"æ¨¡åž‹åŠ è½½æˆåŠŸã€‚\")\n",
        "\n",
        "    # --- ï¼ˆå¯é€‰ï¼‰é«˜çº§ä¼˜åŒ– ---\n",
        "    # å¦‚æžœä½ ä½¿ç”¨çš„æ˜¯ PyTorch 2.0 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¯ä»¥æžå¤§åœ°åŠ é€Ÿè®­ç»ƒ\n",
        "    # model = torch.compile(model)\n",
        "    # print(\"INFO: å·²å¯ç”¨ torch.compile() è¿›è¡ŒåŠ é€Ÿã€‚\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"åŠ è½½æ¨¡åž‹å¤±è´¥: {e}.\")\n",
        "    raise\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# --- 6. PEFT (LoRA) é…ç½® ---\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=\"all-linear\", # è‡ªåŠ¨åŒ¹é…æ‰€æœ‰çº¿æ€§å±‚ï¼Œé¿å…å‡ºé”™\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_dropout=0.05,\n",
        ")\n",
        "\n",
        "# --- 7. è®­ç»ƒå‚æ•°é…ç½® ---\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    run_name=RUN_NAME,\n",
        "    learning_rate=5e-6,\n",
        "    logging_steps=5,\n",
        "    # bf16=True,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_generations=4,\n",
        "    max_prompt_length=512,\n",
        "    max_completion_length=256,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=10,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# --- 8. åˆå§‹åŒ– GRPOTrainer ---\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config\n",
        ")\n",
        "\n",
        "# --- 9. å¼€å§‹è®­ç»ƒ ---\n",
        "logger.info(\"å¼€å§‹ GRPO è®­ç»ƒ...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    logger.info(\"è®­ç»ƒå®Œæˆï¼\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\", exc_info=True)\n",
        "    raise"
      ],
      "metadata": {
        "id": "z78V1R00R89q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æŽ¨ç†æµ‹è¯•\n"
      ],
      "metadata": {
        "id": "UcwcRarvcMA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file: inference.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "\n",
        "# --- 1. é…ç½® ---\n",
        "# åŸºç¡€æ¨¡åž‹ ID (å¿…é¡»ä¸Žè®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨¡åž‹ä¸€è‡´)\n",
        "BASE_MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen/Qwen1.5-0.5B-Chat\")\n",
        "# è®­ç»ƒæ—¶ä½¿ç”¨çš„è¾“å‡ºç›®å½• (ä¸Žä½ çš„è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´)\n",
        "OUTPUT_DIR_FROM_TRAINING = os.getenv(\"OUTPUT_DIR\", \"outputs/GSM8K_GRPO_Final\")\n",
        "\n",
        "# --- è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ checkpoint ---\n",
        "print(f\"INFO: æ­£åœ¨ '{OUTPUT_DIR_FROM_TRAINING}' ç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„ checkpoint...\")\n",
        "checkpoints = glob.glob(os.path.join(OUTPUT_DIR_FROM_TRAINING, \"checkpoint-*\"))\n",
        "if not checkpoints:\n",
        "    raise ValueError(f\"åœ¨ç›®å½• '{OUTPUT_DIR_FROM_TRAINING}' ä¸­æ‰¾ä¸åˆ°ä»»ä½• checkpointã€‚è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ä¸”è®­ç»ƒå·²ä¿å­˜ã€‚\")\n",
        "\n",
        "# æŒ‰æ•°å­—å¤§å°æŽ’åºæ‰¾åˆ°æœ€æ–°çš„ checkpoint\n",
        "latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
        "ADAPTER_PATH = latest_checkpoint\n",
        "print(f\"INFO: è‡ªåŠ¨æ‰¾åˆ°æœ€æ–°çš„ checkpoint: {ADAPTER_PATH}\")\n",
        "\n",
        "\n",
        "# --- 2. åŠ è½½æ¨¡åž‹å’Œé€‚é…å™¨ ---\n",
        "print(f\"INFO: æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡åž‹: {BASE_MODEL_NAME}...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"INFO: åŸºç¡€æ¨¡åž‹åŠ è½½æˆåŠŸã€‚\")\n",
        "\n",
        "print(f\"INFO: æ­£åœ¨åŠ è½½ Tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"INFO: æ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨: {ADAPTER_PATH}...\")\n",
        "# åŠ è½½ LoRA é€‚é…å™¨å¹¶å°†å…¶åº”ç”¨åˆ°åŸºç¡€æ¨¡åž‹ä¸Š\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "print(\"INFO: LoRA é€‚é…å™¨åŠ è½½æˆåŠŸã€‚\")\n",
        "\n",
        "# --- 3. åˆå¹¶æ¨¡åž‹ ---\n",
        "print(\"INFO: æ­£åœ¨åˆå¹¶æ¨¡åž‹å’Œé€‚é…å™¨...\")\n",
        "# å°† LoRA æ¨¡å—çš„æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡åž‹ä¸­\n",
        "model = model.merge_and_unload()\n",
        "print(\"INFO: æ¨¡åž‹åˆå¹¶å®Œæˆã€‚\")\n",
        "\n",
        "# --- 4. æŽ¨ç†å‡½æ•° ---\n",
        "# å®šä¹‰ä¸Žè®­ç»ƒæ—¶ä¸€è‡´çš„ç³»ç»Ÿæç¤º\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def generate_response(question: str):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨å¾®è°ƒåŽçš„æ¨¡åž‹ç”Ÿæˆå¯¹ç»™å®šé—®é¢˜çš„å›žç­”ã€‚\n",
        "    \"\"\"\n",
        "    # 1. æž„å»ºèŠå¤©æ ¼å¼çš„ prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "\n",
        "    # 2. åº”ç”¨èŠå¤©æ¨¡æ¿å¹¶è½¬æ¢ä¸ºè¾“å…¥å¼ é‡\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # 3. ä½¿ç”¨æ¨¡åž‹ç”Ÿæˆå›žç­”\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"é—®é¢˜: {question}\")\n",
        "    print(\"æ¨¡åž‹æ­£åœ¨ç”Ÿæˆå›žç­”...\")\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=256, # ä¸Žè®­ç»ƒæ—¶çš„ max_completion_length ä¿æŒä¸€è‡´\n",
        "        do_sample=False # ä½¿ç”¨è´ªå¿ƒè§£ç ä»¥èŽ·å¾—ç¡®å®šæ€§è¾“å‡º\n",
        "    )\n",
        "\n",
        "    # 4. è§£ç å¹¶æ¸…ç†è¾“å‡º\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    print(\"--- æ¨¡åž‹è¾“å‡º ---\")\n",
        "    print(response)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- 5. æ‰§è¡ŒæŽ¨ç† ---\n",
        "if __name__ == \"__main__\":\n",
        "    # å‡†å¤‡ä¸€äº›æµ‹è¯•é—®é¢˜\n",
        "    test_questions = [\n",
        "        \"Natalia sold 48/2 = 24 clips in the morning. Then she sold 12 clips in the afternoon. How many clips did Natalia sell in total?\",\n",
        "        \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
        "        \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
        "    ]\n",
        "\n",
        "    for q in test_questions:\n",
        "        generate_response(q)\n"
      ],
      "metadata": {
        "id": "LjXbNfDRjSwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç¬¬å…­éƒ¨åˆ†ï¼šGRPO ä»Žå…¥é—¨åˆ°å®žè·µå®Œæ•´æ•™ç¨‹\n",
        "\n",
        "## å‰è¨€\n",
        "\n",
        "æœ¬æŒ‡å—å°†å¸¦ä½ èµ°è¿‡ä¸€æ¡æ¸…æ™°çš„å­¦ä¹ è·¯å¾„ï¼Œä»Žå®è§‚æ¦‚å¿µåˆ°ç†è®ºæ·±åº¦ï¼Œæœ€ç»ˆè½äºŽä»£ç å®žè·µã€‚å­¦ä¹ è·¯å¾„å¦‚ä¸‹ï¼š\n",
        "\n",
        "1. **æ ¸å¿ƒæ€æƒ³ä¸Žå®šä½**ï¼šå¿«é€Ÿå»ºç«‹å¯¹ GRPO çš„ç›´è§‚è®¤çŸ¥ï¼Œæ˜Žç™½å®ƒæ˜¯ä»€ä¹ˆï¼Œä»¥åŠå®ƒåœ¨ä¼—å¤šå¯¹é½ç®—æ³•ä¸­çš„ä½ç½®ã€‚\n",
        "2. **æ·±å…¥åŽŸç†ä¸ŽæŽ¨å¯¼**ï¼šç†è§£ GRPO çš„æ•°å­¦å†…æ ¸ï¼Œå­¦ä¹ å…¶æŸå¤±å‡½æ•°å¦‚ä½•ä»Žç¬¬ä¸€æ€§åŽŸç†æž„å»ºã€‚\n",
        "3. **åŠ¨æ‰‹å®žè·µä¸Žä»£ç **ï¼šé€šè¿‡ç¤ºä¾‹ï¼Œå®žçŽ°è‡ªå®šä¹‰çš„ `GRPOTrainer`ã€‚\n",
        "\n",
        "------\n",
        "\n",
        "## æ ¸å¿ƒæ€æƒ³ä¸Žå®šä½\n",
        "\n",
        "**ä¸€å¥è¯ç†è§£ GRPO**ï¼š**â€œæ“’è´¼å…ˆæ“’çŽ‹â€**\n",
        "\n",
        "æƒ³è±¡è€å¸ˆæ•™å­¦ç”Ÿå†™ä½œæ–‡ï¼šç»™å‡ºä¸€ä¸ªæ ‡å‡†ç­”æ¡ˆï¼ˆchosenï¼‰å’Œå¤šä¸ªé”™è¯¯èŒƒä¾‹ï¼ˆrejectedï¼‰ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆæ ‡å‡†ç­”æ¡ˆï¼ŒåŒæ—¶é¿å¼€æœ€å®¹æ˜“æ··æ·†çš„é”™è¯¯ã€‚GRPO çš„ç²¾é«“æ˜¯ï¼š\n",
        "\n",
        "- **å…³æ³¨æœ€éš¾çš„è´Ÿæ ·æœ¬**ï¼šåœ¨ä¸€å † rejected æ ·æœ¬é‡Œï¼Œæ¨¡åž‹åªéœ€æˆ˜èƒœæœ€å¼ºçš„é‚£ä¸ªã€‚\n",
        "- **æ•´ä½“è€ƒè™‘åå¥½ç¾¤ç»„**ï¼šä¸Ž DPO åªå¤„ç†ä¸€å¯¹ä¸€ä¸åŒï¼ŒGRPO ä¸€æ¬¡æ€§è€ƒè™‘æ‰€æœ‰ rejected æ ·æœ¬ã€‚\n",
        "\n",
        "### GRPO çš„è¯žç”ŸåŠ¨æœº\n",
        "\n",
        "åœ¨çŽ°å®žæ ‡æ³¨ä¸­ï¼Œä¸€ä¸ª prompt å¾€å¾€å¯¹åº”ä¸€ä¸ªå¥½å›žç­”å’Œå¤šä¸ªåå›žç­”ã€‚GRPO çš„ç›®æ ‡æ˜¯**æ›´é«˜æ•ˆåœ°åˆ©ç”¨â€œä¸€å¯¹å¤šâ€åå¥½æ•°æ®**ï¼ŒæŠŠå­¦ä¹ é‡ç‚¹æ”¾åœ¨æœ€éš¾åŒºåˆ†çš„è´Ÿæ ·æœ¬ä¸Šã€‚\n",
        "\n",
        "### æ¨ªå‘å¯¹æ¯”\n",
        "\n",
        "| ç‰¹æ€§     | PPO                     | DPO                        | GRPO                                     |\n",
        "| -------- | ----------------------- | -------------------------- | ---------------------------------------- |\n",
        "| æ ¸å¿ƒç›®æ ‡ | æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±          | ç›´æŽ¥ä»Žåå¥½å¯¹å­¦ä¹            | ä»Žåå¥½ç¾¤ç»„å­¦ä¹ ï¼Œå¤„ç†â€œä¸€å¯¹å¤šâ€             |\n",
        "| æ•°æ®ç±»åž‹ | (state, action, reward) | (prompt, chosen, rejected) | (prompt, chosen, [rejected_1,...])       |\n",
        "| å·¥ä½œæ–¹å¼ | ä¾èµ–å¥–åŠ±æ¨¡åž‹            | è·³è¿‡å¥–åŠ±æ¨¡åž‹ï¼Œç”¨é€»è¾‘æŸå¤±   | DPO çš„æ‰©å±•ï¼Œç”¨ Hinge Loss ä¸“æ³¨æœ€å¼ºè´Ÿæ ·æœ¬ |\n",
        "| å¯¹é½æ–¹æ³• | RLHF æ ¸å¿ƒç®—æ³•           | ç›´æŽ¥å¯¹é½æ–¹æ³•ï¼Œä¸éœ€å¥–åŠ±æ¨¡åž‹ | DPO å˜ä½“ï¼Œä¸éœ€å¥–åŠ±æ¨¡åž‹                   |\n",
        "\n",
        "## GRPO æ·±å…¥åŽŸç†ä¸ŽæŽ¨å¯¼\n",
        "\n",
        "### èµ·ç‚¹ï¼šDPO çš„åŸºçŸ³\n",
        "\n",
        "è¦ç†è§£ GRPOï¼Œå¿…é¡»å…ˆç†è§£ DPO çš„é€»è¾‘ã€‚\n",
        "\n",
        "#### åå¥½é‡åŒ–ï¼ˆBradley-Terry æ¨¡åž‹ï¼‰\n",
        "\n",
        "æˆ‘ä»¬å¦‚ä½•å°†â€œäººç±»æ›´å–œæ¬¢ $A$ è€Œä¸æ˜¯ $B$â€è¿™ä¸ªæ¨¡ç³Šæ¦‚å¿µæ•°å­¦åŒ–ï¼ŸBradley-Terry æ¨¡åž‹æä¾›äº†ä¸€ä¸ªä¼˜é›…çš„æ–¹æ¡ˆã€‚å®ƒå‡è®¾æ¯ä¸ªé€‰é¡¹èƒŒåŽéƒ½æœ‰ä¸€ä¸ªæ½œåœ¨çš„â€œå¥–åŠ±åˆ†æ•°â€ $r^*$ï¼Œè€Œåå¥½æ¦‚çŽ‡å–å†³äºŽè¿™äº›åˆ†æ•°çš„å·®å¼‚ï¼š\n",
        "\n",
        "$$\n",
        "p^*(y_c \\succ y_r \\mid x) = \\sigma\\big(r^*(x, y_c) - r^*(x, y_r)\\big)\n",
        "$$\n",
        "\n",
        "è¿™ä¸ªå…¬å¼æ˜¯æ‰€æœ‰æŽ¨å¯¼çš„åŸºçŸ³ï¼Œå®ƒå°†ä¸»è§‚åå¥½è½¬åŒ–ä¸ºäº†ä¸€ä¸ªå¯è®¡ç®—çš„æ¦‚çŽ‡ã€‚\n",
        "\n",
        "#### å¥–åŠ±å®šä¹‰ï¼ˆDPO æ ¸å¿ƒæ´žå¯Ÿï¼‰\n",
        "\n",
        "ä¼ ç»Ÿæ–¹æ³•éœ€è¦è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„å¥–åŠ±æ¨¡åž‹æ¥ä¼°ç®— $r^*$ã€‚DPO çš„å¤©æ‰ä¹‹å¤„åœ¨äºŽï¼Œå®ƒå‘çŽ°å¥–åŠ±å¯ä»¥ç›´æŽ¥ç”¨ç­–ç•¥æ¨¡åž‹ $\\pi_\\theta$ å’Œå‚è€ƒæ¨¡åž‹ $\\pi_{\\text{ref}}$ çš„æ¦‚çŽ‡æ¯”å€¼æ¥å®šä¹‰ï¼š\n",
        "\n",
        "$$\n",
        "r^*(x, y) \\triangleq \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\n",
        "$$\n",
        "\n",
        "è¿™ä¸ªå®šä¹‰ä¸€ä¸¾ä¸¤å¾—ï¼šå®ƒä½¿å¾—å¥–åŠ±å˜å¾—å¯ç›´æŽ¥è®¡ç®—ï¼ŒåŒæ—¶éšå¼åœ°åŠ å…¥äº†ä¸€ä¸ª KL æ•£åº¦çº¦æŸï¼Œé˜²æ­¢ $\\pi_\\theta$ ä¸ºäº†è¿Žåˆåå¥½è€Œåç¦» $\\pi_{\\text{ref}}$ å¤ªè¿œï¼Œä»Žè€Œä¿è¯ç”Ÿæˆè´¨é‡ã€‚\n",
        "\n",
        "#### DPO æŸå¤±å‡½æ•°\n",
        "\n",
        "å°†å¥–åŠ±å®šä¹‰ä»£å…¥åå¥½æ¦‚çŽ‡å…¬å¼ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªå®Œå…¨ç”±æ¨¡åž‹æ¦‚çŽ‡è¡¨ç¤ºçš„äººç±»åå¥½æ¨¡åž‹ã€‚æœºå™¨å­¦ä¹ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–è§‚æµ‹æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ï¼Œç­‰ä»·äºŽæœ€å°åŒ–å…¶è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚å› æ­¤ï¼ŒDPO çš„æŸå¤±å‡½æ•°ä¸ºï¼š\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma\\big(\\hat{r}_\\theta(y_c) - \\hat{r}_\\theta(y_r)\\big)\n",
        "$$\n",
        "\n",
        "è¿™æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªé€»è¾‘æŸå¤± (Logistic Loss)ï¼Œå®ƒé©±åŠ¨æ¨¡åž‹åŽ»å¢žå¤§ $y_c$ çš„å¥–åŠ±ï¼ŒåŒæ—¶å‡å° $y_r$ çš„å¥–åŠ±ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### æž„æ€é£žè·ƒï¼šä»Žâ€œä¸€å¯¹ä¸€â€åˆ°â€œä¸€å¯¹å¤šâ€\n",
        "\n",
        "é‡æ–°å®šä¹‰é—®é¢˜ï¼šDPO å®Œç¾Žåœ°è§£å†³äº†â€œä¸€å¯¹ä¸€â€æ¯”è¾ƒã€‚ä½†å½“é¢å¯¹ä¸€ä¸ªâ€œå¥½â€ç­”æ¡ˆ $y_c$ å’Œä¸€ç»„â€œåâ€ç­”æ¡ˆ $Y_r$ æ—¶ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®© $y_c$ ä¼˜äºŽæ•´ä¸ªç¾¤ç»„ã€‚GRPO å¯¹æ­¤æå‡ºäº†ä¸€ä¸ªå…³é”®çš„é‡æ–°å®šä¹‰ï¼š\n",
        "\n",
        "> â€œä¼˜äºŽæ•´ä¸ªç¾¤ç»„â€ ç­‰ä»·äºŽ â€œå¥–åŠ±åˆ†æ•°é«˜äºŽç¾¤ç»„ä¸­é‚£ä¸ªåˆ†æ•°æœ€é«˜çš„æˆå‘˜â€ã€‚\n",
        "\n",
        "$$\n",
        "y_c \\succ Y_r \\iff r(y_c) > \\max_{y_r \\in Y_r} r(y_r)\n",
        "$$\n",
        "\n",
        "è¿™ä¸ªå®šä¹‰æ˜¯ GRPO çš„çµé­‚ï¼Œå®ƒå°†ä¼˜åŒ–ç›®æ ‡ä»Žå…³æ³¨æ‰€æœ‰è´Ÿæ ·æœ¬ï¼Œèšç„¦åˆ°äº†é‚£ä¸ªâ€œæœ€å¼ºçš„å¯¹æ‰‹â€ä¸Šã€‚\n",
        "\n",
        "#### å€Ÿé‰´ SVM æ€æƒ³ï¼ˆæœ€å¤§é—´éš”ï¼‰\n",
        "\n",
        "ä»…ä»…è¦æ±‚ $r(y_c)$ æ›´é«˜è¿˜ä¸å¤Ÿé²æ£’ã€‚æˆ‘ä»¬ä»Žç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•æ”¯æŒå‘é‡æœº (SVM) ä¸­å€Ÿé‰´äº†â€œæœ€å¤§é—´éš”â€æ€æƒ³ã€‚SVM ä¸ä»…è¦æ­£ç¡®åˆ†ç±»ï¼Œè¿˜è¿½æ±‚åœ¨ä¸¤ç±»æ•°æ®ä¹‹é—´ç•™å‡ºå°½å¯èƒ½å®½çš„â€œå®‰å…¨åŒºâ€ï¼ˆMarginï¼‰ã€‚\n",
        "\n",
        "ç±»æ¯”ï¼šæˆ‘ä»¬å¯ä»¥æŠŠ $y_c$ çœ‹ä½œæ­£ç±»ï¼Œæ‰€æœ‰ $y_r$ çœ‹ä½œè´Ÿç±»ã€‚é‚£ä¸ªå¥–åŠ±æœ€é«˜çš„ $y_r$ å°±å¥½æ¯”æ˜¯ç¦»å†³ç­–è¾¹ç•Œæœ€è¿‘çš„â€œæ”¯æŒå‘é‡â€ã€‚\n",
        "\n",
        "ç›®æ ‡ï¼šæˆ‘ä»¬ä¸ä»…è¦è®© $r(y_c)$ æˆ˜èƒœ $\\max r(y_r)$ï¼Œè¿˜è¦è®©å®ƒé¢†å…ˆä¸€ä¸ªå®‰å…¨é—´éš”ï¼ˆmarginï¼‰ $\\alpha$ã€‚æˆ‘ä»¬çš„â€œæˆåŠŸæ¡ä»¶â€å˜ä¸ºï¼š\n",
        "\n",
        "$$\n",
        "r(y_c) \\ge \\max_{y_r \\in Y_r} r(y_r) + \\alpha\n",
        "$$\n",
        "\n",
        "#### æž„å»º GRPO æŸå¤±å‡½æ•°ï¼ˆHinge Lossï¼‰\n",
        "\n",
        "å¦‚ä½•å°†è¿™ä¸ªå¸¦é—´éš”çš„â€œæˆåŠŸæ¡ä»¶â€è½¬åŒ–ä¸ºæŸå¤±å‡½æ•°ï¼ŸHinge Lossï¼ˆåˆé¡µæŸå¤±ï¼‰ $$\\max(0, z)$$ æ˜¯å®Œç¾Žçš„å·¥å…·ã€‚å½“æ¡ä»¶æ»¡è¶³æ—¶ï¼ŒæŸå¤±ä¸º 0ï¼›ä¸æ»¡è¶³æ—¶ï¼ŒæŸå¤±ä¸ºæ­£ã€‚\n",
        "\n",
        "å°†â€œæˆåŠŸæ¡ä»¶â€ç§»é¡¹ï¼š\n",
        "\n",
        "$$\n",
        "\\max_{y_r \\in Y_r} r(y_r) - r(y_c) + \\alpha \\le 0\n",
        "$$\n",
        "\n",
        "å½“æ‹¬å·å†…çš„å€¼ $z$ å¤§äºŽ 0 æ—¶ï¼Œè¯´æ˜Žæ¡ä»¶è¢«è¿åã€‚\n",
        "\n",
        "äºŽæ˜¯ï¼ŒGRPO çš„æœ€ç»ˆæŸå¤±å‡½æ•°ä¸ºï¼š\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{GRPO}} = \\max\\Big(0, \\max_{y_r \\in Y_r} \\hat{r}_\\theta(y_r) - \\hat{r}_\\theta(y_c) + \\alpha \\Big)\n",
        "$$\n",
        "\n",
        "è¿™é‡Œï¼Œ$\\hat{r}_\\theta(y)$å®šä¹‰ä¸ºï¼š\n",
        "\n",
        "$$\n",
        "\\hat{r}_\\theta(y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\n",
        "$$\n",
        "\n",
        "- å¥–åŠ±åˆ†æ•°æ—¢æ˜¯â€œç›¸å¯¹åå¥½åˆ†â€ï¼Œåˆéšå¼åŒ…å« KL æ•£åº¦çº¦æŸã€‚\n",
        "\n",
        "#### æœ€å¤§å€¼æ¢¯åº¦è¿‘ä¼¼ï¼ˆå¯é€‰ï¼‰\n",
        "\n",
        "ç”±äºŽ $$$\\max$$$ ä¸å¯å¯¼ï¼Œå¯ä½¿ç”¨ softmax è¿‘ä¼¼ï¼š\n",
        "\n",
        "$$\n",
        "\\max_{y_r \\in Y_r} \\hat{r}_\\theta(y_r) \\approx \\frac{\\sum_{y_r} \\hat{r}_\\theta(y_r) e^{\\lambda \\hat{r}_\\theta(y_r)}}{\\sum_{y_r} e^{\\lambda \\hat{r}_\\theta(y_r)}}\n",
        "$$\n",
        "\n",
        "- $\\lambda$ è¶Šå¤§ï¼Œè¿‘ä¼¼è¶ŠæŽ¥è¿‘çœŸæ­£çš„æœ€å¤§å€¼  \n",
        "- å¹³æ»‘æ¢¯åº¦ï¼Œè®­ç»ƒæ›´ç¨³å®š\n",
        "\n",
        "---\n",
        "\n",
        "### å®Œæ•´è®­ç»ƒæµç¨‹\n",
        "\n",
        "1. ç»™å®šè¾“å…¥ $x$ å’Œæ­£æ ·æœ¬ $y_c$ã€è´Ÿæ ·æœ¬é›†åˆ $Y_r$ã€‚  \n",
        "2. è®¡ç®—å¥–åŠ±åˆ†æ•°ï¼š\n",
        "\n",
        "$$\n",
        "\\hat{r}_\\theta(y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\n",
        "$$\n",
        "\n",
        "3. æ‰¾åˆ°æœ€å¼ºè´Ÿæ ·æœ¬ï¼š\n",
        "\n",
        "$$\n",
        "y_r^* = \\arg\\max_{y_r \\in Y_r} \\hat{r}_\\theta(y_r)\n",
        "$$\n",
        "\n",
        "4. è®¡ç®— GRPO æŸå¤±ï¼š\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{GRPO}} = \\max\\big(0, \\hat{r}_\\theta(y_r^*) - \\hat{r}_\\theta(y_c) + \\alpha \\big)\n",
        "$$\n",
        "\n",
        "5. åå‘ä¼ æ’­æ›´æ–°ç­–ç•¥æ¨¡åž‹ $\\pi_\\theta$ã€‚  \n",
        "\n",
        "è¿™æ ·ï¼ŒGRPO å°†â€œä¸€å¯¹å¤šåå¥½â€é—®é¢˜è½¬åŒ–ä¸ºå¯ä¼˜åŒ–çš„ Hinge Lossï¼ŒåŒæ—¶ä¿ç•™äº† DPO çš„ KL æ•£åº¦çº¦æŸå’Œç¨³å®šè®­ç»ƒä¼˜åŠ¿ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "u903vn_5oOAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyQ5G4VzpKsN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}