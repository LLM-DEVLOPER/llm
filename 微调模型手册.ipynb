{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88cbb58646de4eb8809af64fadac2363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63c0fa7865c14421a951f7276bbe8134",
              "IPY_MODEL_82b89cbe049646df817ce09f5d0bf06a",
              "IPY_MODEL_49fae0384f5d4005954e4bad7a4b23b7"
            ],
            "layout": "IPY_MODEL_351e025fc08b46828e13b610a62a44a1"
          }
        },
        "63c0fa7865c14421a951f7276bbe8134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a20944fcfef4381ab7b27ef6eb161fb",
            "placeholder": "​",
            "style": "IPY_MODEL_f4b9036aba5545b596346a3dcfda6458",
            "value": "README.md: 100%"
          }
        },
        "82b89cbe049646df817ce09f5d0bf06a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_367634585ad24e92986be0ee398339fe",
            "max": 27,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80d077418cdf48bc9de7fe077811538b",
            "value": 27
          }
        },
        "49fae0384f5d4005954e4bad7a4b23b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f63af71d4416475b9831660aa9c59c66",
            "placeholder": "​",
            "style": "IPY_MODEL_d6d43d3310fe47f4b8f68f99cad803ce",
            "value": " 27.0/27.0 [00:00&lt;00:00, 2.29kB/s]"
          }
        },
        "351e025fc08b46828e13b610a62a44a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a20944fcfef4381ab7b27ef6eb161fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b9036aba5545b596346a3dcfda6458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "367634585ad24e92986be0ee398339fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d077418cdf48bc9de7fe077811538b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f63af71d4416475b9831660aa9c59c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6d43d3310fe47f4b8f68f99cad803ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59f86f2fa3b940b3a4a687829467126a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b8996e673f148e8a2dbf6a028d3be1a",
              "IPY_MODEL_3cb3823108044a3784d7bf25ba833374",
              "IPY_MODEL_8a5511f15a4b469494f3c7eba9540ae6"
            ],
            "layout": "IPY_MODEL_8c76ba3cebb24d3ba110c78794a6e1da"
          }
        },
        "6b8996e673f148e8a2dbf6a028d3be1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c67452736747dd8db5fae3f53646d8",
            "placeholder": "​",
            "style": "IPY_MODEL_fd10d4fd2fb3464dab51ac40d8b4b4ca",
            "value": "zhihu_3k_rlfh.tsv: 100%"
          }
        },
        "3cb3823108044a3784d7bf25ba833374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc5129ca5e0b4af0ad73dbbc68429dcf",
            "max": 15557370,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56713605bd7e4ff39f67036e36399cc8",
            "value": 15557370
          }
        },
        "8a5511f15a4b469494f3c7eba9540ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80b2e935174543099a1d09b7e039b31a",
            "placeholder": "​",
            "style": "IPY_MODEL_b28896743cba454fad6a6a94e9a63e56",
            "value": " 15.6M/15.6M [00:00&lt;00:00, 75.1MB/s]"
          }
        },
        "8c76ba3cebb24d3ba110c78794a6e1da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4c67452736747dd8db5fae3f53646d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd10d4fd2fb3464dab51ac40d8b4b4ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc5129ca5e0b4af0ad73dbbc68429dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56713605bd7e4ff39f67036e36399cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80b2e935174543099a1d09b7e039b31a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b28896743cba454fad6a6a94e9a63e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c571c18ae4df4a1a90f2819fc5d9803f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7e8a60dc39a434a8d97f98580ff201f",
              "IPY_MODEL_98449b50d77043e7bba60e6f2e0d5b18",
              "IPY_MODEL_4931472f477d4573bf8413f2e629303d"
            ],
            "layout": "IPY_MODEL_a512cbdf8201442488b1b603459d3f3c"
          }
        },
        "b7e8a60dc39a434a8d97f98580ff201f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f714606ca0c4bd5ac0642d7defeca87",
            "placeholder": "​",
            "style": "IPY_MODEL_896eb5662e3046529d24b3a1d0d72a38",
            "value": "Generating train split: 100%"
          }
        },
        "98449b50d77043e7bba60e6f2e0d5b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0ed61da0cd845c288cbdf14c7ee278e",
            "max": 3460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b64256559f241f5ac1075da9237f11b",
            "value": 3460
          }
        },
        "4931472f477d4573bf8413f2e629303d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8318f3eb4d044269003276083e3e9c5",
            "placeholder": "​",
            "style": "IPY_MODEL_307f549df91f4f1fbc4c80721d72f6c5",
            "value": " 3460/3460 [00:00&lt;00:00, 8714.74 examples/s]"
          }
        },
        "a512cbdf8201442488b1b603459d3f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f714606ca0c4bd5ac0642d7defeca87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896eb5662e3046529d24b3a1d0d72a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0ed61da0cd845c288cbdf14c7ee278e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b64256559f241f5ac1075da9237f11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8318f3eb4d044269003276083e3e9c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "307f549df91f4f1fbc4c80721d72f6c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 微调模型手册\n",
        "\n",
        "以轻量级的 Qwen2.5-0.5B-Instruct 模型为例。"
      ],
      "metadata": {
        "id": "FctiHqA-SlAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第一部分：通用准备工作\n",
        "\n",
        "\n",
        "在开始任何微调之前，我们需要先完成环境和数据的准备。"
      ],
      "metadata": {
        "id": "_KuY0G7xTpA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 第 1 步：统一环境搭建\n",
        "一个稳定且隔离的Python环境是成功的第一步。"
      ],
      "metadata": {
        "id": "oeH29lY7T1Qb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用 Conda 可以创建一个隔离的 Python 环境，防止包版本冲突。"
      ],
      "metadata": {
        "id": "OZkl-OXQT1Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 使用 Conda\"\n",
        "# conda create -n finetune-env python=3.12\n",
        "# conda activate finetune-env"
      ],
      "metadata": {
        "id": "RmAlBKD7T7F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 第 2 步：LLaMA-Factory 部署与依赖安装"
      ],
      "metadata": {
        "id": "9cUn5EisUCiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 设置工作目录"
      ],
      "metadata": {
        "id": "YB-7vDHoqRu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. 设置你要用的子目录名\n",
        "project_dir_name = \"Workspace\"\n",
        "\n",
        "# 2. 尝试挂载 Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    base_dir = Path(\"/content/drive/MyDrive\")  # 云盘根目录\n",
        "    print(\"✅ 成功挂载 Google Drive\")\n",
        "except Exception as e:\n",
        "    base_dir = Path(\"/content\")  # 回退到本地 Colab 空间\n",
        "    print(\"⚠️ 无法挂载 Google Drive，使用本地目录\")\n",
        "\n",
        "# 3. 设置工作目录\n",
        "work_dir = base_dir / project_dir_name\n",
        "work_dir.mkdir(parents=True, exist_ok=True)  # 自动创建（如果不存在）\n",
        "os.chdir(work_dir)\n",
        "print(f\"📂 当前工作目录已切换到: {work_dir.resolve()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PGAKR9VqUSi",
        "outputId": "3c0d6c30-c394-44fc-b7b8-ec9e049ba798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ 无法挂载 Google Drive，使用本地目录\n",
            "📂 当前工作目录已切换到: /content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 克隆 LLaMA-Factory 仓库"
      ],
      "metadata": {
        "id": "0Qja7C7AjcJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF6PY3_tUEFE",
        "outputId": "03269646-5f92-4ac5-b348-42b26f9d722f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkrByNhOGvhf",
        "outputId": "74ec64a2-8316-4b70-fa4d-c86bd0e9f022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'LLaMA-Factory'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 安装核心与高性能依赖"
      ],
      "metadata": {
        "id": "Q3YtyV0TUHoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMOUCpkaimz6",
        "outputId": "2aacaaa8-131b-4a46-b09d-505c44a214ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%nvidia-smi` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 卸载旧版本包（防止冲突）\n",
        "# pip uninstall -y torch torchvision torchaudio flash-attn deepspeed llamafactory tensorflow tensorflow-decision-forests dopamine-rl opencv-python opencv-python-headless opencv-contrib-python thinc\n",
        "\n",
        "# 核心库安装 (请根据您的 CUDA 版本选择合适的 PyTorch 安装命令)\n",
        "!uv pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "# Hugging Face 生态系统和微调工具\n",
        "!uv pip install \"transformers>=4.41.0,<5.0.0\"\n",
        "!uv pip install sentence-transformers\n",
        "!uv pip install -q \"accelerate>=0.30.1\"\n",
        "!uv pip install -q \"peft>=0.10.0\"\n",
        "!uv pip install -q \"datasets>=2.19.1\"\n",
        "!uv pip install -q \"deepspeed>=0.14.2\"\n",
        "!uv pip install -q \"sentence-transformers==4.1.0\"\n",
        "\n",
        "# Qwen-VL 特定库和性能优化库\n",
        "!uv pip install -q \"bitsandbytes>=0.43.1\"\n",
        "!uv pip install -q \"qwen-vl-utils>=0.0.11\"\n",
        "# !pip install -q \"flash-attn==2.5.8\" --no-build-isolation\n",
        "\n",
        "# 可视化和监控工具\n",
        "!uv pip install -q \"supervision>=0.20.0\"\n",
        "!uv pip install -q \"tensorboard==2.18\"\n",
        "!uv pip install -q \"wandb>=0.17.0\"\n",
        "!uv pip install -q peft trl\n",
        "\n",
        "\n",
        "# 安装 Flash Attention（可能需要编译工具 cmake、ninja）\n",
        "!uv pip install flash-attn --no-build-isolation\n",
        "\n",
        "# 安装 DeepSpeed（使用阿里镜像加速）\n",
        "!uv pip install deepspeed -i https://mirrors.aliyun.com/pypi/simple/\n",
        "\n",
        "\n",
        "# 安装其他依赖\n",
        "!uv pip install trl modelscope addict\n",
        "\n",
        "# 可选：验证 GPU 与 PyTorch\n",
        "!python -c \"import torch; print(torch.cuda.is_available(), torch.version.cuda, torch.backends.cudnn.version())\"\n",
        "\n",
        "# !pip install -e .[torch,metrics]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bOmWW1lYUJhz",
        "outputId": "b6e68c05-5623-422e-dafd-36e748c08352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping flash-attn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping deepspeed as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping llamafactory as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-decision-forests as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dopamine-rl as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "peft 0.17.0 requires torch>=1.13.0, which is not installed.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
            "accelerate 1.10.0 requires torch>=2.0.0, which is not installed.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4def949152d04bc99fe247cd5800f053"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torch-2.8.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torchvision-0.23.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torchaudio-2.8.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu126/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.4.0 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu126/torch-2.8.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (821.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.9/821.9 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached https://download.pytorch.org/whl/cu126/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.whl (8.9 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu126/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (897 kB)\n",
            "Using cached https://download.pytorch.org/whl/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "Downloading https://download.pytorch.org/whl/cu126/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchvision-0.23.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchaudio-2.8.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0+cu126 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.13.3 torch-2.8.0+cu126 torchaudio-2.8.0+cu126 torchvision-0.23.0+cu126 triton-3.4.0\n",
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp311-cp311-linux_x86_64.whl size=256043372 sha256=3d41b2fc55753faa7f45d6568ea73a96b96afb48b82994ab9b49bcbcb6c87588\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/31/1f/4b22dd7295b3cb064b8fa9038f6d58fb15c9571555b2d7c39c\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n",
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
            "Collecting deepspeed\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/84/b3/02d000bfd0d6f0ba3f5dac487e4151b3e9b4b3ee71336882102dc9ad79ad/deepspeed-0.17.4.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/1f/7f/13cd798d180af4bf4c0ceddeefba2b864a63c71645abc0308b768d67bb81/hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.1)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/ed/de/0e6edf44d6a04dabd0318a519125ed0415ce437ad5a1ec9b9be03d9048cf/ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch->deepspeed) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.17.4-py3-none-any.whl size=1705588 sha256=582d72088a8edec20fd169bade1f31bfdde2c771ffaac533e6bd1996241ef998\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/63/7d/deeb3ed72a58c02e6de3948fe2cf127aa2f28cbe34586155fe\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.17.4 hjson-3.1.0 ninja-1.13.0\n",
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==3.6.0\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate==1.7.0\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.8.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=2.0.0->accelerate==1.7.0) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.7.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.7.0) (3.0.2)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, datasets, accelerate\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.4\n",
            "    Uninstalling tokenizers-0.21.4:\n",
            "      Successfully uninstalled tokenizers-0.21.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.55.1\n",
            "    Uninstalling transformers-4.55.1:\n",
            "      Successfully uninstalled transformers-4.55.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.10.0\n",
            "    Uninstalling accelerate-1.10.0:\n",
            "      Successfully uninstalled accelerate-1.10.0\n",
            "Successfully installed accelerate-1.7.0 datasets-3.6.0 tokenizers-0.19.1 transformers-4.41.2\n",
            "Collecting trl\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting modelscope\n",
            "  Downloading modelscope-1.29.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.7.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.6.0)\n",
            "Collecting transformers>=4.55.0 (from trl)\n",
            "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from modelscope) (3.18.0)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.11/dist-packages (from modelscope) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from modelscope) (75.2.0)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from modelscope) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from modelscope) (2.5.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->modelscope) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->modelscope) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->modelscope) (2025.8.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.55.0->trl) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.55.0->trl)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=1.4.0->trl) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.7)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
            "Downloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.29.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, modelscope, tokenizers, transformers, trl\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed addict-2.4.0 modelscope-1.29.0 tokenizers-0.21.4 transformers-4.55.2 trl-0.21.0\n",
            "Obtaining file:///content\n",
            "\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 验证安装 LLaMA-Factory\n",
        "\n",
        "运行 LLaMA-Factory 的命令行工具，检查是否能成功打印版本号。"
      ],
      "metadata": {
        "id": "zGktbtKZUgN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVTjtOsCUmor",
        "outputId": "ca9bcd42-905b-43b6-b6b8-8617f4578ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-16 03:37:15,067] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
            "[2025-08-16 03:37:15,069] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
            "[2025-08-16 03:37:19,988] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
            "----------------------------------------------------------\n",
            "| Welcome to LLaMA Factory, version 0.9.4.dev0           |\n",
            "|                                                        |\n",
            "| Project page: https://github.com/hiyouga/LLaMA-Factory |\n",
            "----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果安装成功，应该能看到 LLaMA-Factory 的欢迎信息和版本号。"
      ],
      "metadata": {
        "id": "4gvhVWqZjqq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 第 3 步：数据准备与自动注册\n",
        "高质量的数据是模型“学得好”的关键。不同的微调阶段需要不同格式的数据。"
      ],
      "metadata": {
        "id": "ZGAm5YcGVpvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1 数据格式说明**\n",
        "\n",
        "\n",
        "| 数据格式 / 字段结构                                          | LLaMA-Factory            | Hugging Face Trainer / PEFT   | Hugging Face TRL（DPO/PPO）    | 说明                              |\n",
        "| ------------------------------------------------------------ | ------------------------ | ----------------------------- | ------------------------------ | --------------------------------- |\n",
        "| ✅ `conversations` 列表结构（支持多轮对话）                   | ✅ 原生支持               | ❌ 不支持，需要手动拼接        | ❌ 不支持，需要手动拼接         | 只 LLaMA-Factory 会解析成对话模板 |\n",
        "| ✅ 支持 `system` 消息                                         | ✅ 支持（参与上下文拼接） | ❌ 不支持（需手动拼入 prompt） | ❌ 不支持（需拼入 prompt）      | Hugging Face 不解析角色字段       |\n",
        "| ✅ 单轮问答格式： `{\"input\": \"prompt\", \"output\": \"response\"}` | ❌ 不推荐用此格式         | ✅ 标准支持格式                | ⚠️ 支持（但字段名可能需自定义） | Hugging Face 推荐的监督微调格式   |\n",
        "| ✅ DPO 格式： `{\"instruction\": \"...\", \"output\": [chosen, rejected]}` | ✅ 支持（train_dpo.py）   | ❌ 不支持                      | ✅ 支持（`DPOTrainer`）         | LLaMA 和 TRL 都支持               |\n",
        "| ✅ DPO 格式变体： `{\"prompt\": \"...\", \"chosen\": \"...\", \"rejected\": \"...\"}` | ✅ 支持                   | ❌ 不支持                      | ✅ 支持                         | TRL 推荐格式                      |\n",
        "| ✅ RM 训练格式： `{\"prompt\": \"...\", \"chosen\": \"...\", \"rejected\": \"...\"}` | ✅ 支持                   | ❌ 不支持                      | ⚠️ 部分支持（需自定义 RM 构造） | 用于奖励模型训练                  |\n",
        "| ✅ GRPO / PPO： `{\"prompt\": \"...\"} 或含 history`              | ✅ 支持                   | ❌ 不支持                      | ✅ 支持（`PPOTrainer`）         | history 一般需手动拼入 prompt     |\n",
        "| ✅ `.json` 文件（数组形式）                                   | ✅ 支持                   | ✅ 支持                        | ✅ 支持                         | JSON 文件均支持                   |\n",
        "| ✅ `.jsonl` 文件（每行一个 JSON）                             | ✅ 推荐格式               | ✅ 支持                        | ✅ 支持                         | 更适合大规模训练                  |"
      ],
      "metadata": {
        "id": "U6jv8aSWVz9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **3.2 公开数据集推荐**\n",
        "\n",
        "对于许多场景，社区已经贡献了高质量的数据集，可以直接使用。\n",
        "\n"
      ],
      "metadata": {
        "id": "rCKpzJAAYTIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **SFT (监督微调) 数据集**\n",
        "\n",
        "\n",
        "SFT 阶段的目标是教会模型基础的知识和对话风格。此阶段最常用的格式是 `instruction`/`output` 格式，特别适合指令跟随和单轮问答。\n",
        "\n",
        "- **第一步：下载数据集** 我们将使用优质数据集 `CFYuan/Chat-huanhuan`，它的格式就是标准的 `instruction`/`input`/`output`。创建一个 `download_sft_dataset.py` 脚本来下载并保存它\n",
        "\n"
      ],
      "metadata": {
        "id": "8ERYRAiBb2T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download_sft_dataset.py\n",
        "from modelscope.msdatasets import MsDataset\n",
        "import json\n",
        "\n",
        "def save_dataset_to_json(dataset, output_path):\n",
        "    \"\"\" 将数据集保存为 JSON Lines 格式 (.jsonl) \"\"\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for item in dataset:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    print(f\"数据集已成功保存到: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 从 ModelScope 加载数据集\n",
        "    dataset_name = 'CFYuan/Chat-huanhuan'\n",
        "    ms_dataset = MsDataset.load(dataset_name, subset_name='default', split='train', trust_remote_code=True)\n",
        "\n",
        "    # 定义本地保存路径\n",
        "    output_file_path = \"data/huanhuan_sft_local.json\"\n",
        "\n",
        "    # 直接保存\n",
        "    save_dataset_to_json(ms_dataset, output_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "451AGnf1b8QV",
        "outputId": "ce547fe9-a83e-4906-f41f-db0468bc49a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-15 09:29:06,489 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from Chat-huanhuan. Please make sure that you can trust the external codes.\n",
            "2025-08-15 09:29:08,324 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from CFYuan/Chat-huanhuan. Please make sure that you can trust the external codes.\n",
            "2025-08-15 09:29:08,325 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from CFYuan/Chat-huanhuan. Please make sure that you can trust the external codes.\n",
            "2025-08-15 09:29:08,326 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from CFYuan/Chat-huanhuan. Please make sure that you can trust the external codes.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据集已成功保存到: data/huanhuan_sft_local.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行脚本 python download_sft_dataset.py，数据将被保存到 data/huanhuan_sft_local.json。\n"
      ],
      "metadata": {
        "id": "HFmKEq4vc_Fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **通用指令型：`BelleGroup/train_2M_CN`**\n",
        "  - **简介**：由 BELLE 项目生成的、包含约200万条多样化指令的中文数据集。\n",
        "  - **适用场景**：全面提升模型在中文环境下的通用指令遵循能力（如问答、翻译、写作等）。\n",
        "  - **LLaMA-Factory 名称**: `belle_2m`"
      ],
      "metadata": {
        "id": "z9CBeBnnb7Zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DPO/ORPO 数据准备\n",
        "\n",
        "- **第一步：下载数据集** 我们将使用高质量的中文问答偏好数据集 `liyucheng/zhihu_rlhf_3k`。创建一个 `download_dpo_dataset.py` 脚本来下载并保存它。"
      ],
      "metadata": {
        "id": "q_QVffFDoz85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download_dpo_dataset.py\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "def save_dataset_to_json(dataset, output_path):\n",
        "    \"\"\" 将数据集保存为 JSON Lines 格式 (.jsonl) \"\"\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for item in dataset:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    print(f\"数据集已成功保存到: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 从 Hugging Face Hub 加载数据集\n",
        "    dataset_name = 'liyucheng/zhihu_rlhf_3k'\n",
        "    dataset = load_dataset(dataset_name, split='train')\n",
        "\n",
        "    # 定义本地保存路径\n",
        "    output_file_path = \"data/zhihu_rlhf_local.json\"\n",
        "\n",
        "    # 直接保存\n",
        "    save_dataset_to_json(dataset, output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "88cbb58646de4eb8809af64fadac2363",
            "63c0fa7865c14421a951f7276bbe8134",
            "82b89cbe049646df817ce09f5d0bf06a",
            "49fae0384f5d4005954e4bad7a4b23b7",
            "351e025fc08b46828e13b610a62a44a1",
            "7a20944fcfef4381ab7b27ef6eb161fb",
            "f4b9036aba5545b596346a3dcfda6458",
            "367634585ad24e92986be0ee398339fe",
            "80d077418cdf48bc9de7fe077811538b",
            "f63af71d4416475b9831660aa9c59c66",
            "d6d43d3310fe47f4b8f68f99cad803ce",
            "59f86f2fa3b940b3a4a687829467126a",
            "6b8996e673f148e8a2dbf6a028d3be1a",
            "3cb3823108044a3784d7bf25ba833374",
            "8a5511f15a4b469494f3c7eba9540ae6",
            "8c76ba3cebb24d3ba110c78794a6e1da",
            "b4c67452736747dd8db5fae3f53646d8",
            "fd10d4fd2fb3464dab51ac40d8b4b4ca",
            "dc5129ca5e0b4af0ad73dbbc68429dcf",
            "56713605bd7e4ff39f67036e36399cc8",
            "80b2e935174543099a1d09b7e039b31a",
            "b28896743cba454fad6a6a94e9a63e56",
            "c571c18ae4df4a1a90f2819fc5d9803f",
            "b7e8a60dc39a434a8d97f98580ff201f",
            "98449b50d77043e7bba60e6f2e0d5b18",
            "4931472f477d4573bf8413f2e629303d",
            "a512cbdf8201442488b1b603459d3f3c",
            "3f714606ca0c4bd5ac0642d7defeca87",
            "896eb5662e3046529d24b3a1d0d72a38",
            "b0ed61da0cd845c288cbdf14c7ee278e",
            "3b64256559f241f5ac1075da9237f11b",
            "e8318f3eb4d044269003276083e3e9c5",
            "307f549df91f4f1fbc4c80721d72f6c5"
          ]
        },
        "id": "L5FqvnH2o9Pp",
        "outputId": "a6c8e22c-ebe6-43ac-f997-260d6a369914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88cbb58646de4eb8809af64fadac2363"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "zhihu_3k_rlfh.tsv:   0%|          | 0.00/15.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59f86f2fa3b940b3a4a687829467126a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/3460 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c571c18ae4df4a1a90f2819fc5d9803f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据集已成功保存到: data/zhihu_rlhf_local.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行脚本 python download_dpo_dataset.py，数据将被保存到 data/zhihu_rlhf_local.json。"
      ],
      "metadata": {
        "id": "EMqxCth5o_4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了让模型先学习“知乎问答”的基本模式，我们从 DPO 数据中提取高质量的问答对，作为 SFT 数据。创建一个 create_sft_from_dpo.py 脚本。"
      ],
      "metadata": {
        "id": "xjbsdBm8pRIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create_sft_from_dpo.py\n",
        "import json\n",
        "\n",
        "def convert_dpo_to_sft(dpo_path, sft_path):\n",
        "    sft_data = []\n",
        "    with open(dpo_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            item = json.loads(line)\n",
        "            sft_item = {\n",
        "                \"instruction\": item[\"prompt\"],\n",
        "                \"input\": \"\",\n",
        "                \"output\": item[\"chosen\"] # 使用高质量回答作为SFT的答案\n",
        "            }\n",
        "            sft_data.append(sft_item)\n",
        "\n",
        "    with open(sft_path, 'w', encoding='utf-8') as f:\n",
        "        for item in sft_data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    print(f\"已从 {dpo_path} 提取并创建 SFT 数据集于: {sft_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    convert_dpo_to_sft(\"data/zhihu_rlhf_local.json\", \"data/zhihu_sft_local.json\")"
      ],
      "metadata": {
        "id": "r23Pz4mYqDdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行 python create_sft_from_dpo.py。"
      ],
      "metadata": {
        "id": "ODKvnkdnqHJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 通过代码注册数据集 (推荐)"
      ],
      "metadata": {
        "id": "bIceZUDbpDZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "下载完数据集后，手动编辑 `dataset_info.json` 较为繁琐。我们可以创建一个脚本来自动完成注册。\n",
        "\n",
        "- **创建 `register_datasets.py` 脚本** 在 `LLaMA-Factory` 根目录下创建该文件。"
      ],
      "metadata": {
        "id": "d9K_Z04UpIwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register_datasets.py\n",
        "import json\n",
        "import os\n",
        "\n",
        "def register_dataset(dataset_info_path, dataset_name, file_name, columns, stage):\n",
        "    \"\"\"\n",
        "    以编程方式读取、更新并保存 dataset_info.json 文件，并包含 stage 信息。\n",
        "    \"\"\"\n",
        "    if os.path.exists(dataset_info_path):\n",
        "        with open(dataset_info_path, 'r', encoding='utf-8') as f:\n",
        "            all_datasets = json.load(f)\n",
        "    else:\n",
        "        all_datasets = {}\n",
        "\n",
        "    all_datasets[dataset_name] = {\n",
        "        \"file_name\": file_name,\n",
        "        \"stage\": stage, # 添加数据集适用的阶段\n",
        "        \"columns\": columns,\n",
        "        \"ranking\": stage==\"rm\",\n",
        "    }\n",
        "\n",
        "    with open(dataset_info_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_datasets, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"成功注册数据集 '{dataset_name}' (阶段: {stage})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    info_path = \"data/dataset_info.json\"\n",
        "    # 注册 SFT 章节的数据集\n",
        "    register_dataset(info_path, \"huanhuan_sft_local\", \"huanhuan_sft_local.json\",\n",
        "                     {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}, stage=\"sft\")\n",
        "    # 注册 DPO 章节可选 SFT 步骤的数据集\n",
        "    register_dataset(info_path, \"zhihu_sft_local\", \"zhihu_sft_local.json\",\n",
        "                     {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}, stage=\"sft\")\n",
        "    # 注册 DPO 章节的数据集\n",
        "    # DPO 阶段在 LLaMA-Factory 中内部使用 \"rm\" (Reward Modeling) 的数据加载逻辑\n",
        "    register_dataset(info_path, \"zhihu_dpo_local\", \"zhihu_rlhf_local.json\",\n",
        "                     {\"prompt\": \"prompt\", \"chosen\": \"chosen\", \"rejected\": \"rejected\"}, stage=\"rm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "rBENvHappE2J",
        "outputId": "7d6eaf75-8a90-4945-b6b4-269e3353b8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/dataset_info.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2582901332.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0minfo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/dataset_info.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# 注册 SFT 章节的数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     register_dataset(info_path, \"huanhuan_sft_local\", \"huanhuan_sft_local.json\", \n\u001b[0m\u001b[1;32m     29\u001b[0m                      {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}, stage=\"sft\")\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# 注册 DPO 章节可选 SFT 步骤的数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2582901332.py\u001b[0m in \u001b[0;36mregister_dataset\u001b[0;34m(dataset_info_path, dataset_name, file_name, columns, stage)\u001b[0m\n\u001b[1;32m     19\u001b[0m     }\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_info_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"成功注册数据集 '{dataset_name}' (阶段: {stage})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/dataset_info.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 运行脚本\n",
        "\n",
        "执行 python register_datasets.py，即可一次性完成所有本地数据集的注册。"
      ],
      "metadata": {
        "id": "zUnoh9-ppOGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### GRPO (生成式重放策略优化) 数据准备\n",
        "GRPO 只需要一系列“提示”(Prompt)即可，非常适合数学、代码等生成任务。这里我们使用经典的 GSM8K 数学推理数据集。详见后续章节。"
      ],
      "metadata": {
        "id": "WFlb8SUzpToc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第二部分：SFT 微调章节"
      ],
      "metadata": {
        "id": "_16afEM4WRwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**目标**：为模型注入基础知识或特定领域的说话风格。SFT 是最基础也是最重要的一步，它为后续更高级的对齐技术（如 DPO）打下坚实的基础。\n",
        "\n",
        "**示例**：在本章节中，我们将使用在第一部分准备好的“甄嬛传”风格数据集 (`huanhuan_sft_local`)，通过 SFT-LoRA 技术，训练一个具有特定说话风格的模型。"
      ],
      "metadata": {
        "id": "QLh2cJQ-pnAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 方案一：使用 LLaMA-Factory 进行 SFT\n",
        "\n",
        "LLaMA-Factory 提供了高度集成的命令行工具，是进行 SFT 微调的最高效选择。"
      ],
      "metadata": {
        "id": "UMMQX1Gyp_cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SFT 训练命令\n",
        "下面是用于启动 SFT 训练的 bash 脚本。\n",
        "\n",
        "```bash\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "export FORCE_TORCHRUN=1\n",
        "\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "OUTPUT_PATH_SFT='./output/Zhenhuan_Style_SFT_LLF'\n",
        "DATASET_NAME_SFT='huanhuan_sft_local'\n",
        "\n",
        "llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --dataset $DATASET_NAME_SFT \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --output_dir $OUTPUT_PATH_SFT \\\n",
        "    --overwrite_cache \\\n",
        "    --overwrite_output_dir \\\n",
        "    --cutoff_len 4096 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 \\\n",
        "    \\\n",
        "    # --- QLoRA 量化参数 ---\n",
        "    --quantization_bit 4 \\\n",
        "    --double_quantization True \\\n",
        "    --quantization_type nf4 \\\n",
        "    \\\n",
        "    # --- LoRA 参数 ---\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --lora_target q_proj,v_proj\n",
        "```"
      ],
      "metadata": {
        "id": "T2aduvhk690L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 参数深度解析\n",
        "\n",
        "这里对上述命令中的关键参数进行详细说明：\n",
        "\n",
        "| 分类           | 参数                              | 说明                                                         |\n",
        "| -------------- | --------------------------------- | ------------------------------------------------------------ |\n",
        "| **核心参数**   | `--stage sft`                     | 指定当前任务为 **监督微调 (Supervised Fine-Tuning)** 阶段。  |\n",
        "|                | `--do_train`                      | 明确指示脚本执行训练流程。                                   |\n",
        "|                | `--model_name_or_path`            | 指定基础模型。可以是 Hugging Face Hub ID、ModelScope ID 或本地路径。 |\n",
        "|                | `--dataset`                       | 指定在 `dataset_info.json` 中注册的数据集名称。              |\n",
        "|                | `--template qwen`                 | **极其重要**。指定对话模板，必须与基础模型（如此处的 Qwen）严格匹配，否则模型无法正确理解输入。 |\n",
        "|                | `--finetuning_type lora`          | 指定微调方法为 LoRA，这是一种高效的参数微调技术。            |\n",
        "|                | `--output_dir`                    | 指定所有训练产物（模型权重、日志、检查点）的保存目录。       |\n",
        "| **训练控制**   | `--overwrite_cache`               | 覆盖预处理后的数据缓存。当您修改了数据集或数据处理方式时，建议开启。 |\n",
        "|                | `--overwrite_output_dir`          | 允许覆盖输出目录中已有的内容，方便重复实验。                 |\n",
        "|                | `--cutoff_len 4096`               | 设置模型处理的最大序列长度（tokens）。需要根据您的任务和显存大小进行调整。 |\n",
        "|                | `--per_device_train_batch_size 4` | 每块 GPU 在单次前向传播中处理的样本数量。                    |\n",
        "|                | `--gradient_accumulation_steps 4` | 梯度累积步数。`有效批次大小 = batch_size * 累积步数`。这是在显存有限时扩大批次大小的常用技巧。 |\n",
        "|                | `--learning_rate 1e-4`            | 学习率。对于 LoRA 微调，`1e-4` 是一个常用的、效果不错的初始值。 |\n",
        "|                | `--num_train_epochs 3`            | 训练的总轮数。对于 SFT，通常 1-3 轮即可获得不错的效果。      |\n",
        "|                | `--plot_loss`                     | 在训练结束后，在输出目录中生成一张训练损失曲线图 `training_loss.png`。 |\n",
        "|                | `--fp16`                          | 启用半精度（16-bit）浮点数进行训练，可以大幅节省显存并提升训练速度。 |\n",
        "| **QLoRA 量化** | `--quantization_bit 4`            | **QLoRA 核心**。指定使用 4-bit 对模型基座进行量化，极大降低了显存占用。 |\n",
        "|                | `--double_quantization True`      | 启用双重量化，可以进一步节省少量显存，推荐开启。             |\n",
        "|                | `--quantization_type nf4`         | 指定量化类型为 `nf4` (Normal Float 4)，这是 QLoRA 推荐的、理论上更优的 4-bit 数据类型。 |\n",
        "| **LoRA 参数**  | `--lora_rank 8`                   | LoRA 矩阵的秩 (r)。决定了 LoRA 适配器的参数量大小。常用值为 8, 16, 32, 64。值越大，可训练参数越多，拟合能力越强，但过大也可能导致过拟合。 |\n",
        "|                | `--lora_alpha 16`                 | LoRA 的缩放因子。通常设置为 `lora_rank` 的 2 倍，这是一个经验性的最佳实践。 |\n",
        "|                | `--lora_dropout 0.05`             | 在 LoRA 矩阵上应用的 Dropout 比率，用于防止过拟合。          |\n",
        "|                | `--lora_target all`               | **推荐设置**。指定将 LoRA 应用到模型中的哪些模块。设置为 `all` 后，LLaMA-Factory 会自动识别所有可应用的线性层（如 `q_proj`, `v_proj` 等），省去手动指定的麻烦。 |\n",
        "\n"
      ],
      "metadata": {
        "id": "RJzKVZoUqRTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 执行训练"
      ],
      "metadata": {
        "id": "ZW2H--mqqcqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "# os.environ[\"WANDB_MODE\"] = \"offline\"  # 离线模式\n",
        "\n",
        "# --- 请在此处修改你的路径和名称 ---\n",
        "# 模型ID，可以是 Hugging Face Hub ID 或 ModelScope ID\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "OUTPUT_PATH='./output/Zhenhuan_Style_SFT'\n",
        "# 使用您在步骤2.1和2.3中准备好的本地数据集\n",
        "DATASET_NAME='huanhuan_sft_local'\n",
        "# ---------------------------------\n",
        "DS_CONFIG_PATH='examples/deepspeed/ds_z3_config.json'\n",
        "\n",
        "!FORCE_TORCHRUN=1 llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --dataset $DATASET_NAME_SFT \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --output_dir $OUTPUT_PATH_SFT \\\n",
        "    --overwrite_cache \\\n",
        "    --overwrite_output_dir \\\n",
        "    --cutoff_len 4096 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 \\\n",
        "    \\\n",
        "    --quantization_bit 4 \\\n",
        "    --double_quantization True \\\n",
        "    --quantization_type nf4 \\\n",
        "    \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --lora_target all \\\n",
        "    --report_to none"
      ],
      "metadata": {
        "id": "4Ymp281MVpBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 方案二：使用 Hugging Face TRL 进行 SFT\n",
        "\n",
        "对于希望更深入定制训练逻辑的用户，可以直接使用 Hugging Face 的 TRL 库。"
      ],
      "metadata": {
        "id": "g87HMcZo7QXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_sft_with_trl.py\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def train_sft():\n",
        "    # --- 1. 定义模型、数据集和输出路径 ---\n",
        "    model_id = 'qwen/Qwen1.5-0.5B-Chat'\n",
        "    dataset_name = \"huanhuan_sft_local\"\n",
        "    output_dir = \"./output/Zhenhuan_Style_SFT_TRL\"\n",
        "\n",
        "    # --- 2. 配置 QLoRA 量化 ---\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,                      # 激活4-bit量化\n",
        "        bnb_4bit_quant_type=\"nf4\",              # 量化类型 (nf4, fp4)\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,  # 计算数据类型\n",
        "        bnb_4bit_use_double_quant=True,         # 激活嵌套量化\n",
        "    )\n",
        "\n",
        "    # --- 3. 加载模型和分词器 ---\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map={\"\": 0} # 指定GPU\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # --- 4. 加载数据集 ---\n",
        "    dataset = load_dataset(\"json\", data_files=f\"data/{dataset_name}.json\", split=\"train\")\n",
        "\n",
        "    # --- 5. 配置训练参数 ---\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=1e-4,\n",
        "        num_train_epochs=3,\n",
        "        logging_steps=10,\n",
        "        fp16=True,\n",
        "        save_strategy=\"epoch\",\n",
        "    )\n",
        "\n",
        "    # --- 6. 配置 LoRA ---\n",
        "    peft_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        # 指定要应用 LoRA 的模块。这是一个关键参数！\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # --- 7. 创建并启动 SFTTrainer ---\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"instruction\", # 指定包含文本的列\n",
        "        max_seq_length=1024,\n",
        "        peft_config=peft_config,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    print(f\"SFT 训练完成，模型已保存至 {output_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_sft()"
      ],
      "metadata": {
        "id": "zylT8PeY7Uqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行脚本 python train_sft_with_trl.py。"
      ],
      "metadata": {
        "id": "XIHSaZZZ7ffy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SFT 模型合并与测试\n",
        "\n",
        "无论使用哪种方案，训练完成后得到的都只是一个轻量的 LoRA 适配器（adapter），而不是一个完整的模型。我们需要将这个适配器与原始的基础模型合并，才能得到一个可以独立部署和使用的模型。\n",
        "\n"
      ],
      "metadata": {
        "id": "aa2MwLTjX1do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 合并模型\n",
        "\n",
        "```bash\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "BASE_MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "ADAPTER_PATH='./output/Zhenhuan_Style_SFT'\n",
        "EXPORT_PATH='./output/Zhenhuan_SFT_Merged'\n",
        "\n",
        "llamafactory-cli export \\\n",
        "    --model_name_or_path $BASE_MODEL_PATH \\\n",
        "    --adapter_name_or_path $ADAPTER_PATH \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --export_dir $EXPORT_PATH \\\n",
        "    --export_size 2 \\\n",
        "    --export_legacy_format False\n",
        "```"
      ],
      "metadata": {
        "id": "_zGQuolEqsTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "ADAPTER_PATH='./output/Zhenhuan_Style_SFT'\n",
        "EXPORT_PATH='./output/Zhenhuan_SFT_Merged'\n",
        "\n",
        "!FORCE_TORCHRUN=1 llamafactory-cli export \\\n",
        "    --model_name_or_path $BASE_MODEL_PATH \\\n",
        "    --adapter_name_or_path $ADAPTER_PATH \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --export_dir $EXPORT_PATH \\\n",
        "    --export_size 2 \\\n",
        "    --export_legacy_format False"
      ],
      "metadata": {
        "id": "h5DcpK_QZnuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 启动 Web UI 聊天测试"
      ],
      "metadata": {
        "id": "tydGwxbwZicA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "MERGED_MODEL_PATH='./output/Zhenhuan_SFT_Merged'\n",
        "\n",
        "!llamafactory-cli webchat \\\n",
        "    --model_name_or_path $MERGED_MODEL_PATH \\\n",
        "    --template qwen"
      ],
      "metadata": {
        "id": "pgXFHyaQbDE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 通过代码进行交互式推理\n",
        "\n",
        "通过 Python 脚本来调用模型，进行更灵活的测试。"
      ],
      "metadata": {
        "id": "fnVo6Tv7cTua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# --- 请确保这里的路径指向您合并后的模型 ---\n",
        "model_path = './output/Zhenhuan_SFT_Merged'\n",
        "\n",
        "print(f\"正在从 '{model_path}' 加载模型和分词器...\")\n",
        "\n",
        "# 加载分词器和模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16, # 推荐使用 bfloat16 以获得更好的性能和兼容性\n",
        "    device_map=\"auto\" # 自动将模型加载到可用的 GPU\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"模型加载完成，准备生成回答。\")\n",
        "\n",
        "# --- 关键步骤：构建符合模板的对话 ---\n",
        "\n",
        "# 1. 将你的问题构造成一个列表，其中包含字典，每个字典代表一个角色和其内容\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"你好，可以介绍一下你是谁吗？\"}\n",
        "]\n",
        "\n",
        "# 2. 使用 tokenizer.apply_chat_template 来格式化输入\n",
        "#    这个函数会自动添加 <|im_start|>user\\n...<|im_end|>\\n<|im_start|>assistant\\n 等特殊标记\n",
        "#    add_generation_prompt=True 会在最后加上 assistant 的起始标记，引导模型开始生成\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "# --- 生成回答 ---\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "# --- 解码并打印 ---\n",
        "# outputs[0] 包含了输入的 prompt 和新生成的回答\n",
        "# 我们需要从 outputs 中剥离掉输入的 prompt 部分，只解码新生成的内容\n",
        "response_ids = outputs[0][inputs.shape[-1]:]\n",
        "response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- 对话 ---\")\n",
        "print(\"User:\", messages[0][\"content\"])\n",
        "print(\"Assistant:\", response)"
      ],
      "metadata": {
        "id": "co0WF9GLcPNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第三部分：DPO 微调章节\n",
        "\n",
        "**目标**：在模型具备基础能力后，通过“好/坏”回答对比，让其学习人类的偏好，生成更优质的回答。\n",
        "\n"
      ],
      "metadata": {
        "id": "PPUO5HuKo2uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (可选) 预备步骤：SFT\n",
        "\n",
        "**为什么需要 SFT？** 直接对基础模型进行 DPO 也可以，但通常效果不如先进行 SFT。SFT 阶段可以让模型先学习目标领域的基本知识和数据分布（例如，先学会如何回答知乎风格的问题），为后续的偏好学习提供一个更好的起点，从而让 DPO 训练更稳定、效果更好。\n",
        "\n",
        "**示例**：使用我们从知乎偏好数据中提取的 SFT 数据集，先进行一轮 SFT。\n",
        "```bash\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "export FORCE_TORCHRUN=1\n",
        "\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "OUTPUT_PATH_SFT='./output/ZhihuQA_SFT'\n",
        "DATASET_NAME_SFT='zhihu_sft_local'\n",
        "\n",
        "llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --dataset $DATASET_NAME_SFT \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --output_dir $OUTPUT_PATH_SFT \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-5 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --plot_loss \\\n",
        "    --fp16\n",
        "    \\\n",
        "    # --- QLoRA 量化参数 ---\n",
        "    --quantization_bit 4 \\\n",
        "    --double_quantization True \\\n",
        "    --quantization_type nf4 \\\n",
        "    \\\n",
        "    # --- LoRA 参数 ---\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --lora_target all \\\n",
        "    --report_to none\n"
      ],
      "metadata": {
        "id": "k5GaV1ou2lHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "# os.environ[\"WANDB_MODE\"] = \"offline\"  # 离线模式\n",
        "os.environ[\"FORCE_TORCHRUN\"] = \"1\"\n",
        "\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "OUTPUT_PATH_SFT='./output/ZhihuQA_SFT'\n",
        "DATASET_NAME_SFT='zhihu_sft_local'\n",
        "\n",
        "!llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --dataset $DATASET_NAME_SFT \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --output_dir $OUTPUT_PATH_SFT \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-5 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 \\\n",
        "    --quantization_bit 4 \\\n",
        "    --double_quantization True \\\n",
        "    --quantization_type nf4 \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --lora_target all"
      ],
      "metadata": {
        "id": "4i49LsIx2y6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DPO 训练\n",
        "\n",
        "现在，我们在 SFT 预训练过的模型基础上，使用完整的 DPO 数据集进行偏好对齐。\n",
        "\n",
        "```bash\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "export FORCE_TORCHRUN=1\n",
        "# 消除日志中的并行化警告，使输出更干净\n",
        "export TOKENIZERS_PARALLELISM=false\n",
        "\n",
        "# DPO时，--model_name_or_path 应指向原始基础模型\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "# 使用 --adapter_name_or_path 加载 SFT 阶段的 LoRA 权重\n",
        "SFT_ADAPTER_PATH='./output/ZhihuQA_SFT'\n",
        "OUTPUT_PATH_DPO='./output/ZhihuQA_DPO'\n",
        "DATASET_NAME_DPO='zhihu_dpo_local'\n",
        "\n",
        "llamafactory-cli train \\\n",
        "    --stage dpo \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --adapter_name_or_path $SFT_ADAPTER_PATH \\\n",
        "    --dataset $DATASET_NAME_DPO \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target all \\\n",
        "    --output_dir $OUTPUT_PATH_DPO \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --learning_rate 1e-6 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 \\\n",
        "    --quantization_bit 4\n",
        "```"
      ],
      "metadata": {
        "id": "kzLioDtx2-Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DPO时，--model_name_or_path 应指向原始基础模型\n",
        "MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "# 使用 --adapter_name_or_path 加载 SFT 阶段的 LoRA 权重\n",
        "SFT_ADAPTER_PATH='./output/ZhihuQA_SFT'\n",
        "OUTPUT_PATH_DPO='./output/ZhihuQA_DPO'\n",
        "DATASET_NAME_DPO='zhihu_dpo_local'\n",
        "\n",
        "!FORCE_TORCHRUN=1 llamafactory-cli train \\\n",
        "    --stage dpo \\\n",
        "    --do_train \\\n",
        "    --model_name_or_path $MODEL_PATH \\\n",
        "    --adapter_name_or_path $SFT_ADAPTER_PATH \\\n",
        "    --dataset $DATASET_NAME_DPO \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target all \\\n",
        "    --output_dir $OUTPUT_PATH_DPO \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 1e-6 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --fp16 \\\n",
        "    --quantization_bit 4 \\\n",
        "    --gradient_checkpointing"
      ],
      "metadata": {
        "id": "xBx3-tF52_Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hugging Face TRL"
      ],
      "metadata": {
        "id": "2BhC_J_sTg6z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtsCoTBYRntE"
      },
      "outputs": [],
      "source": [
        "# train_dpo_with_trl.py\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import DPOTrainer\n",
        "\n",
        "def train_dpo():\n",
        "    # --- 1. 定义模型、数据集和输出路径 ---\n",
        "    sft_model_id = \"./output/ZhihuQA_SFT\"\n",
        "    dataset_name = \"zhihu_dpo_local\"\n",
        "    output_dir = \"./output/ZhihuQA_DPO_TRL\"\n",
        "\n",
        "    # --- 2. 配置 QLoRA 量化 ---\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # --- 3. 加载模型和分词器 ---\n",
        "    model = AutoModelForCausalLM.from_pretrained(sft_model_id, quantization_config=bnb_config, device_map={\"\": 0})\n",
        "    tokenizer = AutoTokenizer.from_pretrained(sft_model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # --- 4. 加载数据集 ---\n",
        "    dataset = load_dataset(\"json\", data_files=f\"data/{dataset_name}.json\", split=\"train\")\n",
        "\n",
        "    # --- 5. 配置训练参数 ---\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=1e-6,\n",
        "        num_train_epochs=1,\n",
        "        logging_steps=5,\n",
        "        fp16=True,\n",
        "        save_strategy=\"epoch\",\n",
        "    )\n",
        "\n",
        "    # --- 6. 配置 LoRA ---\n",
        "    peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, target_modules=[\"q_proj\", \"v_proj\"], task_type=\"CAUSAL_LM\")\n",
        "\n",
        "    # --- 7. 创建并启动 DPOTrainer ---\n",
        "    dpo_trainer = DPOTrainer(\n",
        "        model,\n",
        "        args=training_args,\n",
        "        beta=0.1,\n",
        "        train_dataset=dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        peft_config=peft_config,\n",
        "        dataset_map_kwargs={\"prompt\": \"prompt\", \"chosen\": \"chosen\", \"rejected\": \"rejected\"}\n",
        "    )\n",
        "\n",
        "    dpo_trainer.train()\n",
        "    print(f\"DPO 训练完成，模型已保存至 {output_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_dpo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行脚本 python train_dpo_with_trl.py。"
      ],
      "metadata": {
        "id": "mbAyETAQ_jWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DPO 模型合并与测试\n",
        "\n",
        "- **合并模型**\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "BASE_MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "ADAPTER_PATH='./output/ZhihuQA_DPO'\n",
        "EXPORT_PATH='./output/ZhihuQA_DPO_Merged'\n",
        "\n",
        "llamafactory-cli export \\\n",
        "    --model_name_or_path $BASE_MODEL_PATH \\\n",
        "    --adapter_name_or_path $ADAPTER_PATH \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --export_dir $EXPORT_PATH \\\n",
        "    --export_size 2 \\\n",
        "    --export_legacy_format False\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "B2aURA-sm9mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL_PATH='qwen/Qwen1.5-0.5B-Chat'\n",
        "ADAPTER_PATH='./output/ZhihuQA_DPO'\n",
        "EXPORT_PATH='./output/ZhihuQA_DPO_Merged'\n",
        "\n",
        "!llamafactory-cli export \\\n",
        "    --model_name_or_path $BASE_MODEL_PATH \\\n",
        "    --adapter_name_or_path $ADAPTER_PATH \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --export_dir $EXPORT_PATH \\\n",
        "    --export_size 2 \\\n",
        "    --export_legacy_format False"
      ],
      "metadata": {
        "id": "LIzhOfZVnLqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **启动 Web UI 聊天测试**\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "MERGED_MODEL_PATH='./output/ZhihuQA_DPO_Merged'\n",
        "\n",
        "llamafactory-cli webchat \\\n",
        "    --model_name_or_path $MERGED_MODEL_PATH \\\n",
        "    --template qwen\n",
        "```"
      ],
      "metadata": {
        "id": "lFORrB8BnLEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 代码推理\n",
        "\n",
        "如果你无法启动 Web UI，或者希望在代码中直接调用模型，可以使用以下 Python 脚本进行交互式推理。"
      ],
      "metadata": {
        "id": "149px7VtoF3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inference.py\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# --- 请确保这里的路径指向您合并后的DPO模型 ---\n",
        "model_path = './output/ZhihuQA_DPO_Merged'\n",
        "\n",
        "print(f\"正在从 '{model_path}' 加载模型和分词器...\")\n",
        "\n",
        "# 加载分词器和模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"模型加载完成，准备生成回答。\")\n",
        "\n",
        "# 构造对话\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"请问，为什么天空是蓝色的？\"}\n",
        "]\n",
        "\n",
        "# 使用 apply_chat_template 格式化输入\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "# 生成回答\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "# 解码并打印\n",
        "response_ids = outputs[0][inputs.shape[-1]:]\n",
        "response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- 对话 ---\")\n",
        "print(\"User:\", messages[0][\"content\"])\n",
        "print(\"Assistant:\", response)"
      ],
      "metadata": {
        "id": "-7r3oFy2oIeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第四部分：DeepSpeed 与多卡训练配置"
      ],
      "metadata": {
        "id": "IA_I9ln5kD6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSeed配置"
      ],
      "metadata": {
        "id": "s9Uu9cGgoWTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DeepSpeed 配置**: LLaMA-Factory 在 `examples/deepspeed` 目录下提供了多种配置文件，如 `ds_z3_config.json` (ZeRO Stage 3)。这些配置通过优化内存使用，使得在有限的硬件上训练更大的模型成为可能。\n",
        "\n",
        "**多卡/多机训练**:\n",
        "\n",
        "- **单机多卡**: 只需修改 `CUDA_VISIBLE_DEVICES` 环境变量，例如 `export CUDA_VISIBLE_DEVICES=0,1`，然后使用 `torchrun` 启动训练脚本。\n",
        "- **多机多卡**: 需要创建一个 `hostfile` 文件，列出所有机器的 IP 地址和 GPU 数量，并在 `torchrun` 命令中指定该文件。所有机器需要能够免密 SSH 访问。"
      ],
      "metadata": {
        "id": "T_jEWs_Iki4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 准备 DeepSpeed 配置文件\n",
        "\n",
        "DeepSpeed 官方仓库提供示例文件：\n",
        "- GitHub：https://github.com/microsoft/DeepSpeed\n",
        "- 常见位置：examples 文件夹里，例如 ds_config.json\n",
        "\n",
        "LLaMA-Factory 官方文档也有推荐配置：\n",
        "- https://github.com/liuhaotian/LLaMA-Factory\n",
        "- DeepSpeed 配置: LLaMA-Factory 在 examples/deepspeed 目录下提供了多种配置文件，如 ds_z3_config.json (ZeRO Stage 3)。这些配置通过优化内存使用，使得在有限的硬件上训练更大的模型成为可能。"
      ],
      "metadata": {
        "id": "fPr_xXeK-TXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DeepSpeed 单机单卡配置\n",
        "```json\n",
        "{\n",
        "  \"train_batch_size\": 4,\n",
        "  \"train_micro_batch_size_per_gpu\": 4,\n",
        "  \"gradient_accumulation_steps\": 1,\n",
        "  \"steps_per_print\": 50,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": 5e-5,\n",
        "      \"betas\": [0.9, 0.999],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\": 0.01\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 5e-5,\n",
        "      \"warmup_num_steps\": 100\n",
        "    }\n",
        "  },\n",
        "  \"fp16\": {\n",
        "    \"enabled\": true,\n",
        "    \"loss_scale\": 0\n",
        "  },\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 1,\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"none\"\n",
        "    },\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"none\"\n",
        "    },\n",
        "    \"overlap_comm\": false,\n",
        "    \"contiguous_gradients\": true\n",
        "  },\n",
        "  \"gradient_clipping\": 1.0,\n",
        "  \"wall_clock_breakdown\": false,\n",
        "  \"steps_per_checkpoint\": 200,\n",
        "  \"activation_checkpointing\": {\n",
        "    \"partition_activations\": false,\n",
        "    \"contiguous_memory_optimization\": true\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "说明：适合本地单卡快速调试和小模型训练，不需要 offload，也不开激活分区。"
      ],
      "metadata": {
        "id": "uHVeaP_LCFav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DeepSpeed 单机多卡配置\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"train_batch_size\": 32,\n",
        "  \"train_micro_batch_size_per_gpu\": 4,\n",
        "  \"gradient_accumulation_steps\": 2,\n",
        "  \"steps_per_print\": 50,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": 5e-5,\n",
        "      \"betas\": [0.9, 0.999],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\": 0.01\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 5e-5,\n",
        "      \"warmup_num_steps\": 500\n",
        "    }\n",
        "  },\n",
        "  \"fp16\": {\n",
        "    \"enabled\": true,\n",
        "    \"loss_scale\": 0\n",
        "  },\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 2,\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"none\"\n",
        "    },\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"none\"\n",
        "    },\n",
        "    \"overlap_comm\": true,\n",
        "    \"contiguous_gradients\": true\n",
        "  },\n",
        "  \"gradient_clipping\": 1.0,\n",
        "  \"wall_clock_breakdown\": false,\n",
        "  \"steps_per_checkpoint\": 500,\n",
        "  \"activation_checkpointing\": {\n",
        "    \"partition_activations\": true,\n",
        "    \"contiguous_memory_optimization\": true\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "说明：适合单机多卡训练，开启 ZeRO stage 2，支持梯度累积和激活检查点节省显存。\n"
      ],
      "metadata": {
        "id": "JUSqLQqnCFeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DeepSpeed 多机多卡配置\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"train_batch_size\": 128,\n",
        "  \"train_micro_batch_size_per_gpu\": 4,\n",
        "  \"gradient_accumulation_steps\": 4,\n",
        "  \"steps_per_print\": 50,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": 5e-5,\n",
        "      \"betas\": [0.9, 0.999],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\": 0.01\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 5e-5,\n",
        "      \"warmup_num_steps\": 1000\n",
        "    }\n",
        "  },\n",
        "  \"fp16\": {\n",
        "    \"enabled\": true,\n",
        "    \"loss_scale\": 0\n",
        "  },\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 3,\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"cpu\"\n",
        "    },\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"cpu\"\n",
        "    },\n",
        "    \"overlap_comm\": true,\n",
        "    \"contiguous_gradients\": true\n",
        "  },\n",
        "  \"gradient_clipping\": 1.0,\n",
        "  \"wall_clock_breakdown\": true,\n",
        "  \"steps_per_checkpoint\": 1000,\n",
        "  \"activation_checkpointing\": {\n",
        "    \"partition_activations\": true,\n",
        "    \"contiguous_memory_optimization\": true\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "说明：适合多机分布式大模型训练，开启 ZeRO stage 3，参数和优化器 offload 到 CPU/NVMe，开启通信重叠和详细时间统计。\n",
        "\n",
        "\n",
        "| 配置   | train\\_batch\\_size | micro\\_batch | ZeRO stage | Offload | 激活检查点 | 单机单卡可用  |\n",
        "| ---- | ------------------ | ------------ | ---------- | ------- | ----- | ------- |\n",
        "| 单机单卡 | 4                  | 4            | 1          | none    | false | ✅       |\n",
        "| 单机多卡 | 32                 | 4            | 2          | none    | true  | ✅       |\n",
        "| 多机多卡 | 128                | 4            | 3          | cpu     | true  | ❌（多机必用） |\n"
      ],
      "metadata": {
        "id": "sCvvaKvCCFg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DeepSeed 配置参数"
      ],
      "metadata": {
        "id": "c2eYijhqCk8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 字段                                                        | 默认值          | 可选值             | 说明                  | 通用建议                         |\n",
        "| --------------------------------------------------------- | ------------ | --------------- | ------------------- | ---------------------------- |\n",
        "| `train_batch_size`                                        | 16           | 正整数             | 所有 GPU 累积 batch     | 按显存和卡数调节，单机单卡可以小一点           |\n",
        "| `train_micro_batch_size_per_gpu`                          | 4            | 1\\~16           | 每张 GPU 的 batch size | 显存够大可增加，推荐 1\\~4              |\n",
        "| `gradient_accumulation_steps`                             | 4            | 1\\~16           | 梯度累积步数              | 用于增加有效 batch size            |\n",
        "| `steps_per_print`                                         | 50           | 10\\~100         | 日志打印频率              | 单机 10\\~50 足够                 |\n",
        "| `optimizer.type`                                          | AdamW        | Adam, SGD       | 优化器类型               | AdamW 通用微调                   |\n",
        "| `optimizer.params.lr`                                     | 5e-5         | 1e-6\\~5e-4      | 学习率                 | 小模型可略大，大模型保持 1e-5\\~5e-5      |\n",
        "| `optimizer.params.betas`                                  | \\[0.9,0.999] | -               | AdamW 参数            | 通用默认                         |\n",
        "| `optimizer.params.eps`                                    | 1e-8         | -               | AdamW 参数            | 通用默认                         |\n",
        "| `optimizer.params.weight_decay`                           | 0.01         | 0\\~0.1          | AdamW 参数            | 避免过拟合                        |\n",
        "| `scheduler.type`                                          | WarmupLR     | Cosine, Linear  | 学习率调度               | WarmupLR 通用                  |\n",
        "| `scheduler.params.warmup_min_lr`                          | 0            | ≥0              | 初始学习率               | 默认 0                         |\n",
        "| `scheduler.params.warmup_max_lr`                          | 5e-5         | 0\\~1e-3         | 最大学习率               | 按 lr 调整                      |\n",
        "| `scheduler.params.warmup_num_steps`                       | 500          | 100\\~2000       | Warmup 步数           | 小数据集 100~~500，大数据集 500~~2000 |\n",
        "| `fp16.enabled`                                            | true         | true/false      | 是否开启混合精度            | 节省显存，加速训练                    |\n",
        "| `fp16.loss_scale`                                         | 0            | 正整数             | 动态 loss scale       | 0 自动即可                       |\n",
        "| `fp16.loss_scale_window`                                  | 1000         | -               | 调整窗口                | 通用默认                         |\n",
        "| `fp16.hysteresis`                                         | 2            | -               | 动态 loss scale       | 通用默认                         |\n",
        "| `fp16.min_loss_scale`                                     | 1            | -               | 最小 loss scale       | 通用默认                         |\n",
        "| `zero_optimization.stage`                                 | 2            | 0\\~3            | ZeRO 优化阶段           | 单机单卡可 1 或 2，多卡可 3            |\n",
        "| `zero_optimization.offload_optimizer.device`              | none         | cpu, nvme, none | 优化器 offload         | 多机大模型必开                      |\n",
        "| `zero_optimization.offload_param.device`                  | none         | cpu, nvme, none | 参数 offload          | 多机多卡可开                       |\n",
        "| `zero_optimization.overlap_comm`                          | true         | true/false      | 通信和计算重叠             | 多机必开，单机可关                    |\n",
        "| `zero_optimization.contiguous_gradients`                  | true         | true/false      | 梯度连续内存              | 通用保持 true                    |\n",
        "| `gradient_clipping`                                       | 1.0          | ≥0              | 防止梯度爆炸              | 1.0 足够                       |\n",
        "| `wall_clock_breakdown`                                    | false        | true/false      | 打印详细时间              | 单机 False 就够                  |\n",
        "| `steps_per_checkpoint`                                    | 500          | 正整数             | 每多少步保存 checkpoint   | 数据量小可调小                      |\n",
        "| `activation_checkpointing.partition_activations`          | true         | true/false      | 激活检查点               | 大模型必开                        |\n",
        "| `activation_checkpointing.contiguous_memory_optimization` | true         | true/false      | 内存优化                | 通用保持 True                    |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "单机单卡：只会在一张 GPU 上训练，micro batch 小，ZeRO stage 可降为 1。\n",
        "\n",
        "单机多卡：train_micro_batch_size_per_gpu × GPU 数 = train_batch_size。开启 ZeRO stage 2 或 3，offload 可选 CPU/NVMe。\n",
        "\n",
        "多机多卡：DeepSpeed 启动命令加 --num_nodes、--num_gpus，ZeRO stage 2~3，offload 参数必开。"
      ],
      "metadata": {
        "id": "j8iii0NU_OzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA-Factory + DeepSpeed 启动命令模板"
      ],
      "metadata": {
        "id": "DZduKbVh-VyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepSpeed + PyTorch + LLaMA-Factory 都支持 TensorBoard 本地可视化：\n",
        "\n",
        "```bash\n",
        "pip install tensorboard\n",
        "```\n",
        "\n",
        "- 训练脚本中加参数：\n",
        "\n",
        "```bash\n",
        "--report_to tensorboard\n",
        "```\n",
        "\n",
        "然后启动 TensorBoard：\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir ./output/Zhenhuan_Style_SFT\n",
        "```\n",
        "\n",
        "\n",
        "在浏览器中访问 http://localhost:6006 即可看到训练曲线、loss、lr 等指标。\n"
      ],
      "metadata": {
        "id": "1ZfU6BtPFSao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 训练参数"
      ],
      "metadata": {
        "id": "WSEakS3-8oPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 环境变量参数\n",
        "\n",
        "| 参数/变量名            | 作用                   | 备注                                             |\n",
        "| ---------------------- | ---------------------- | ------------------------------------------------ |\n",
        "| `CUDA_VISIBLE_DEVICES` | 指定使用的 GPU 索引    | 从 0 开始。0 表示使用第一张卡。                  |\n",
        "| `FORCE_TORCHRUN`       | 强制使用 torchrun 启动 | 解决 LLaMA-Factory 中 DeepSpeed 的启动检查问题。 |\n",
        "| `MODEL_PATH`           | 基础模型的路径或 ID    | 可为 HuggingFace Hub、ModelScope ID 或本地路径。 |\n",
        "| `OUTPUT_PATH_SFT`      | 训练输出目录           | 存放 checkpoints、适配器权重和日志。             |\n",
        "| `DATASET_NAME_SFT`     | 使用的数据集名称       | 必须是 `dataset_info.json` 中注册过的名称。      |\n",
        "\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "F1DyncvzAIcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 核心参数\n",
        "\n",
        "| 参数                     | 作用             | 备注                                  |\n",
        "| ------------------------ | ---------------- | ------------------------------------- |\n",
        "| `--stage sft`            | 指定训练阶段     | `sft` 表示监督微调。                  |\n",
        "| `--do_train`             | 执行训练         | 必须添加此参数才会开始训练。          |\n",
        "| `--model_name_or_path`   | 模型名称或路径   | 引用 `$MODEL_PATH`。                  |\n",
        "| `--dataset`              | 数据集名称或路径 | 引用 `$DATASET_NAME_SFT`。            |\n",
        "| `--template qwen`        | 对话模板         | 确保与基础模型（如 Qwen）的格式一致。 |\n",
        "| `--finetuning_type lora` | 微调类型         | `lora` 是最常用的高效微调方法。       |\n",
        "| `--output_dir`           | 输出目录         | 引用 `$OUTPUT_PATH_SFT`。             |\n",
        "\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "p_oqgtit8adK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 训练控制参数\n",
        "\n",
        "| 参数                            | 作用             | 备注                                             |\n",
        "| ------------------------------- | ---------------- | ------------------------------------------------ |\n",
        "| `--overwrite_cache`             | 覆盖数据缓存     | 更换数据集或格式后建议使用。                     |\n",
        "| `--overwrite_output_dir`        | 覆盖输出目录     | 允许覆盖之前训练产生的文件。                     |\n",
        "| `--cutoff_len`                  | 最大序列长度     | 单位为 token，依据显存与任务设置。               |\n",
        "| `--per_device_train_batch_size` | 单卡 batch size  | 越大通常训练更稳定，受限于显存。                 |\n",
        "| `--gradient_accumulation_steps` | 梯度累积步数     | 提升等效 batch size，适用于显存有限情况。        |\n",
        "| `--learning_rate`               | 学习率           | QLoRA 推荐起点为 `1e-4`。                        |\n",
        "| `--num_train_epochs`            | 训练轮数         | SFT 阶段通常 1~3 轮即可。                        |\n",
        "| `--plot_loss`                   | 绘制训练损失曲线 | 会生成 `training_loss.png`，保存于输出目录。     |\n",
        "| `--fp16`                        | 启用半精度训练   | 使用 16-bit 浮点数训练，节省显存、提升训练速度。 |\n",
        "\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "vVICdNou8ag5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### QLoRA 量化参数\n",
        "\n",
        "| 参数                      | 作用     | 备注                                         |\n",
        "| ------------------------- | -------- | -------------------------------------------- |\n",
        "| `--quantization_bit 4`    | 量化位数 | 4-bit 是 QLoRA 的核心设定。                  |\n",
        "| `--double_quantization`   | 双重量化 | 节省显存，性能影响小，推荐开启。             |\n",
        "| `--quantization_type nf4` | 量化类型 | `nf4 (Normal Float 4)` 是推荐的 4-bit 类型。 |\n",
        "\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "DJ1B-dih8aj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### LoRA 参数\n",
        "\n",
        "| 参数             | 作用              | 备注                                              |\n",
        "| ---------------- | ----------------- | ------------------------------------------------- |\n",
        "| `--lora_rank`    | LoRA 矩阵秩 (r)   | 决定可训练参数量，常用值：8, 16, 32, 64。         |\n",
        "| `--lora_alpha`   | LoRA 缩放因子     | 通常设置为 `lora_rank` 的 2 倍。                  |\n",
        "| `--lora_dropout` | LoRA Dropout 比例 | 常设为 0.05 或 0.1，用于防止过拟合。              |\n",
        "| `--lora_target`  | LoRA 目标层       | **推荐做法**：对于 LLaMA-Factory，可以直接设置为 all，框架会自动识别并选择模型中所有可用的线性层（如 q_proj, v_proj, k_proj, o_proj, gate_proj 等），这是最省心且效果通常最好的选择。 |"
      ],
      "metadata": {
        "id": "YhBTgwDP8amg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "单机单卡可以直接运行上面的脚本，DeepSpeed 会自动管理内存和优化。\n",
        "\n",
        "如果以后想多机，只需在命令里加上：\n",
        "\n",
        "```bash\n",
        "--hostfile ./my_hostfile\n",
        "```\n",
        "\n",
        "并在 hostfile 中列出各机器 IP 和 GPU 数量，DeepSpeed 会自动分布式训练。\n"
      ],
      "metadata": {
        "id": "p1EaFGvwANM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### hostfile.txt 示例（单机单卡）\n",
        "\n",
        "```\n",
        "localhost slots=1\n",
        "```\n",
        "\n",
        "- **localhost**：本机名称或 IP 地址\n",
        "- **slots=1**：本机可用 GPU 数量，这里是 1 张 GPU\n",
        "\n",
        "如果你有多张 GPU，可以写成：\n",
        "\n",
        "```\n",
        "localhost slots=4\n",
        "```\n",
        "\n",
        "表示本机有 4 张 GPU 可用，DeepSpeed 会在 4 张卡上分布训练任务。\n"
      ],
      "metadata": {
        "id": "Uo_tQNgGAxry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### hostfile.txt 示例（多机多卡）\n",
        "\n",
        "假设有两台机器：\n",
        "\n",
        "- 机器 A，IP: 192.168.1.10，有 4 张 GPU\n",
        "- 机器 B，IP: 192.168.1.11，有 2 张 GPU\n",
        "\n",
        "hostfile 可以写成：\n",
        "\n",
        "```\n",
        "192.168.1.10 slots=4\n",
        "192.168.1.11 slots=2\n",
        "```\n",
        "\n",
        "- DeepSpeed 会自动把 batch 和计算任务按 GPU 数量分配\n",
        "- 只需在 **一台机器上运行训练命令**，DeepSpeed 会通过 SSH 调用其他机器"
      ],
      "metadata": {
        "id": "kAimIuSQAxuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 注意事项\n",
        "\n",
        "1. **单机单卡**：hostfile 可以省略，直接在本机跑即可\n",
        "2. **多机训练**：\n",
        "   - 确保所有机器能通过 SSH 免密访问\n",
        "   - 所有机器都安装好相同版本的 Python、依赖和 PyTorch/DeepSpeed\n",
        "3. **slots 和 batch size**：每张 GPU batch size * slots = 全局 batch size\n",
        "4. **单机多卡**：hostfile 也可以写 localhost slots=4 这样 DeepSpeed 会按 GPU 分配"
      ],
      "metadata": {
        "id": "2wL7wcjeAPJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第五部分：GRPO 微调\n",
        "\n"
      ],
      "metadata": {
        "id": "FACMjWXFgQs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### GRPO (Generative Replay Policy Optimization)\n",
        "\n",
        "**目标**：提升模型的推理能力和事实准确性。GRPO 是一种新颖的对齐算法，它通过让模型重新生成并评估自己的答案来进行学习和优化，特别适合数学计算、代码生成、事实问答等需要高精度的任务。\n",
        "\n",
        "**本章方案**：我们将采用 `TRL` 和 `EasyR1` 的原生工作流。首先在本章内准备 `GSM8K` 数据文件，然后使用 `TRL` 库编写脚本完成 SFT 预备训练，最后切换到 `EasyR1` 框架来执行专业的 GRPO 训练。"
      ],
      "metadata": {
        "id": "sQoC-MMtrUH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GSM8K 数据集准备\n",
        "\n",
        "在开始训练之前，我们需要为 GRPO 准备 `GSM8K` 数据集文件。"
      ],
      "metadata": {
        "id": "ocrwZvQWzF1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file: prepare_gsm8k_for_grpo.py\n",
        "import json\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. 配置与辅助函数 ---\n",
        "\n",
        "# 定义系统提示，这是我们希望模型遵循的指令，确保输出格式统一\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "# 定义一个“思维链”(Chain-of-Thought)示例，用于给模型一个清晰的模仿样本\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "The single-digit numbers are 1, 2, 3, 4, 5, 6, 7, 8, 9. A prime number is a number greater than 1 that has no positive divisors other than 1 and itself. 7 is a prime number. 9 is divisible by 3. 8 is divisible by 2.\n",
        "</reasoning>\n",
        "<answer>\n",
        "7\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def ensure_dir_exists(path):\n",
        "    \"\"\"确保文件所在的目录存在。\"\"\"\n",
        "    dir_name = os.path.dirname(path)\n",
        "    if dir_name and not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "        print(f\"创建目录: {dir_name}\")\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    \"\"\"\n",
        "    从原始答案文本（例如 '...过程... #### 7'）中稳健地提取最终的纯数字答案。\n",
        "    \"\"\"\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    # 分割字符串并取最后一部分，去除可能的前后空格\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "# --- 2. 核心格式化函数 ---\n",
        "\n",
        "def format_gsm8k_for_grpo(dataset, output_path, use_one_shot=True):\n",
        "    \"\"\"\n",
        "    加载原始 GSM8K 数据集，将其完全格式化为 GRPO 训练所需的最终格式，并保存到 JSONL 文件。\n",
        "\n",
        "    最终格式: {\"prompt\": [{\"role\": \"system\", ...}, {\"role\": \"user\", ...}], \"answer\": \"...\"}\n",
        "    \"\"\"\n",
        "    ensure_dir_exists(output_path)\n",
        "\n",
        "    print(f\"开始进行完整的格式化处理并保存到 {output_path}...\")\n",
        "\n",
        "    processed_count = 0\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        # 使用 tqdm 显示进度条\n",
        "        for item in tqdm(dataset, desc=\"格式化 GSM8K 数据\"):\n",
        "            # 1. 构建聊天格式的 prompt 列表，首先是系统提示\n",
        "            prompt_list = [{'role': 'system', 'content': SYSTEM_PROMPT}]\n",
        "\n",
        "            # 2. (可选) 添加 one-shot 示例来引导模型学习格式\n",
        "            if use_one_shot:\n",
        "                prompt_list.extend([\n",
        "                    {'role': 'user', 'content': 'What is the largest single-digit prime number?'},\n",
        "                    {'role': 'assistant', 'content': XML_COT_FORMAT}\n",
        "                ])\n",
        "\n",
        "            # 3. 添加当前样本的实际问题 (原始键名为 'question')\n",
        "            prompt_list.append({'role': 'user', 'content': item['question']})\n",
        "\n",
        "            # 4. 提取纯净答案 (原始键名为 'answer')\n",
        "            answer_text = item.get('answer', '')\n",
        "            extracted_answer = extract_hash_answer(answer_text)\n",
        "\n",
        "            # 5. 只写入包含有效答案的样本，确保数据质量\n",
        "            if extracted_answer is not None:\n",
        "                formatted_item = {\n",
        "                    \"prompt\": prompt_list,\n",
        "                    \"answer\": extracted_answer\n",
        "                }\n",
        "                f.write(json.dumps(formatted_item, ensure_ascii=False) + '\\n')\n",
        "                processed_count += 1\n",
        "\n",
        "    print(f\"✅ 处理完成！共 {processed_count} 条有效样本已保存到: {output_path}\")\n",
        "\n",
        "\n",
        "print(\"正在从 Hugging Face Hub 加载 gsm8k 数据集...\")\n",
        "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "format_gsm8k_for_grpo(gsm8k_dataset, \"data/gsm8k_grpo_local.jsonl\")\n"
      ],
      "metadata": {
        "id": "6N3oZ1kpzW8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行 python prepare_gsm8k_for_grpo.py 来生成 GRPO 训练所需的数据文件。"
      ],
      "metadata": {
        "id": "rNXkYR9vzeYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用 EasyR1 执行 GRPO 训练\n",
        "\n",
        "现在，我们使用 `EasyR1` 框架在 SFT 的基础上进行 GRPO 训练。"
      ],
      "metadata": {
        "id": "GedCp_DvzxF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 安装 EasyR1"
      ],
      "metadata": {
        "id": "oaWDo4gVz0cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(work_dir)\n",
        "print(f\"📂 当前工作目录已切换到: {work_dir.resolve()}\")"
      ],
      "metadata": {
        "id": "L-XYPTufzuUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # 克隆 EasyR1 仓库\n",
        "# !git clone https://github.com/hiyouga/EasyR1.git\n",
        "\n",
        "# # 安装 EasyR1 及其依赖\n",
        "# %cd EasyR1\n",
        "# !pip install -e ."
      ],
      "metadata": {
        "id": "Z2tHr0ja0T1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_PATH=\"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "\n",
        "# !python -m verl.trainer.main \\\n",
        "#     config=examples/config.yaml \\\n",
        "#     worker.actor.model.model_path=${MODEL_PATH} \\\n",
        "#     dataset.train_path=data/gsm8k_grpo_local.jsonl\n"
      ],
      "metadata": {
        "id": "ryke00pBQXW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 执行 GRPO 训练 (使用 TRL 自定义脚本)\n",
        "\n",
        "创建一个完整的 Python 脚本 `train_grpo.py`，它将实现 GRPO 的训练循环。"
      ],
      "metadata": {
        "id": "IHPgxTfJRDdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file: train_grpo.py\n",
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# --- 1. 基本设置和日志记录 ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- 新增：环境诊断检查 ---\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"✅ CUDA is available. Found {torch.cuda.device_count()} GPU(s).\")\n",
        "    logger.info(f\"Current device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    logger.warning(\"⚠️ CUDA is not available. Training will run on CPU. Please check your NVIDIA driver and PyTorch installation.\")\n",
        "\n",
        "# --- 2. 通过环境变量进行配置 ---\n",
        "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen/Qwen1.5-0.5B-Chat\")\n",
        "# 指向由 prepare_gsm8k_for_grpo.py 生成的、已完全格式化好的数据集\n",
        "DATASET_PATH = os.getenv(\"DATASET_PATH\", \"data/gsm8k_grpo_local.jsonl\")\n",
        "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"outputs/GSM8K_GRPO_Final\")\n",
        "RUN_NAME = os.getenv(\"RUN_NAME\", \"grpo-qwen1.5-0.5b-gsm8k-final\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "logger.info(f\"模型 ID: {MODEL_NAME}\")\n",
        "logger.info(f\"数据集路径: {DATASET_PATH}\")\n",
        "logger.info(f\"输出目录: {OUTPUT_DIR}\")\n",
        "logger.info(f\"运行名称: {RUN_NAME}\")\n",
        "\n",
        "# --- 3. 数据集加载 ---\n",
        "# 数据已由预处理脚本完全格式化，此处直接加载即可。\n",
        "try:\n",
        "    logger.info(f\"正在从预处理文件加载数据集: {DATASET_PATH}\")\n",
        "    dataset = load_dataset('json', data_files=DATASET_PATH, split='train')\n",
        "    logger.info(f\"数据集加载成功，样本数量: {len(dataset)}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"加载预处理数据集失败: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 4. 奖励函数定义 ---\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"从模型生成的 XML 格式文本中提取 <answer> 标签内的内容。\"\"\"\n",
        "    try:\n",
        "        answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
        "        return answer\n",
        "    except IndexError:\n",
        "        logger.warning(\"Failed to extract answer from XML format.\")\n",
        "        return \"\"\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    \"\"\"主要奖励：根据答案正确性给予高分。\"\"\"\n",
        "    responses = [comp[0]['content'] for comp in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    if prompts and answer and responses and extracted_responses:\n",
        "        logger.info(f\"问题:\\n{prompts[0][-1]['content']}\\n标准答案:\\n{answer[0]}\\n模型输出:\\n{responses[0]}\\n提取的答案:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if resp == ans else 0.0 for resp, ans in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"辅助奖励：如果答案是数字，给予少量分数。\"\"\"\n",
        "    responses = [comp[0]['content'] for comp in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"辅助奖励：如果格式基本正确，给予少量分数。\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [comp[0][\"content\"] for comp in completions]\n",
        "    return [0.5 if re.search(pattern, r, re.DOTALL) else 0.0 for r in responses]\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"辅助奖励：根据 XML 标签的完整性给予精细分数。\"\"\"\n",
        "    def count_xml(text):\n",
        "        score = 0.0\n",
        "        if \"<reasoning>\" in text: score += 0.125\n",
        "        if \"</reasoning>\" in text: score += 0.125\n",
        "        if \"<answer>\" in text: score += 0.125\n",
        "        if \"</answer>\" in text: score += 0.125\n",
        "        return score\n",
        "    contents = [comp[0][\"content\"] for comp in completions]\n",
        "    return [count_xml(c) for c in contents]\n",
        "\n",
        "# --- 5. 模型和 Tokenizer 加载 ---\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\" # 自动将模型加载到 GPU\n",
        "    )\n",
        "    logger.info(\"模型加载成功。\")\n",
        "\n",
        "    # --- （可选）高级优化 ---\n",
        "    # 如果你使用的是 PyTorch 2.0 或更高版本，取消下面的注释可以极大地加速训练\n",
        "    # model = torch.compile(model)\n",
        "    # print(\"INFO: 已启用 torch.compile() 进行加速。\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"加载模型失败: {e}.\")\n",
        "    raise\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# --- 6. PEFT (LoRA) 配置 ---\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=\"all-linear\", # 自动匹配所有线性层，避免出错\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_dropout=0.05,\n",
        ")\n",
        "\n",
        "# --- 7. 训练参数配置 ---\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    run_name=RUN_NAME,\n",
        "    learning_rate=5e-6,\n",
        "    logging_steps=5,\n",
        "    # bf16=True,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_generations=4,\n",
        "    max_prompt_length=512,\n",
        "    max_completion_length=256,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=10,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# --- 8. 初始化 GRPOTrainer ---\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config\n",
        ")\n",
        "\n",
        "# --- 9. 开始训练 ---\n",
        "logger.info(\"开始 GRPO 训练...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    logger.info(\"训练完成！\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"训练过程中发生错误: {e}\", exc_info=True)\n",
        "    raise"
      ],
      "metadata": {
        "id": "z78V1R00R89q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 推理测试\n"
      ],
      "metadata": {
        "id": "UcwcRarvcMA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file: inference.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "\n",
        "# --- 1. 配置 ---\n",
        "# 基础模型 ID (必须与训练时使用的模型一致)\n",
        "BASE_MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen/Qwen1.5-0.5B-Chat\")\n",
        "# 训练时使用的输出目录 (与你的训练脚本保持一致)\n",
        "OUTPUT_DIR_FROM_TRAINING = os.getenv(\"OUTPUT_DIR\", \"outputs/GSM8K_GRPO_Final\")\n",
        "\n",
        "# --- 自动查找最新的 checkpoint ---\n",
        "print(f\"INFO: 正在 '{OUTPUT_DIR_FROM_TRAINING}' 目录中查找最新的 checkpoint...\")\n",
        "checkpoints = glob.glob(os.path.join(OUTPUT_DIR_FROM_TRAINING, \"checkpoint-*\"))\n",
        "if not checkpoints:\n",
        "    raise ValueError(f\"在目录 '{OUTPUT_DIR_FROM_TRAINING}' 中找不到任何 checkpoint。请确保路径正确且训练已保存。\")\n",
        "\n",
        "# 按数字大小排序找到最新的 checkpoint\n",
        "latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
        "ADAPTER_PATH = latest_checkpoint\n",
        "print(f\"INFO: 自动找到最新的 checkpoint: {ADAPTER_PATH}\")\n",
        "\n",
        "\n",
        "# --- 2. 加载模型和适配器 ---\n",
        "print(f\"INFO: 正在加载基础模型: {BASE_MODEL_NAME}...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"INFO: 基础模型加载成功。\")\n",
        "\n",
        "print(f\"INFO: 正在加载 Tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"INFO: 正在加载 LoRA 适配器: {ADAPTER_PATH}...\")\n",
        "# 加载 LoRA 适配器并将其应用到基础模型上\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "print(\"INFO: LoRA 适配器加载成功。\")\n",
        "\n",
        "# --- 3. 合并模型 ---\n",
        "print(\"INFO: 正在合并模型和适配器...\")\n",
        "# 将 LoRA 模块的权重合并到基础模型中\n",
        "model = model.merge_and_unload()\n",
        "print(\"INFO: 模型合并完成。\")\n",
        "\n",
        "# --- 4. 推理函数 ---\n",
        "# 定义与训练时一致的系统提示\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def generate_response(question: str):\n",
        "    \"\"\"\n",
        "    使用微调后的模型生成对给定问题的回答。\n",
        "    \"\"\"\n",
        "    # 1. 构建聊天格式的 prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "\n",
        "    # 2. 应用聊天模板并转换为输入张量\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # 3. 使用模型生成回答\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"问题: {question}\")\n",
        "    print(\"模型正在生成回答...\")\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=256, # 与训练时的 max_completion_length 保持一致\n",
        "        do_sample=False # 使用贪心解码以获得确定性输出\n",
        "    )\n",
        "\n",
        "    # 4. 解码并清理输出\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    print(\"--- 模型输出 ---\")\n",
        "    print(response)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- 5. 执行推理 ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 准备一些测试问题\n",
        "    test_questions = [\n",
        "        \"Natalia sold 48/2 = 24 clips in the morning. Then she sold 12 clips in the afternoon. How many clips did Natalia sell in total?\",\n",
        "        \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
        "        \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
        "    ]\n",
        "\n",
        "    for q in test_questions:\n",
        "        generate_response(q)\n"
      ],
      "metadata": {
        "id": "LjXbNfDRjSwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第六部分：GRPO 从入门到实践完整教程\n",
        "\n",
        "## 前言\n",
        "\n",
        "本指南将带你走过一条清晰的学习路径，从宏观概念到理论深度，最终落于代码实践。学习路径如下：\n",
        "\n",
        "1. **核心思想与定位**：快速建立对 GRPO 的直观认知，明白它是什么，以及它在众多对齐算法中的位置。\n",
        "2. **深入原理与推导**：理解 GRPO 的数学内核，学习其损失函数如何从第一性原理构建。\n",
        "3. **动手实践与代码**：通过示例，实现自定义的 `GRPOTrainer`。\n",
        "\n",
        "------\n",
        "\n",
        "## 核心思想与定位\n",
        "\n",
        "**一句话理解 GRPO**：**“擒贼先擒王”**\n",
        "\n",
        "想象老师教学生写作文：给出一个标准答案（chosen）和多个错误范例（rejected），目标是生成标准答案，同时避开最容易混淆的错误。GRPO 的精髓是：\n",
        "\n",
        "- **关注最难的负样本**：在一堆 rejected 样本里，模型只需战胜最强的那个。\n",
        "- **整体考虑偏好群组**：与 DPO 只处理一对一不同，GRPO 一次性考虑所有 rejected 样本。\n",
        "\n",
        "### GRPO 的诞生动机\n",
        "\n",
        "在现实标注中，一个 prompt 往往对应一个好回答和多个坏回答。GRPO 的目标是**更高效地利用“一对多”偏好数据**，把学习重点放在最难区分的负样本上。\n",
        "\n",
        "### 横向对比\n",
        "\n",
        "| 特性     | PPO                     | DPO                        | GRPO                                     |\n",
        "| -------- | ----------------------- | -------------------------- | ---------------------------------------- |\n",
        "| 核心目标 | 最大化累积奖励          | 直接从偏好对学习           | 从偏好群组学习，处理“一对多”             |\n",
        "| 数据类型 | (state, action, reward) | (prompt, chosen, rejected) | (prompt, chosen, [rejected_1,...])       |\n",
        "| 工作方式 | 依赖奖励模型            | 跳过奖励模型，用逻辑损失   | DPO 的扩展，用 Hinge Loss 专注最强负样本 |\n",
        "| 对齐方法 | RLHF 核心算法           | 直接对齐方法，不需奖励模型 | DPO 变体，不需奖励模型                   |\n",
        "\n",
        "## GRPO 深入原理与推导\n",
        "\n",
        "### 起点：DPO 的基石\n",
        "\n",
        "要理解 GRPO，必须先理解 DPO 的逻辑。\n",
        "\n",
        "#### 偏好量化（Bradley-Terry 模型）\n",
        "\n",
        "我们如何将“人类更喜欢 $A$ 而不是 $B$”这个模糊概念数学化？Bradley-Terry 模型提供了一个优雅的方案。它假设每个选项背后都有一个潜在的“奖励分数” $r^*$，而偏好概率取决于这些分数的差异：\n",
        "\n",
        "$$\n",
        "p^*(y_c \\succ y_r \\mid x) = \\sigma\\big(r^*(x, y_c) - r^*(x, y_r)\\big)\n",
        "$$\n",
        "\n",
        "这个公式是所有推导的基石，它将主观偏好转化为了一个可计算的概率。\n",
        "\n",
        "#### 奖励定义（DPO 核心洞察）\n",
        "\n",
        "传统方法需要训练一个独立的奖励模型来估算 $r^*$。DPO 的天才之处在于，它发现奖励可以直接用策略模型 $\\pi_\\theta$ 和参考模型 $\\pi_{\\text{ref}}$ 的概率比值来定义：\n",
        "\n",
        "$$\n",
        "r^*(x, y) \\triangleq \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\n",
        "$$\n",
        "\n",
        "这个定义一举两得：它使得奖励变得可直接计算，同时隐式地加入了一个 KL 散度约束，防止 $\\pi_\\theta$ 为了迎合偏好而偏离 $\\pi_{\\text{ref}}$ 太远，从而保证生成质量。\n",
        "\n",
        "#### DPO 损失函数\n",
        "\n",
        "将奖励定义代入偏好概率公式，我们就得到了一个完全由模型概率表示的人类偏好模型。机器学习的目标是最大化观测数据的对数似然，等价于最小化其负对数似然。因此，DPO 的损失函数为：\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma\\big(\\hat{r}_\\theta(y_c) - \\hat{r}_\\theta(y_r)\\big)\n",
        "$$\n",
        "\n",
        "这本质上就是一个逻辑损失 (Logistic Loss)，它驱动模型去增大 $y_c$ 的奖励，同时减小 $y_r$ 的奖励。\n",
        "\n",
        "---\n",
        "\n",
        "### 构思飞跃：从“一对一”到“一对多”\n",
        "\n",
        "重新定义问题：DPO 完美地解决了“一对一”比较。但当面对一个“好”答案 $y_c$ 和一组“坏”答案 $Y_r$ 时，我们的目标是让 $y_c$ 优于整个群组。GRPO 对此提出了一个关键的重新定义：\n",
        "\n",
        "> “优于整个群组” 等价于 “奖励分数高于群组中那个分数最高的成员”。\n",
        "\n",
        "$$\n",
        "y_c \\succ Y_r \\iff r(y_c) > \\max_{y_r \\in Y_r} r(y_r)\n",
        "$$\n",
        "\n",
        "这个定义是 GRPO 的灵魂，它将优化目标从关注所有负样本，聚焦到了那个“最强的对手”上。\n",
        "\n",
        "#### 借鉴 SVM 思想（最大间隔）\n",
        "\n",
        "仅仅要求 $r(y_c)$ 更高还不够鲁棒。我们从经典机器学习算法支持向量机 (SVM) 中借鉴了“最大间隔”思想。SVM 不仅要正确分类，还追求在两类数据之间留出尽可能宽的“安全区”（Margin）。\n",
        "\n",
        "类比：我们可以把 $y_c$ 看作正类，所有 $y_r$ 看作负类。那个奖励最高的 $y_r$ 就好比是离决策边界最近的“支持向量”。\n",
        "\n",
        "目标：我们不仅要让 $r(y_c)$ 战胜 $\\max r(y_r)$，还要让它领先一个安全间隔（margin） $\\alpha$。我们的“成功条件”变为：\n",
        "\n",
        "$$\n",
        "r(y_c) \\ge \\max_{y_r \\in Y_r} r(y_r) + \\alpha\n",
        "$$\n",
        "\n",
        "#### 构建 GRPO 损失函数（Hinge Loss）\n",
        "\n",
        "如何将这个带间隔的“成功条件”转化为损失函数？Hinge Loss（合页损失） $$\\max(0, z)$$ 是完美的工具。当条件满足时，损失为 0；不满足时，损失为正。\n",
        "\n",
        "将“成功条件”移项：\n",
        "\n",
        "$$\n",
        "\\max_{y_r \\in Y_r} r(y_r) - r(y_c) + \\alpha \\le 0\n",
        "$$\n",
        "\n",
        "当括号内的值 $z$ 大于 0 时，说明条件被违反。\n",
        "\n",
        "于是，GRPO 的最终损失函数为：\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{GRPO}} = \\max\\Big(0, \\max_{y_r \\in Y_r} \\hat{r}_\\theta(y_r) - \\hat{r}_\\theta(y_c) + \\alpha \\Big)\n",
        "$$\n",
        "\n",
        "这里，$\\hat{r}_\\theta(y)$定义为：\n",
        "\n",
        "$$\n",
        "\\hat{r}_\\theta(y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\n",
        "$$\n",
        "\n",
        "- 奖励分数既是“相对偏好分”，又隐式包含 KL 散度约束。\n",
        "\n",
        "#### 最大值梯度近似（可选）\n",
        "\n",
        "由于 $$$\\max$$$ 不可导，可使用 softmax 近似：\n",
        "\n",
        "$$\n",
        "\\max_{y_r \\in Y_r} \\hat{r}_\\theta(y_r) \\approx \\frac{\\sum_{y_r} \\hat{r}_\\theta(y_r) e^{\\lambda \\hat{r}_\\theta(y_r)}}{\\sum_{y_r} e^{\\lambda \\hat{r}_\\theta(y_r)}}\n",
        "$$\n",
        "\n",
        "- $\\lambda$ 越大，近似越接近真正的最大值  \n",
        "- 平滑梯度，训练更稳定\n",
        "\n",
        "---\n",
        "\n",
        "### 完整训练流程\n",
        "\n",
        "1. 给定输入 $x$ 和正样本 $y_c$、负样本集合 $Y_r$。  \n",
        "2. 计算奖励分数：\n",
        "\n",
        "$$\n",
        "\\hat{r}_\\theta(y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\n",
        "$$\n",
        "\n",
        "3. 找到最强负样本：\n",
        "\n",
        "$$\n",
        "y_r^* = \\arg\\max_{y_r \\in Y_r} \\hat{r}_\\theta(y_r)\n",
        "$$\n",
        "\n",
        "4. 计算 GRPO 损失：\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{GRPO}} = \\max\\big(0, \\hat{r}_\\theta(y_r^*) - \\hat{r}_\\theta(y_c) + \\alpha \\big)\n",
        "$$\n",
        "\n",
        "5. 反向传播更新策略模型 $\\pi_\\theta$。  \n",
        "\n",
        "这样，GRPO 将“一对多偏好”问题转化为可优化的 Hinge Loss，同时保留了 DPO 的 KL 散度约束和稳定训练优势。\n",
        "\n"
      ],
      "metadata": {
        "id": "u903vn_5oOAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyQ5G4VzpKsN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}