å‚è€ƒæ–‡æ¡£ï¼š

[https://qwen.readthedocs.io/zh-cn/latest/getting_started/concepts.html](https://qwen.readthedocs.io/zh-cn/latest/getting_started/concepts.html)





## 1.å¿«é€Ÿä½¿ç”¨
---åŸºäºhugging face **transformers**åº“æ¥åŠ è½½åƒé—®æ¨¡å‹

```java
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
```

ä¼šé¦–å…ˆåœ¨æœ¬åœ°cacheä¸­æ£€ç´¢æ¨¡å‹æ–‡ä»¶ï¼Œæœªæ‰¾åˆ°ä¼šè¿›è¡Œä¸‹è½½

**æµå¤±è¾“å‡º**ä½¿ç”¨from transformers import TextStreameråº“è½¬åŒ–æ¨¡å‹ç”Ÿæˆçš„token



---åŸºäºvllméƒ¨ç½²åƒé—®æœåŠ¡ï¼Œå¯ä»¥é€šè¿‡openaiæ–¹å¼è®¿é—®æœåŠ¡

## 2.æ ¸å¿ƒæ¦‚å¿µ
**ç°å­˜ç±»å‹ï¼š**åƒé—®ç°æœ‰æ¨¡å‹ç±»å‹

Qwen-Max > Plus > Turbo

**æ¨¡å‹ç±»å‹ï¼š**åƒé—®æ˜¯<font style="color:rgb(0, 0, 0);">å› æœè¯­è¨€æ¨¡å‹ (causal Language Models)ï¼Œä¹Ÿå«å› æœè¯­è¨€æ¨¡å‹ (causal Language Models)æˆ–è€…ä»…è§£ç å™¨è¯­è¨€æ¨¡å‹ (decoder-only language models)ï¼š</font>

<font style="color:rgb(0, 0, 0);">å®ƒä½¿ç”¨ä¹‹å‰ç”Ÿæˆçš„ token ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä¸€æ¬¡ç”Ÿæˆä¸€ä¸ª token çš„æ–‡æœ¬ã€‚</font>

---

**<font style="color:rgb(0, 0, 0);">é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰å’ŒåŸºæ¨¡å‹(Base-model)</font>**

<font style="color:rgb(0, 0, 0);">åŸºç¡€è¯­è¨€æ¨¡å‹ (base language models) æ˜¯åœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šè®­ç»ƒçš„åŸºæœ¬æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚å®ƒä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯æ•æ‰è¯­è¨€çš„ç»Ÿè®¡æ¨¡å¼å’Œç»“æ„ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆè¿è´¯ä¸”å…·æœ‰ä¸Šä¸‹æ–‡å…³è”æ€§çš„æ–‡æœ¬ã€‚</font>

**<font style="color:rgb(0, 0, 0);">è¦ç‚¹ï¼šä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæƒ…å¢ƒå­¦ä¹ ã€ä¸‹æ¸¸å¾®è°ƒç­‰ã€‚</font>**

**<font style="color:rgb(0, 0, 0);">åè®­ç»ƒ (Post-training) å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ (Instruction-tuned models)</font>**

<font style="color:rgb(0, 0, 0);">æŒ‡ä»¤å¾®è°ƒè¯­è¨€æ¨¡å‹ (Instruction-tuned language models) æ˜¯ä¸“é—¨è®¾è®¡ç”¨äºç†è§£å¹¶ä»¥å¯¹è¯é£æ ¼æ‰§è¡Œç‰¹å®šæŒ‡ä»¤çš„æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ç»è¿‡å¾®è°ƒï¼Œèƒ½å‡†ç¡®åœ°è§£é‡Šç”¨æˆ·å‘½ä»¤ï¼Œå¹¶èƒ½ä»¥æ›´é«˜çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ‰§è¡Œè¯¸å¦‚æ‘˜è¦ã€ç¿»è¯‘å’Œé—®ç­”ç­‰ä»»åŠ¡ã€‚</font>

<font style="color:rgb(0, 0, 0);">å¯¹äº Qwen æ¨¡å‹ï¼ŒæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹æ˜¯æŒ‡å¸¦æœ‰ â€œ-Instructâ€ åç¼€çš„æ¨¡å‹ï¼Œä¾‹å¦‚ Qwen2.5-7B-Instruct å’Œ Qwen2.5-72B-Instruct ã€‚</font>

**<font style="color:rgb(0, 0, 0);">è¦ç‚¹ï¼šä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ¨¡å‹è¿›è¡Œå¯¹è¯å¼çš„ä»»åŠ¡æ‰§è¡Œã€ä¸‹æ¸¸å¾®è°ƒç­‰ã€‚</font>**

---

**<font style="color:rgb(0, 0, 0);">Tokens & Tokenization</font>**

<font style="color:rgb(0, 0, 0);">token ä»£è¡¨æ¨¡å‹å¤„ç†å’Œç”Ÿæˆçš„åŸºæœ¬å•ä½ã€‚å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä½¿ç”¨å¤æ‚çš„ tokenization æ¥å¤„ç†äººç±»è¯­è¨€çš„å¹¿é˜”å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒè¯è¡¨å¤§å°å¯æ§ã€‚Qwen è¯è¡¨ç›¸å¯¹è¾ƒå¤§ï¼Œæœ‰ 15 1646 ä¸ª tokenã€‚</font>

<font style="color:rgb(0, 0, 0);">Qwené‡‡ç”¨äº†åä¸ºå­—èŠ‚å¯¹ç¼–ç ï¼ˆByte Pair Encodingï¼Œç®€ç§°BPEï¼‰çš„å­è¯tokenizationæ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•è¯•å›¾å­¦ä¹ èƒ½å¤Ÿç”¨æœ€å°‘çš„ token è¡¨ç¤ºæ–‡æœ¬çš„ token ç»„åˆã€‚</font>

<font style="color:rgb(0, 0, 0);">Qwenè¯è¡¨ä¸­å› BPEè€Œäº§ç”Ÿçš„ token æ•°é‡ä¸º 15 1643 ä¸ªï¼Œè¿™æ˜¯ä¸€ä¸ªé€‚ç”¨äºå¤šç§è¯­è¨€çš„å¤§è¯è¡¨ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œå¯¹äºè‹±è¯­æ–‡æœ¬ï¼Œ1ä¸ªtokenå¤§çº¦æ˜¯3~4ä¸ªå­—ç¬¦ï¼›è€Œå¯¹äºä¸­æ–‡æ–‡æœ¬ï¼Œåˆ™å¤§çº¦æ˜¯1.5~1.8ä¸ªæ±‰å­—ã€‚</font>

---

**<font style="color:rgb(0, 0, 0);">æ§åˆ¶ Token å’Œ å¯¹è¯æ¨¡æ¿</font>**

<font style="color:rgb(0, 0, 0);">æ§åˆ¶ token å’Œå¯¹è¯æ¨¡æ¿éƒ½ä½œä¸ºæŒ‡å¯¼æ¨¡å‹è¡Œä¸ºå’Œè¾“å‡ºçš„æœºåˆ¶ã€‚</font>

<font style="color:rgb(0, 0, 0);">ä» Qwen2.5 å¼€å§‹ï¼ŒQwen æ¨¡å‹å®¶æ—ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å’Œä¸“é¡¹æ¨¡å‹ï¼Œå°†ä½¿ç”¨ç»Ÿä¸€çš„è¯æ±‡è¡¨ï¼Œå…¶ä¸­åŒ…å«äº†æ‰€æœ‰å­ç³»åˆ—çš„æ§åˆ¶ token ã€‚Qwen2.5 çš„è¯æ±‡è¡¨ä¸­æœ‰ 22 ä¸ªæ§åˆ¶ tokenï¼Œä½¿å¾—è¯æ±‡è¡¨çš„æ€»è§„æ¨¡è¾¾åˆ° 15 1665 ã€‚</font>

+ <font style="color:rgb(0, 0, 0);">é€šç”¨ token 1ä¸ªï¼š</font>`<font style="color:rgb(0, 0, 0);"><|endoftext|></font>`
+ <font style="color:rgb(0, 0, 0);">å¯¹è¯ token 2ä¸ªï¼š</font>`<font style="color:rgb(0, 0, 0);"><|im_start|></font>`<font style="color:rgb(0, 0, 0);"> </font><font style="color:rgb(0, 0, 0);">å’Œ</font><font style="color:rgb(0, 0, 0);"> </font>`<font style="color:rgb(0, 0, 0);"><|im_end|></font>`
+ <font style="color:rgb(0, 0, 0);">å·¥å…·è°ƒç”¨ token 2ä¸ªï¼š</font><font style="color:rgb(0, 0, 0);"> </font>`<font style="color:rgb(0, 0, 0);"><tool_call></font>`<font style="color:rgb(0, 0, 0);"> </font><font style="color:rgb(0, 0, 0);">å’Œ</font><font style="color:rgb(0, 0, 0);"> </font>`<font style="color:rgb(0, 0, 0);"></tool_call></font>`
+ <font style="color:rgb(0, 0, 0);">è§†è§‰ç›¸å…³ token 11ä¸ª</font>
+ <font style="color:rgb(0, 0, 0);">ä»£ç ç›¸å…³ token 6ä¸ª</font>

<font style="color:rgb(0, 0, 0);">è¦ç‚¹: Qwen ä½¿ç”¨å¸¦æœ‰æ§åˆ¶ token çš„ ChatML ä½œä¸ºå¯¹è¯æ¨¡æ¿ã€‚</font>

**<font style="color:rgb(0, 0, 0);">é•¿åº¦é™åˆ¶</font>**

<font style="color:rgb(0, 0, 0);">å¯¹äºQwen2.5ï¼Œåœ¨è®­ç»ƒä¸­çš„æ‰“åŒ…åºåˆ—é•¿åº¦ä¸º 3 2768 ä¸ª token</font><font style="color:rgb(0, 0, 0);"> </font>[[4]](https://qwen.readthedocs.io/zh-cn/latest/getting_started/concepts.html#yarn)<font style="color:rgb(0, 0, 0);">ã€‚é¢„è®­ç»ƒä¸­çš„æœ€å¤§æ–‡æ¡£é•¿åº¦å³ä¸ºæ­¤é•¿åº¦ã€‚è€Œåè®­ç»ƒä¸­ï¼Œuserå’Œassistantçš„æœ€å¤§æ¶ˆæ¯é•¿åº¦åˆ™æœ‰æ‰€ä¸åŒã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œassistantæ¶ˆæ¯é•¿åº¦å¯è¾¾ 8192 ä¸ª tokenã€‚</font>

**<font style="color:rgb(0, 0, 0);">è¦ç‚¹ï¼šQwen2 æ¨¡å‹å¯ä»¥å¤„ç† 32K æˆ– 128K token é•¿çš„æ–‡æœ¬ï¼Œå…¶ä¸­ 8K é•¿åº¦å¯ä½œä¸ºè¾“å‡ºã€‚</font>**

## <font style="color:rgb(0, 0, 0);">3.Hugging Face transformers</font>
å­¦ä¼šä½¿ç”¨transformersåº“åŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº¤äº’ï¼ŒåŒ…æ‹¬å¤šè½®å¯¹è¯ã€æ‰¹å¤„ç†å’Œæµå¼è¾“å‡º

**æ˜¾å­˜å ç”¨**

<font style="color:rgb(0, 0, 0);">ä¸€èˆ¬è€Œè¨€ï¼Œæ¨¡å‹åŠ è½½æ‰€éœ€æ˜¾å­˜å¯ä»¥æŒ‰å‚æ•°é‡ä¹˜äºŒè®¡ç®—ï¼Œä¾‹å¦‚ï¼Œ7B æ¨¡å‹éœ€è¦ 14GB æ˜¾å­˜åŠ è½½ï¼Œå…¶åŸå› åœ¨äºï¼Œå¯¹äºå¤§è¯­è¨€æ¨¡å‹ï¼Œè®¡ç®—æ‰€ç”¨æ•°æ®ç±»å‹ä¸º16ä½æµ®ç‚¹æ•°ã€‚å½“ç„¶ï¼Œæ¨ç†è¿è¡Œæ—¶è¿˜éœ€è¦æ›´å¤šæ˜¾å­˜ä»¥è®°å½•æ¿€æ´»çŠ¶æ€ã€‚</font>

<font style="color:rgb(0, 0, 0);">å¯¹äº </font>`<font style="color:rgb(0, 0, 0);">transformers</font>`<font style="color:rgb(0, 0, 0);"> ï¼Œæ¨èåŠ è½½æ—¶ä½¿ç”¨ </font>`<font style="color:rgb(0, 0, 0);">torch_dtype="auto"</font>`<font style="color:rgb(0, 0, 0);"> ï¼Œè¿™æ ·æ¨¡å‹å°†ä»¥ </font>`<font style="color:rgb(0, 0, 0);">bfloat16</font>`<font style="color:rgb(0, 0, 0);"> æ•°æ®ç±»å‹åŠ è½½ã€‚å¦åˆ™ï¼Œé»˜è®¤ä¼šä»¥ </font>`<font style="color:rgb(0, 0, 0);">float32</font>`<font style="color:rgb(0, 0, 0);"> æ•°æ®ç±»å‹åŠ è½½ï¼Œæ‰€éœ€æ˜¾å­˜å°†ç¿»å€ã€‚ä¹Ÿå¯ä»¥æ˜¾å¼ä¼ å…¥ </font>`<font style="color:rgb(0, 0, 0);">torch.bfloat16</font>`<font style="color:rgb(0, 0, 0);"> æˆ– </font>`<font style="color:rgb(0, 0, 0);">torch.float16</font>`<font style="color:rgb(0, 0, 0);"> ä½œä¸º </font>`<font style="color:rgb(0, 0, 0);">torch_dtype</font>`<font style="color:rgb(0, 0, 0);"> ã€‚</font>

## <font style="color:rgb(0, 0, 0);">4.æœ¬åœ°éƒ¨ç½²</font>
### --Ollamaæœ¬åœ°éƒ¨ç½²
<font style="color:rgb(0, 0, 0);">é€‚ç”¨äºMacOSã€Linuxå’ŒWindowsæ“ä½œç³»ç»Ÿã€‚</font>

<font style="color:rgb(0, 0, 0);">ä¸‹è½½Ollamaåï¼Œä¸€æ¡å‘½ä»¤å¯åŠ¨ï¼š</font>

`<font style="color:rgb(0, 0, 0);">ollama run qwen2.5</font>`<font style="color:rgb(0, 0, 0);">  
</font>`<font style="color:rgb(0, 0, 0);">ollama</font>`<font style="color:rgb(0, 0, 0);">å¹¶ä¸æ‰˜ç®¡åŸºæ¨¡å‹ã€‚å³ä¾¿æ¨¡å‹æ ‡ç­¾ä¸å¸¦instructåç¼€ï¼Œå®é™…ä¹Ÿæ˜¯instructæ¨¡å‹ã€‚</font>

**<font style="color:rgb(0, 0, 0);">---ç”¨Ollamaè¿è¡Œä½ è‡ªå·±çš„GGUFæ–‡ä»¶</font>**

<font style="color:rgb(0, 0, 0);">é¦–å…ˆè¦åˆ›å»ºä¸€ä¸ªåä¸ºModelfileçš„æ–‡ä»¶ï¼Œå°†è‡ªå·±çš„ggufæ–‡ä»¶åŠ è½½</font>

---

### ---MLX-LMæœ¬åœ°éƒ¨ç½²
å¯ä»¥è¿è¡Œåœ¨MacOSç³»ç»Ÿçš„æœ¬åœ°æ¨¡å‹å¹³å°

éœ€è¦è¿è¡ŒMLXæ ¼å¼çš„æ¨¡å‹æ–‡ä»¶

ä¸€æ¡å‘½ä»¤ä¹Ÿå¯ä»¥å°†æ™®é€šæ¨¡å‹è½¬æ¢ä¸ºmlxæ ¼å¼çš„æ¨¡å‹

`mlx_lm.convert --hf-path Qwen/Qwen2.5-7B-Instruct --mlx-path mlx/Qwen2.5-7B-Instruct/ -q`

---

### ---llama.cpp
**1.é¦–å…ˆclone github ä¸Šçš„llama-cppé¡¹ç›®åˆ°æœ¬åœ°**

```java
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

2.ç¼–è¯‘llama-cppï¼Œå‰æéœ€è¦æœ¬åœ°è¦æœ‰c++çš„ç¼–è¯‘ç¯å¢ƒ

```java
cmake -B build
cmake --build build --config Release
```

3.ä¸‹è½½æ¨¡å‹æˆ–è€…è‡ªè¡Œè½¬æ¢æ ¼å¼

ä¸‹è½½æ¨¡å‹å³åˆ°huggingfaceä¸Šç›´æ¥ä¸‹è½½å¯¹åº”æ¨¡å‹çš„ggufæ ¼å¼ï¼Œè¿™é‡Œè´´ä¸€ä¸ªä¸‹è½½qwenæ¨¡å‹çš„ç¤ºä¾‹åœ°å€ï¼š

[https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/tree/main](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/tree/main)

å»ºè®®å°†æ–‡ä»¶ä¸‹è½½åˆ°/modelsè·¯å¾„ä¸‹

4.ç„¶åå°±å¯ä»¥è¿è¡Œllama-cliå‘½ä»¤åŠ è½½å¤§æ¨¡å‹

é¦–å…ˆè¿›å…¥åˆ°/llama.cpp/build/binä¸‹

1. **<font style="color:rgb(0, 0, 0);">å¯¹è¯æ¨¡å¼å¯åŠ¨ï¼ˆå•è½®å¯¹è¯ï¼‰</font>**

```java
./llama-cli -m qwen2.5-0.5b-instruct-q5_k_m.gguf \
    -co -cnv -p "You are Qwen, created by Alibaba Cloud. You are a helpful assistant." \
    -fa -ngl 80 -n 512
```

**<font style="color:rgb(0, 0, 0);">-m æˆ– â€“model</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">æ˜¾ç„¶ï¼Œè¿™æ˜¯æ¨¡å‹è·¯å¾„ã€‚</font>

**<font style="color:rgb(0, 0, 0);">-co æˆ– â€“color</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">ä¸ºè¾“å‡ºç€è‰²ä»¥åŒºåˆ†æç¤ºè¯ã€ç”¨æˆ·è¾“å…¥å’Œç”Ÿæˆçš„æ–‡æœ¬ã€‚æç¤ºæ–‡æœ¬ä¸ºæ·±é»„è‰²ï¼›ç”¨æˆ·æ–‡æœ¬ä¸ºç»¿è‰²ï¼›ç”Ÿæˆçš„æ–‡æœ¬ä¸ºç™½è‰²ï¼›é”™è¯¯æ–‡æœ¬ä¸ºçº¢è‰²ã€‚</font>

**<font style="color:rgb(0, 0, 0);">-cnv æˆ– â€“conversation</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">åœ¨å¯¹è¯æ¨¡å¼ä¸‹è¿è¡Œã€‚ç¨‹åºå°†ç›¸åº”åœ°åº”ç”¨èŠå¤©æ¨¡æ¿ã€‚</font>

**<font style="color:rgb(0, 0, 0);">-p æˆ– â€“prompt</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">åœ¨å¯¹è¯æ¨¡å¼ä¸‹ï¼Œå®ƒä½œä¸ºç³»ç»Ÿæç¤ºã€‚</font>

**<font style="color:rgb(0, 0, 0);">-fa æˆ– â€“flash-attn</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">å¦‚æœç¨‹åºç¼–è¯‘æ—¶æ”¯æŒ GPUï¼Œåˆ™å¯ç”¨Flash Attentionæ³¨æ„åŠ›å®ç°ã€‚</font>

**<font style="color:rgb(0, 0, 0);">-ngl æˆ– â€“n-gpu-layers</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">å¦‚æœç¨‹åºç¼–è¯‘æ—¶æ”¯æŒ GPUï¼Œåˆ™å°†è¿™ä¹ˆå¤šå±‚åˆ†é…ç»™ GPU è¿›è¡Œè®¡ç®—ã€‚</font>

**<font style="color:rgb(0, 0, 0);">-n æˆ– â€“predict</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">è¦é¢„æµ‹çš„tokenæ•°é‡ã€‚</font>

<font style="color:rgb(0, 0, 0);">ä½ ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ¢ç´¢å…¶ä»–é€‰é¡¹ï¼š</font>

<font style="color:rgb(0, 0, 0);background-color:rgb(248, 248, 248);">./llama-cli -h</font>

2. **<font style="color:rgb(0, 0, 0);">äº’åŠ¨æ¨¡å¼è®¿é—®å¤§æ¨¡å‹ï¼ˆè¿ç»­å¯¹è¯ï¼‰</font>**

```java
./llama-cli -m /home/zhangsh82/data/zsh/llama.cpp/models/qwen2.5-0.5b-instruct-q5_k_m.gguf \
    -co -sp -i -if -p "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n" \
    --in-prefix "<|im_start|>user\n" --in-suffix "<|im_end|>\n<|im_start|>assistant\n" \
    -fa -ngl 80 -n 512
```

<font style="color:rgb(0, 0, 0);">æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨äº†ä¸€äº›æ–°çš„é€‰é¡¹ï¼š</font>

**<font style="color:rgb(0, 0, 0);">-sp æˆ– â€“special</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">æ˜¾ç¤ºç‰¹æ®Štokenã€‚</font>

**<font style="color:rgb(0, 0, 0);">-i æˆ– â€“interactive</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">è¿›å…¥äº’åŠ¨æ¨¡å¼ã€‚ä½ å¯ä»¥ä¸­æ–­æ¨¡å‹ç”Ÿæˆå¹¶æ·»åŠ æ–°æ–‡æœ¬ã€‚</font>

**<font style="color:rgb(0, 0, 0);">-if æˆ– â€“interactive-first</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">ç«‹å³ç­‰å¾…ç”¨æˆ·è¾“å…¥ã€‚å¦åˆ™ï¼Œæ¨¡å‹å°†ç«‹å³è¿è¡Œå¹¶æ ¹æ®æç¤ºç”Ÿæˆæ–‡æœ¬ã€‚</font>

**<font style="color:rgb(0, 0, 0);">-p æˆ– â€“prompt</font>****<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">åœ¨äº’åŠ¨æ¨¡å¼ä¸‹ï¼Œè¿™æ˜¯æ¨¡å‹ç»­å†™ç”¨çš„ä¸Šæ–‡ã€‚</font>

`**<font style="color:rgb(0, 0, 0);">--in-prefix</font>**`**<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">ç”¨æˆ·è¾“å…¥é™„åŠ çš„å‰ç¼€å­—ç¬¦ä¸²ã€‚</font>

`**<font style="color:rgb(0, 0, 0);">--in-suffix</font>**`**<font style="color:rgb(0, 0, 0);">:</font>**

<font style="color:rgb(0, 0, 0);">ç”¨æˆ·è¾“å…¥é™„åŠ çš„åç¼€å­—ç¬¦ä¸²ã€‚</font>

3. <font style="color:rgb(0, 0, 0);">éäº’åŠ¨æ¨¡å¼ï¼ˆä»…æ”¯æŒç»­å†™åŠŸèƒ½ï¼‰</font>

```java
./llama-cli -m Qwen2.5-7b-instruct-q5_k_m.gguf \
    -co -sp -p "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\ngive me a short introduction to LLMs.<|im_end|>\n<|im_start|>assistant\n" \
    -fa -ngl 80 -n 512
```

## 5.é‡åŒ–
### --é‡åŒ–å®šä¹‰åŠæ¦‚å¿µ
é‡åŒ–é¦–å…ˆåˆ†ä¸ºPTQ(è®­ç»ƒåé‡åŒ–)å’ŒQATï¼ˆè®­ç»ƒä¸­é‡åŒ–ï¼‰ï¼Œæˆ‘ä»¬è¿™é‡Œåªä»‹ç»PTQ

é‡åŒ–è¿˜åˆ†ä¸ºæƒé‡é‡åŒ–å’Œæ¿€æ´»é‡åŒ–

--æƒé‡é‡åŒ–å³å°†è®­ç»ƒåçš„æƒé‡ï¼ˆå®šå€¼ï¼‰é‡åŒ–åˆ°æŒ‡å®šå¤§å°ï¼Œè¿™éƒ¨åˆ†é‡åŒ–ä¸éœ€è¦æ ¡å‡†æ•°æ®

--æ¿€æ´»å€¼é‡åŒ–æ˜¯æ¨¡å‹åœ¨æ‰§è¡Œå‰å‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸€å±‚è®¡ç®—å‡ºæ¥çš„â€œä¸­é—´ç»“æœâ€ã€‚æ¯”å¦‚ï¼š

```plain
y = ReLU(Wx + b)  â† è¿™é‡Œçš„ y å°±æ˜¯æ¿€æ´»å€¼
```

+ **æ¿€æ´»å€¼ä¸æ˜¯å›ºå®šçš„ï¼Œå®ƒå–å†³äºè¾“å…¥æ•°æ®ï¼**
+ å®ƒåœ¨ä¸åŒè¾“å…¥ä¸‹åˆ†å¸ƒèŒƒå›´å¯èƒ½ä¸åŒï¼Œå› æ­¤æˆ‘ä»¬æ²¡æ³•é é™æ€åˆ†æç›´æ¥çŸ¥é“å®ƒçš„æœ€å¤§/æœ€å°å€¼ã€‚
+ æ‰€ä»¥å¿…é¡»è¦ç”¨ä¸€äº›çœŸå®æ ·æœ¬ **è·‘ä¸€éå‰å‘ä¼ æ’­**ï¼ŒæŠŠæ¯ä¸€å±‚çš„æ¿€æ´»å€¼æ”¶é›†èµ·æ¥ï¼Œå†å»ç»Ÿè®¡å®ƒä»¬çš„åˆ†å¸ƒèŒƒå›´ã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ çœ‹åˆ° PTQ å·¥å…·ä¼šè¦æ±‚ä½ æä¾› â€œæ ¡å‡†æ•°æ®â€ æ¥è·‘å‡ è½®æ¨ç† â€”â€” å°±æ˜¯ä¸ºäº†**æ‰¾å‡ºæ¿€æ´»å€¼çš„é‡åŒ–åŒºé—´**ã€‚

---

ç”¨æ•°æ®ä¸¾ä¸€ä¸ªé‡åŒ–çš„ä¾‹å­ï¼š

æ˜¯çš„ï¼INT4 / INT8 æœ¬èº«ç¡®å®æ˜¯å­—é¢ä¸Šçš„æ„æ€ â€”â€” æŒ‡çš„æ˜¯ï¼š

**ä½¿ç”¨ 4 ä½æˆ– 8 ä½æ•´æ•°æ¥è¡¨ç¤ºåŸæœ¬çš„æµ®ç‚¹æ•°å€¼ï¼ˆé€šå¸¸æ˜¯ FP16 æˆ– FP32ï¼‰**ã€‚

ä½†**å®ƒèƒŒåçš„æ˜ å°„è¿‡ç¨‹**ã€**èƒ½è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´å’Œç²¾åº¦**ï¼Œæ‰æ˜¯é‡ç‚¹ã€‚ä¸‹é¢æˆ‘ä»¬æ¥ç”¨ä¾‹å­è¯´æ¸…æ¥šï¼š

---

#### âœ… ä¸€å¥è¯å®šä¹‰ï¼š
| ç±»å‹ | ä½æ•° | å¯è¡¨ç¤ºçš„æ•´æ•°èŒƒå›´ |
| --- | --- | --- |
| INT8 | 8 ä½æœ‰ç¬¦å·æ•´æ•° | -128 ~ +127 |
| INT4 | 4 ä½æœ‰ç¬¦å·æ•´æ•° | -8 ~ +7 |


ä½†æ³¨æ„ï¼æˆ‘ä»¬é‡åŒ–çš„å¯¹è±¡ï¼ˆå¦‚æ¿€æ´»æˆ–æƒé‡ï¼‰**ä¸æ˜¯æ•´æ•°ï¼Œè€Œæ˜¯æµ®ç‚¹æ•°**ã€‚æˆ‘ä»¬æ˜¯ç”¨è¿™äº›æ•´æ•°æ¥**è¿‘ä¼¼è¡¨è¾¾**åŸæ¥çš„æµ®ç‚¹æ•°ã€‚

---

#### ğŸ¯ ä¸¾ä¸ªå®Œæ•´çš„é‡åŒ–ä¾‹å­ï¼ˆä»¥ INT4 ä¸ºä¾‹ï¼‰
å‡è®¾ä½ æœ‰ä¸€ç»„æµ®ç‚¹æ•°ï¼š

```python
åŸå§‹æ•°æ® = [-2.5, -1.0, 0.0, 1.0, 2.0]
```

#### ğŸ”¸ æ­¥éª¤ 1ï¼šè®¡ç®—æ¿€æ´»çš„èŒƒå›´
å‡è®¾æˆ‘ä»¬ç»Ÿè®¡å‡ºèŒƒå›´æ˜¯ `min = -2.5`ï¼Œ`max = 2.0`ã€‚

ğŸ”¸ æ­¥éª¤ 2ï¼šæ˜ å°„åˆ° INT4

INT4 å¯è¡¨ç¤ºæ•´æ•°èŒƒå›´ä¸ºï¼š`[-8, 7]`ï¼ˆå…± 16 ä¸ªæ•°å€¼ï¼‰

æˆ‘ä»¬è¦æŠŠ `[-2.5, 2.0]` æ˜ å°„åˆ° `[-8, 7]`ï¼Œè®¡ç®—ï¼š

```python
scale = (max - min) / (qmax - qmin)
      = (2.0 - (-2.5)) / (7 - (-8)) = 4.5 / 15 = 0.3

zero_point = round(-min / scale) = round(2.5 / 0.3) = 8
```

#### ğŸ”¸ æ­¥éª¤ 3ï¼šé‡åŒ–å…¬å¼
```python
q(x) = round(x / scale) + zero_point
     = round(x / 0.3) + 8
```

#### ç»“æœï¼š
| åŸå§‹å€¼ x | x / scale | round() | +zero_point | é‡åŒ–åæ•´æ•°ï¼ˆINT4ï¼‰ |
| --- | --- | --- | --- | --- |
| -2.5 | -8.33 | -8 | 0 | **-8** âœ… æœ€å° |
| -1.0 | -3.33 | -3 | 5 | **-3** |
| 0.0 | 0.0 | 0 | 8 | **0** |
| 1.0 | 3.33 | 3 | 11 | **3** |
| 2.0 | 6.66 | 7 | 15 | **7** âœ… æœ€å¤§ |


è¿™ç»„æµ®ç‚¹æ•°å°±è¢«å‹ç¼©æˆäº†åªéœ€è¦ 4 ä½è¡¨ç¤ºçš„æ•´æ•° âœ…

---

#### ğŸ” åé‡åŒ–ï¼ˆdequantizationï¼‰
æˆ‘ä»¬å¯ä»¥ç”¨ `åé‡åŒ–å…¬å¼` æŠŠæ•´æ•°æ¢å¤æˆè¿‘ä¼¼çš„æµ®ç‚¹æ•°ï¼š

```python
x = scale * (q - zero_point)
```

æ¯”å¦‚ `q = -3`ï¼š

```python
x â‰ˆ 0.3 * (-3 - 8) = 0.3 * (-11) = -3.3
```

è™½ç„¶è·ŸåŸå§‹å€¼ `-1.0` ä¸ä¸€æ ·ï¼Œä½†å·®åˆ«å¯ä»¥æ¥å— â€”â€” **è¿™å°±æ˜¯â€œç²¾åº¦-å­˜å‚¨â€çš„æƒè¡¡**ã€‚

---

#### ğŸ“Š INT4 vs INT8 çš„åŒºåˆ«
| é¡¹ç›® | INT8 | INT4 |
| --- | --- | --- |
| å­˜å‚¨å¤§å° | 1 å­—èŠ‚ | åŠå­—èŠ‚ï¼ˆ2 ä¸ªå‚æ•°å  1 å­—èŠ‚ï¼‰ |
| è¡¨ç¤ºèŒƒå›´ | -128 ~ 127 | -8 ~ 7 |
| ç²¾åº¦ | é«˜ | è¾ƒä½ |
| æ¨ç†é€Ÿåº¦ | å¿« | æ›´å¿« |
| é€‚ç”¨åœºæ™¯ | éƒ¨ç½²ä¸»æµè®¾å¤‡ | æç«¯å‹ç¼©ï¼ˆè¾¹ç¼˜/å¾®ç«¯/NPUï¼‰ |


---

#### ğŸ§  æ€»ç»“ä¸€å¥è¯ï¼š
**INT4/INT8 æ˜¯å¯¹æµ®ç‚¹æ•°çš„â€œæœ‰æŸè¿‘ä¼¼è¡¨è¾¾â€ï¼Œä»¥æ›´ä½çš„ä½å®½æ¢å–æ›´é«˜çš„æ¨ç†æ•ˆç‡å’Œæ›´å°çš„å†…å­˜å ç”¨ã€‚**

è¿™ä¸ªå‹ç¼©è¿‡ç¨‹é  scale å’Œ zero_point æ¥å®ç°çº¿æ€§æ˜ å°„ï¼Œæ˜¯é‡åŒ–çš„æ ¸å¿ƒåŸç†ä¹‹ä¸€ã€‚

---

### --ç”Ÿæˆé‡åŒ–æ ¡å‡†æ•°æ®é›†ï¼ˆæ•°æ®é›†æ ¼å¼è¦æ±‚ä¸é«˜ï¼Œæœ€ç»ˆéœ€è¦çš„æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå¥å­çš„åˆ—è¡¨ï¼‰
æ•°æ®é›†å¯ä»¥ä»huggingfaceæˆ–è€…modelscopeä¸‹è½½ï¼Œä½†æ˜¯huggingfaceå¯èƒ½å‡ºç°ç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥å…ˆåœ¨huggä¸Šæ‰¾åˆ°æƒ³ä¸‹è½½çš„æ•°æ®é›†ï¼Œå†å»modelscopeä¸‹è½½ã€‚

ä¸¾ä¾‹ï¼š

[https://modelscope.cn/datasets/modelscope/wikitext/quickstart](https://modelscope.cn/datasets/modelscope/wikitext/quickstart)

```python
from modelscope.msdatasets import MsDataset
ds =  MsDataset.load('mixedbread-ai/wikipedia-data-en-2023-11')
print("åŠ è½½å®Œæˆ")
```

ä¸‹è½½å¥½çš„æ•°æ®é›†ä¼šä¿å­˜åœ¨~./cache/modelscope/hub/datasetsç›®å½•ä¸‹

ç„¶åä»æ•°æ®é›†ä¸­é€‰å–200æ¡çº¯æ–‡æœ¬å‡ºæ¥ä½œä¸ºæ ¡å‡†æ•°æ®é›†

```python
# ä»wikiæ•°æ®é›†ä¸­æŠ½å–200æ¡æ•°æ®çš„textä½œä¸ºé‡åŒ–æ ¡å‡†æ•°æ®é›†
dataset = MsDataset.load('mixedbread-ai/wikipedia-data-en-2023-11')  # å¦‚æœæœ‰å­é›†ï¼Œå¯é€šè¿‡é¢å¤–å‚æ•°æŒ‡å®š
print(dataset['train'][0])
calibration_texts = dataset['train'].shuffle(seed=42).select(range(200))['text']
print("æ ¡å‡†æ•°æ®æ ·æœ¬æ•°ï¼š", len(calibration_texts))
```

calibration_textså¯ä»¥ç›´æ¥ä½œä¸ºæ ¡å‡†æ•°æ®é›†ä½¿ç”¨

### --AWQé‡åŒ–ï¼š
å¤ªå¥½äº†ï¼ŒAWQï¼ˆActivation-aware Weight Quantizationï¼‰æ˜¯ç°åœ¨éå¸¸ğŸ”¥ä¸»æµçš„ä¸€ç§é«˜ç²¾åº¦ã€ä½èµ„æºå¼€é”€çš„**åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æ–¹æ¡ˆ**ï¼Œç‰¹åˆ«é€‚åˆ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰åœºæ™¯ï¼Œæ¯”å¦‚ LLaMAã€Qwenã€ChatGLM ç­‰ã€‚ä¸‹é¢æˆ‘æ¥ç”¨é€šä¿—ã€å®æˆ˜çš„è§’åº¦å¸¦ä½ è¿‡ä¸€éï¼š

---

#### ğŸš€ AWQ æ˜¯ä»€ä¹ˆï¼Ÿ
**AWQ = Activation-aware Weight Quantization**  
æ˜¯ä¸€ç§åªé‡åŒ–æƒé‡ï¼ˆè€Œä¸æ˜¯æ¿€æ´»ï¼‰çš„è½»é‡çº§é‡åŒ–æ–¹æ³•ï¼Œ  
**ä½†å®ƒåœ¨é‡åŒ–æƒé‡æ—¶ï¼Œè€ƒè™‘äº†æ¿€æ´»åˆ†å¸ƒå¯¹è¯¯å·®çš„å½±å“ï¼Œä»è€Œæ›´ç²¾ç¡®åœ°é€‰æ‹©é‡åŒ–æ–¹æ¡ˆ**ã€‚

å®ƒå±äºåè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ï¼Œä¸éœ€è¦å†è®­ç»ƒï¼Œä¸éœ€è¦ Labelï¼Œåªéœ€è¦æ¨¡å‹å’Œä¸€ç‚¹æ ¡å‡†æ•°æ®ã€‚

---

#### ğŸ§  å®ƒä¸»è¦è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
ä¼ ç»Ÿçš„æƒé‡é‡åŒ–ï¼ˆæ¯”å¦‚å¯¹ weight åš min-max æˆ– GPTQï¼‰ï¼š

+ åªè€ƒè™‘äº†**æƒé‡æœ¬èº«çš„åˆ†å¸ƒ**
+ æ²¡è€ƒè™‘æƒé‡å’Œæ¿€æ´»ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œå¯¹æœ€ç»ˆè¾“å‡ºé€ æˆçš„è¯¯å·®

è€Œ **AWQ çš„å…³é”®ç‚¹æ˜¯ï¼š**

åœ¨é‡åŒ– weight ä¹‹å‰ï¼Œ**å…ˆåˆ†ææ¿€æ´»çš„å“åº”ï¼Œæ‰¾åˆ°æƒé‡ä¸­å¯¹è¾“å‡ºå½±å“æœ€å¤§çš„éƒ¨åˆ†ï¼Œå†æ›´ç²¾å‡†åœ°é‡åŒ–è¿™äº›æƒé‡**ã€‚

---

#### ğŸ¯ ä¸¾ä¸ªç›´è§‚ç±»æ¯”ï¼š
å‡è®¾ä½ åœ¨å‹ç¼©ä¸€å¼ å›¾åƒï¼ˆæ¨¡å‹çš„æƒé‡ï¼‰ï¼Œä¼ ç»Ÿæ–¹æ³•æ˜¯æ•´ä½“å‹ç¼©ï¼ˆæ¯”å¦‚ JPEGï¼‰ï¼Œ  
ä½† AWQ æ˜¯å…ˆçœ‹ä½ äººçœ¼æœ€å®¹æ˜“æ³¨æ„çš„åœ°æ–¹ï¼ˆæ¿€æ´»é«˜å“åº”çš„ä½ç½®ï¼‰ï¼Œç„¶åè¿™äº›åœ°æ–¹ç”¨é«˜ç²¾åº¦å‹ç¼©ï¼Œå…¶ä»–åœ°æ–¹å¯ä»¥ç²—ä¸€ç‚¹ã€‚

===> æœ€ç»ˆå¾—åˆ°çš„å‹ç¼©å›¾åƒï¼ˆé‡åŒ–åæ¨¡å‹ï¼‰åœ¨ä½ â€œçœŸæ­£ä½¿ç”¨å®ƒçš„æ—¶å€™â€ï¼ˆæ¨ç†ï¼‰ï¼Œæ›´æ¥è¿‘åŸå›¾æ•ˆæœï¼

---

#### ğŸ” AWQ åšäº†ä»€ä¹ˆäº‹ï¼ˆæŒ‰æ­¥éª¤ï¼‰ï¼Ÿ
1. **è¾“å…¥å‡ æ¡æ ¡å‡†æ ·æœ¬**ï¼ˆä¸€èˆ¬å‡ åæ¡å°±å¤Ÿï¼‰
2. å¯¹æ¯ä¸€å±‚ï¼š
    - è·‘å‰å‘ä¼ æ’­ï¼Œè®°å½•æ¿€æ´»å€¼åˆ†å¸ƒ
    - åˆ†æå“ªäº›é€šé“/ç¥ç»å…ƒçš„æ¿€æ´»å€¼å¯¹è¾“å‡ºå½±å“å¤§
3. æ ¹æ®åˆ†æç»“æœï¼Œå¯¹ weight åš**é€šé“çº§çš„ç¼©æ”¾ + ç²¾è°ƒé‡åŒ–ï¼ˆscaling + clippingï¼‰**
4. é‡åŒ–å®Œä¹‹åï¼Œè¿˜ä¼šåšä¸€é **outlier-aware æƒé‡ä¿®æ­£ï¼ˆchannel-wise rescalingï¼‰**

---

#### âš™ï¸ æŠ€æœ¯ç‰¹æ€§æ€»ç»“
| ç‰¹æ€§ | è¯´æ˜ |
| --- | --- |
| ğŸ“¦ æ˜¯å¦é‡åŒ–æ¿€æ´» | âŒ ä¸é‡åŒ–æ¿€æ´»ï¼Œåªé‡åŒ–æƒé‡ |
| ğŸ§  æ˜¯å¦æ„ŸçŸ¥æ¿€æ´»åˆ†å¸ƒ | âœ… æ„ŸçŸ¥ï¼Œä½œä¸ºæƒé‡é‡åŒ–çš„æŒ‡å¯¼ä¾æ® |
| ğŸ”© æ˜¯å¦è®­ç»ƒ | âŒ æ— éœ€è®­ç»ƒï¼Œåè®­ç»ƒé‡åŒ– |
| ğŸª¶ ç²¾åº¦è¡¨ç° | ğŸŸ¢ æ¥è¿‘ FP16ï¼ˆç”šè‡³ Q4 <1% Dropï¼‰ |
| ğŸš€ æ€§èƒ½å…¼å®¹æ€§ | éå¸¸é€‚åˆ llama.cpp / vllm / ggml ç­‰æ¨ç†å¼•æ“ |
| ğŸ”„ æ”¯æŒ INT æ ¼å¼ | å¤šæ”¯æŒ INT4ï¼ˆgroup-wiseï¼‰/ INT8 |
| ğŸ§° å·¥å…· | [awq](https://github.com/mit-han-lab/llm-awq)<br/>ã€[autoawq](https://github.com/casper-hansen/AutoAWQ) |


---

#### ğŸ“ˆ å®æµ‹è¡¨ç°ï¼ˆä»¥ LLaMA ä¸ºä¾‹ï¼‰
| æ¨¡å‹ | FP16 | AWQ-INT4 |
| --- | --- | --- |
| LLaMA-7B | 58.1 | 57.9 |
| LLaMA-13B | 60.6 | 60.2 |


å‡ ä¹æ— ç²¾åº¦æŸå¤±ï¼Œå‹ç¼© 2-4 å€ï¼Œæ¨ç†åŠ é€Ÿ 1.5~2.5 å€ ğŸ‘

---

#### ğŸ› ï¸ å®æˆ˜é‡åŒ–å·¥å…·æ¨èï¼š
```bash
pip install autoawq
```

ä¸€è¡Œå‘½ä»¤å¿«é€Ÿé‡åŒ–ï¼š

```bash
autoawq quantize \
  --model-path path_to_your_model \
  --quantize-bit 4 \
  --output-path path_to_quantized_model \
  --w-group-size 128 \
  --calib-samples 128
```

---

#### âœ… æ€»ç»“ä¸€å¥è¯ï¼š
AWQ æ˜¯ä¸€ç§**åªé‡åŒ–æƒé‡ä½†è€ƒè™‘æ¿€æ´»åˆ†å¸ƒ**çš„åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œå®ƒåœ¨ä¸ç‰ºç‰²ç²¾åº¦çš„å‰æä¸‹å¤§å¹…æå‡äº†æ¨ç†æ•ˆç‡ï¼Œéå¸¸é€‚åˆ LLM éƒ¨ç½²è½åœ°åœºæ™¯ï¼

---



#### é‚£ä¹ˆAWQæ˜¯æ€ä¹ˆå»æ‰¾åˆ°æ¯”è¾ƒé‡è¦çš„é€šé“å‘¢ï¼Ÿ
AWQ çš„ **å…³é”®åˆ›æ–°ç‚¹** å°±æ˜¯ï¼š

åœ¨é‡åŒ–æƒé‡ä¹‹å‰ï¼Œ**è¯„ä¼°å‡ºå“ªäº› weight å¯¹è¾“å‡ºå½±å“å¤§ï¼Œç„¶åæ›´ç²¾å‡†åœ°é‡åŒ–å®ƒä»¬ã€‚**

ä¸‹é¢æˆ‘ç»™ä½ æ·±å…¥ä½†é€šä¿—åœ°è§£é‡Šå®ƒæ˜¯ _æ€ä¹ˆåšåˆ°è¿™ä»¶äº‹çš„_ ğŸ‘‡

---

#### ğŸ§  èƒŒåçš„æƒ³æ³•ï¼šä¸æ˜¯æ‰€æœ‰çš„ weight éƒ½ä¸€æ ·é‡è¦
åœ¨ Transformer ä¸­ï¼Œå°¤å…¶æ˜¯ QKV / Linear å±‚ä¸­ï¼Œæœ‰äº›é€šé“æˆ–è€… weightï¼š

+ ä¸€ç‚¹ç‚¹çš„å˜åŒ–å°±ä¼šå¯¼è‡´è¾“å‡ºå¾ˆå¤§åå·®ï¼ˆå¯¹è¾“å‡ºâ€œæ•æ„Ÿâ€ï¼‰
+ è€Œæœ‰äº›é€šé“ï¼Œå“ªæ€•ä½ éšä¾¿é‡åŒ–ç‚¹ï¼Œä¹Ÿä¸å¤ªå½±å“æœ€ç»ˆç»“æœ

æ‰€ä»¥ï¼š**AWQ ä¸å¯¹æ‰€æœ‰æƒé‡â€œä¸€åˆ€åˆ‡â€åœ°é‡åŒ–**ï¼Œè€Œæ˜¯ï¼š

**æ‰¾å‡ºå¯¹è¾“å‡ºæ•æ„Ÿçš„é€šé“ï¼Œä¸“é—¨å¯¹å®ƒä»¬åšæ›´ç»†è‡´çš„ scale è°ƒæ•´ï¼Œä»è€Œå‡å°‘é‡åŒ–è¯¯å·®**ã€‚

---

#### ğŸ§ª æŠ€æœ¯ç»†èŠ‚ï¼šå¦‚ä½•åˆ¤æ–­â€œå½±å“å¤§â€ï¼Ÿ
AWQ çš„è®ºæ–‡å’Œå®ç°ä¸­ï¼Œä¸»è¦ç”¨äº†ä¸€ä¸ªç»å…¸ä¸”ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼š

#### âœ… åˆ©ç”¨**æ¿€æ´»å€¼çš„èŒƒæ•°ï¼ˆactivation normï¼‰**æ¥åˆ¤æ–­é€šé“é‡è¦æ€§
ä»¥ Linear å±‚ä¸ºä¾‹ï¼ˆy = Wxï¼‰ï¼š

+ æ¯ä¸ªé€šé“çš„è¾“å‡ºå…¶å®æ˜¯ `dot(wáµ¢, x)`ï¼Œå…¶ä¸­ï¼š
    - `wáµ¢` æ˜¯å½“å‰é€šé“çš„æƒé‡å‘é‡
    - `x` æ˜¯è¾“å…¥æ¿€æ´»å‘é‡ï¼ˆæ¯”å¦‚ä¸€ä¸ª token çš„ embeddingï¼‰
+ å¦‚æœ `x` å¯¹è¿™ä¸ªé€šé“æ¥è¯´å–å€¼ç‰¹åˆ«å¤§ï¼ˆæ¯”å¦‚ norm å¾ˆé«˜ï¼‰ï¼Œé‚£è¿™ä¸ªé€šé“çš„è¾“å‡ºå°±ä¼šæ›´å¤§ï¼Œè¯¯å·®å°±æ›´å®¹æ˜“æ”¾å¤§

æ‰€ä»¥ï¼š

**æ¿€æ´»è¶Šå¤§çš„é€šé“ï¼Œæ„å‘³ç€è¿™ä¸ªé€šé“åœ¨æ¨ç†æ—¶è¶Šâ€œæ´»è·ƒâ€æˆ–é‡è¦ï¼Œæƒé‡é‡åŒ–æ—¶å°±å¾—å°å¿ƒå¤„ç†**

---

#### ğŸ› ï¸ å®é™…æ€ä¹ˆå¤„ç†çš„ï¼Ÿ
AWQ åœ¨é‡åŒ–æ—¶åšäº†ä¸¤ä¸ªåŠ¨ä½œï¼š

#### 1ï¸âƒ£ å¯¹æ¯ä¸ªé€šé“çš„æ¿€æ´»å€¼åšç»Ÿè®¡ï¼ˆå‰å‘ä¼ æ’­ N æ¡æ ·æœ¬ï¼‰
+ ä¸€èˆ¬åªè·‘å‡ åæ¡æ ¡å‡†æ ·æœ¬ï¼ˆ128 æ¡å¤Ÿç”¨äº†ï¼‰
+ è®°å½•æ¯ä¸ªé€šé“çš„æ¿€æ´»å€¼ï¼ˆæ¯”å¦‚ mean / max / normï¼‰

#### 2ï¸âƒ£ æ ¹æ®æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯ï¼Œåˆ†ç»„æˆ–ç¼©æ”¾æƒé‡
+ æŠŠé€šé“æŒ‰é‡è¦æ€§æ’åº
+ æ¯ç»„é€šé“ç”¨è‡ªå·±çš„ `scale` å’Œé‡åŒ–èŒƒå›´ï¼ˆgroup-wiseï¼‰
+ æœ‰çš„å®ç°è¿˜ä¼š clip æ‰æ¿€æ´»å¤ªå¤§çš„ outlier é€šé“

---

#### ğŸ“Œ ä¸¾ä¸ªå®é™…ä¾‹å­
å‡è®¾æŸä¸€å±‚æœ‰ 8 ä¸ªé€šé“ï¼Œæ ¡å‡†æ•°æ®å‰å‘ä¼ æ’­åï¼Œä½ è®°å½•åˆ°ï¼š

| é€šé“ | æ¿€æ´»èŒƒæ•°ï¼ˆnormï¼‰ |
| --- | --- |
| 0 | 0.2 |
| 1 | 3.1 |
| 2 | 0.9 |
| 3 | 0.1 |
| 4 | 5.3 â† é«˜ |
| 5 | 0.5 |
| 6 | 2.7 |
| 7 | 0.3 |


ä½ å°±ä¼šè®¤ä¸ºï¼š**é€šé“ 4ã€1ã€6 æ›´é‡è¦**ï¼Œ  
å®ƒä»¬ç”¨æ›´ç»†è‡´çš„é‡åŒ–ç­–ç•¥ï¼Œæˆ–è€…ä¼˜å…ˆåˆ†é…æ›´å°çš„ `scale`ï¼Œä»¥å‡å°‘è¯¯å·®ã€‚

---

#### ğŸ§  è¡¥å……ï¼šå®ƒå¹¶æ²¡æœ‰çœŸçš„é‡åŒ–æ¿€æ´»
è™½ç„¶å« Activation-awareï¼Œä½†æ¿€æ´»åªæ˜¯â€œè¢«æ„ŸçŸ¥â€ï¼Œä¸æ˜¯â€œè¢«é‡åŒ–â€ã€‚

æ‰€ä»¥ AWQ çš„æ ¸å¿ƒæ˜¯ï¼š

+ **åˆ†ææ¿€æ´» â†’ å¼•å¯¼æƒé‡é‡åŒ–ç­–ç•¥**
+ **ä¸æ”¹å˜æ¨ç†ç»“æ„ï¼Œä¹Ÿä¸å¢åŠ è¿è¡Œæ—¶å¼€é”€**

---

#### âœ… æ€»ç»“ä¸€å¥è¯ï¼š
AWQ æ˜¯é€šè¿‡ **ç»Ÿè®¡æ¯ä¸ªé€šé“çš„æ¿€æ´»èŒƒæ•°ï¼ˆnormï¼‰æ¥è¯„ä¼°å®ƒå¯¹è¾“å‡ºçš„å½±å“å¤§å°**ï¼Œè¿›è€Œåœ¨é‡åŒ–æ—¶å¯¹é‡è¦é€šé“è¿›è¡Œæ›´ç²¾ç»†çš„å¤„ç†ï¼Œä»è€Œæ˜¾è‘—å‡å°‘ç²¾åº¦æŸå¤±ã€‚

---

å¦‚æœä½ æœ‰å…´è¶£æ·±å…¥ä»£ç å±‚çœ‹çœ‹ï¼Œæˆ‘å¯ä»¥å¸¦ä½ è¯»ä¸€ä¸‹ AutoAWQ æˆ– MIT-AWQ ä¸­è¿™éƒ¨åˆ†çš„å®ç°é€»è¾‘ï¼Œæˆ–è€…å¸®ä½ åšä¸ªé€šé“æ¿€æ´»åˆ†æçš„ demo ğŸš€ è¦ä¸è¦ä¸€èµ·ç©ç©ï¼Ÿ



### ---GPTQé‡åŒ–
#### æ¦‚å¿µ
ä¸åŒäºä¸Šé¢çš„AWQæ–¹å¼é‡åŒ–ï¼ŒGPTQé€šè¿‡è¯¯å·®è¡¥å¿å’Œåˆ†åˆ—é‡åŒ–çš„æ–¹å¼è¿›è¡Œé‡åŒ–

å®ƒä¹Ÿä¼šéœ€è¦ä¸€äº›æ ¡å‡†æ•°æ®ï¼Œè¿™äº›æ ¡å‡†æ•°æ®ä¸»è¦ç”¨äºè§‚å¯Ÿé‡åŒ–è¯¯å·®ï¼ŒGPTQé‡‡ç”¨çš„é€åˆ—é‡åŒ–çš„æ–¹å¼ï¼Œåœ¨é‡åŒ–æ¯ä¸€åˆ—æ—¶ï¼Œä¼šå»è€ƒè™‘å› ä¸ºå‰é¢åˆ—å¯¼è‡´çš„è¯¯å·®ã€‚

GPTQçš„é‡åŒ–è¿‡ç¨‹æ˜¯ï¼Œé’ˆå¯¹æ¯ä¸€åˆ—ï¼Œå…ˆæ‰¾åˆ°ä¸€ä¸ªåŸºå‡†çš„é‡åŒ–scaleï¼Œç„¶åé€šè¿‡ç¼©æ”¾å¾—åˆ°ä¸€æ‰¹scaleï¼Œç„¶åå¼€å§‹ä¸æ–­å°è¯•è¿™äº›scaleä¸‹å’ŒåŸå§‹æƒé‡çš„è¯¯å·®ï¼Œæ‰¾åˆ°è¿™æ‰¹sclaeä¸­è¯¯å·®æœ€å°çš„å€¼ä½œä¸ºæœ€ç»ˆscaleï¼Œä½†æ˜¯ä¸å¯é¿å…ä¾ç„¶å­˜åœ¨è¯¯å·®ï¼Œå› æ­¤GPTQè¿˜æœ‰è¯¯å·®è¡¥å¿çš„æ–¹å¼æ¥å¼¥è¡¥å‰é¢åˆ—é‡åŒ–è¿‡ç¨‹ä¸­äº§ç”Ÿçš„è¯¯å·®ã€‚ç”¨æ•´ä½“æƒé‡å¾—åˆ°çš„Y=WXï¼Œå‡å»å‰é¢æ¯ä¸€åˆ—ä½¿ç”¨é‡åŒ–æƒé‡å¾—åˆ°çš„æ¿€æ´»å€¼ï¼Œå°±æ˜¯æˆ‘ä»¬æœ€ç»ˆå¸Œæœ›çš„ç›®æ ‡æ¿€æ´»å€¼ã€‚æœ‰äº†ç›®æ ‡å€¼åï¼Œå†é€šè¿‡ä¸Šè¿°çš„å¤šscaleå°è¯•çš„æ–¹æ³•æ‰¾åˆ°è¿™ä¸€åˆ—æœ€åˆé€‚çš„scaleå€¼ã€‚

#### ä½¿ç”¨GPTQé‡åŒ–Qwenæ¨¡å‹ï¼ˆå‚è€ƒçš„å®˜æ–¹ç¤ºä¾‹ï¼‰ï¼š
```plain
git clone https://github.com/AutoGPTQ/AutoGPTQ
cd AutoGPTQ
pip install -vvv --no-build-isolation -e .
```

å¼€å§‹é‡åŒ–ï¼š

```python
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
from transformers import AutoTokenizer
from modelscope.msdatasets import MsDataset
# Specify paths and hyperparameters for quantization
model_path = "/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct"
quant_path = "./models/Qwen2.5-0.5B-Instruct-GPTQ"
quantize_config = BaseQuantizeConfig(
    bits=8, # 4 or 8
    group_size=128,
    damp_percent=0.01,
    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad
    static_groups=False,
    sym=True,
    true_sequential=True,
    model_name_or_path=None,
    model_file_base_name="model"
)
max_len = 8192

# Load your tokenizer and model with AutoGPTQ
# To learn about loading model to multiple GPUs,
# visit https://github.com/AutoGPTQ/AutoGPTQ/blob/main/docs/tutorial/02-Advanced-Model-Loading-and-Best-Practice.md
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)
```

å‡†å¤‡æ ¡å‡†æ•°æ®ï¼š

```python
# ä»wikiæ•°æ®é›†ä¸­æŠ½å–200æ¡æ•°æ®çš„textä½œä¸ºé‡åŒ–æ ¡å‡†æ•°æ®é›†
dataset = MsDataset.load('mixedbread-ai/wikipedia-data-en-2023-11')  # å¦‚æœæœ‰å­é›†ï¼Œå¯é€šè¿‡é¢å¤–å‚æ•°æŒ‡å®š
calibration_texts = dataset['train'].shuffle(seed=42).select(range(200))['text']
print("æ ¡å‡†æ•°æ®æ ·æœ¬æ•°ï¼š", len(calibration_texts))
```

```python
#AUTO-GPTQé‡åŒ–æ—¶ä½¿ç”¨çš„æ ¡å‡†æ•°æ®ï¼Œéœ€è¦ä½¿ç”¨tokenizerå…ˆè½¬æ¢ä¸ºinput_idsçš„å½¢å¼ï¼Œä¸èƒ½ä½¿ç”¨å­—ç¬¦ä¸²
calib_data = [tokenizer(text) for text in calibration_texts]
```

æ‰§è¡Œé‡åŒ–è¿‡ç¨‹ï¼š

```python
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)
model.quantize(calib_data, cache_examples_on_gpu=False)
```

ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨ï¼š

```python
model.save_quantized(quant_path, use_safetensors=True)
tokenizer.save_pretrained(quant_path)
```

ä½¿ç”¨vllmå¯åŠ¨ä¸€ä¸ªæ¨¡å‹æœåŠ¡ï¼š

```python
!CUDA_VISIBLE_DEVICES=7 vllm serve ./models/Qwen2.5-0.5B-Instruct-GPTQ \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --dtype half \
  --gpu-memory-utilization 0.4 \
  --max-num-seqs 1000
```

**æ³¨æ„ï¼š**ä½¿ç”¨GPTQé‡åŒ–çš„Int4çš„åƒé—®æ¨¡å‹ï¼Œæ— æ³•ä½¿ç”¨vllmå¯åŠ¨ï¼Œvllmç›®å‰ä»…æ”¯æŒFP8çš„é‡åŒ–æ¨¡å‹

### --bpw
åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œ**bpw**ï¼ˆæ¯æƒé‡ä½æ•°ï¼Œbits per weightï¼‰æŒ‡çš„æ˜¯è¡¨ç¤ºæ¯ä¸ªæ¨¡å‹æƒé‡æ‰€ä½¿ç”¨çš„å¹³å‡ä½æ•°ã€‚ä¾‹å¦‚ï¼Œä¼ ç»Ÿçš„å…¨ç²¾åº¦ï¼ˆFP32ï¼‰æ¨¡å‹æ¯ä¸ªæƒé‡å ç”¨32ä½ï¼Œè€Œé€šè¿‡é‡åŒ–æŠ€æœ¯ï¼Œå¯ä»¥å°†æ¯ä¸ªæƒé‡çš„ä½æ•°å‡å°‘åˆ°æ›´ä½çš„å€¼ï¼Œå¦‚8ä½ã€4ä½ï¼Œç”šè‡³æ›´ä½ã€‚è¿™æœ‰åŠ©äºé™ä½æ¨¡å‹çš„å†…å­˜å ç”¨å’Œè®¡ç®—éœ€æ±‚ï¼Œä»è€ŒåŠ å¿«æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å­˜å‚¨ç©ºé—´ã€‚[Medium](https://netraneupane.medium.com/hands-on-llms-quantization-a4c7ab1421c2?utm_source=chatgpt.com)

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®é™…çš„bpwå€¼å¯èƒ½å¹¶éæ•´æ•°ï¼Œå› ä¸ºé‡åŒ–æ–¹æ³•å¯èƒ½ä½¿ç”¨æ··åˆç²¾åº¦ç­–ç•¥ï¼Œå¯¹æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†é‡‡ç”¨ä¸åŒçš„ä½å®½è¿›è¡Œé‡åŒ–ã€‚ä¾‹å¦‚ï¼Œä¸€ç§é‡åŒ–æ–¹æ³•å¯èƒ½å¯¹æŸäº›å±‚ä½¿ç”¨4ä½ï¼Œå¯¹å…¶ä»–å±‚ä½¿ç”¨5ä½ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªå¹³å‡çš„bpwå€¼ã€‚è¿™ç§æ··åˆç²¾åº¦çš„é‡åŒ–ç­–ç•¥å¯ä»¥åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œè¿›ä¸€æ­¥å‡å°‘æ¨¡å‹å¤§å°ã€‚

åœ¨é€‰æ‹©é‡åŒ–æ–¹æ¡ˆæ—¶ï¼Œç†è§£bpwæœ‰åŠ©äºæƒè¡¡æ¨¡å‹ç²¾åº¦ä¸èµ„æºæ¶ˆè€—ä¹‹é—´çš„å…³ç³»ã€‚è¾ƒä½çš„bpwé€šå¸¸æ„å‘³ç€æ›´å°çš„æ¨¡å‹å°ºå¯¸å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´ä¸€å®šçš„ç²¾åº¦ä¸‹é™ã€‚å› æ­¤ï¼Œéœ€è¦æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯å’Œç¡¬ä»¶é™åˆ¶ï¼Œé€‰æ‹©åˆé€‚çš„bpwå€¼å’Œé‡åŒ–ç­–ç•¥ã€‚



### --llama-cpp
#### --æ— æ ¡å‡†é‡åŒ–
##### 1.é¦–å…ˆå°†æ¨¡å‹ç”Ÿæˆggufæ–‡ä»¶ï¼ˆå»ºè®®å…ˆå‡åˆ°f32ï¼‰
```python
!python convert_hf_to_gguf.py /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct --outtype f32 --outfile ./models/qwen2.5-0.5b-instruct-f32.gguf
```

##### 2.æ‰§è¡Œllama-quantizeè„šæœ¬ï¼Œç›´æ¥é‡åŒ–ï¼š
```python
./build/bin/llama-quantize ./models/qwen2.5-0.5b-instruct-f32.gguf ./models/qwen2.5-0.5b-instruct-q8_0.gguf F16
```

psï¼šå¦‚æœæ˜¯åœ¨jupyterä¸­æ‰§è¡Œï¼Œä¼šæŠ¥é”™ï¼Œéœ€è¦å†™æˆpythonä¸­å¯æ‰§è¡Œçš„å½¢å¼ï¼š

```python
#æ— æ ¡å‡†é‡åŒ–ï¼ˆä¸ä½¿ç”¨æ ¡å‡†æ•°æ®é›†ï¼‰
import subprocess

command = [
    "./build/bin/llama-quantize",
    "./models/qwen2.5-0.5b-instruct-f32.gguf",
    "./models/qwen2.5-0.5b-instruct-f16.gguf",
    "F16"
]
subprocess.run(command)
```

#### --åŸºäºAWQæ–¹å¼é‡åŒ–
##### 1.é¦–å…ˆå®‰è£…autoawqåŒ…
```python
!pip install autoawq
```

psï¼šå¯èƒ½éœ€è¦è°ƒæ•´transformersåŒ…ç‰ˆæœ¬ï¼Œåšå¥½å…¼å®¹æ€§

##### 2.åŠ è½½æ¨¡å‹ï¼Œç¡®å®šé‡åŒ–å‚æ•°
```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer
from modelscope.msdatasets import MsDataset
 
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct"
quant_path = "./models/qwen2.5-0.5b-instruct-awq"

quant_config = {"zero_point" : True,"q_group_size" : 128,"w_bit" : 4, "version" : "GEMM"}

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoAWQForCausalLM.from_pretrained(model_path,device_map = "auto")
```

##### 3.é‡æ–°è·å–æ ¡å‡†æ•°æ®é›†
```python
# ä»wikiæ•°æ®é›†ä¸­æŠ½å–200æ¡æ•°æ®çš„textä½œä¸ºé‡åŒ–æ ¡å‡†æ•°æ®é›†
dataset = MsDataset.load('mixedbread-ai/wikipedia-data-en-2023-11')  # å¦‚æœæœ‰å­é›†ï¼Œå¯é€šè¿‡é¢å¤–å‚æ•°æŒ‡å®š
calibration_texts = dataset['train'].shuffle(seed=42).select(range(200))['text']
print("æ ¡å‡†æ•°æ®æ ·æœ¬æ•°ï¼š", len(calibration_texts))
```

```python
calib_data = [text for text in calibration_texts]
```

##### 4.æ‰§è¡Œé‡åŒ–å‡†å¤‡æ“ä½œ
```python
model.quantize(
    tokenizer,
    quant_config=quant_config,
    calib_data=calib_data,
    text_column='text',  # æŒ‡å®šæ–‡æœ¬å­—æ®µçš„é”®å
    export_compatible=True  #è¿™é‡ŒæŒ‡å®šä¸ºTrueä¼šè®©æ¨¡å‹åœ¨ç”Ÿæˆé‡åŒ–æ¨¡å‹æ—¶æ›´å…·å…¼å®¹æ€§ï¼Œå¯ä»¥ç›´æ¥è·‘èµ·æ¥
)
```

æ³¨æ„ï¼šè¿™é‡Œçš„é‡åŒ–å‡½æ•°æ‰§è¡Œå®Œåï¼Œæ¨¡å‹å¹¶æœªè¢«é‡åŒ–ï¼Œåªæ˜¯ä¸ºé‡åŒ–åšå¥½äº†å‡†å¤‡ï¼Œå‡†ç¡®çš„è¯´æ˜¯ç¡®å®šäº†é‡åŒ–æ—¶è¦é‡‡ç”¨çš„scaleã€‚è¯¦ç»†è§£é‡Šï¼š

ä»£ç ç‰‡æ®µä¸­ï¼Œè°ƒç”¨ `model.quantize()` æ–¹æ³•æ—¶ï¼Œæ·»åŠ å‚æ•° `export_compatible=True`ï¼Œå…¶ä½œç”¨å¹¶éç›´æ¥å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–ï¼Œè€Œæ˜¯å¯¹æƒé‡è¿›è¡Œè°ƒæ•´ï¼Œä½¿å…¶æ›´é€‚åˆåç»­çš„é‡åŒ–è¿‡ç¨‹ã€‚è¿™ç§è°ƒæ•´åŸºäºæ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–ï¼ˆActivation-aware Weight Quantization, AWQï¼‰æ–¹æ³•ï¼Œä¸»è¦æ­¥éª¤åŒ…æ‹¬ï¼šîˆ†

1. **æƒé‡è°ƒæ•´**ï¼šé€šè¿‡åˆ†ææ¨¡å‹åœ¨æ ¡å‡†æ•°æ®é›†ä¸Šçš„æ¿€æ´»åˆ†å¸ƒï¼Œç¡®å®šæ¯ä¸ªæƒé‡é€šé“çš„é‡è¦æ€§ã€‚ç„¶åï¼Œå¯¹é‡è¦çš„æƒé‡é€šé“è¿›è¡Œç¼©æ”¾ï¼Œä»¥å‡å°‘é‡åŒ–è¯¯å·®ã€‚ îˆ€citeîˆ‚turn0search8îˆîˆ†
2. **å»¶è¿Ÿé‡åŒ–**ï¼šåœ¨ä¸Šè¿°è°ƒæ•´å®Œæˆåï¼Œæ¨¡å‹çš„æƒé‡å°šæœªè¢«å®é™…é‡åŒ–ã€‚çœŸæ­£çš„é‡åŒ–æ­¥éª¤é€šå¸¸åœ¨åç»­ä½¿ç”¨å…¶ä»–å·¥å…·ï¼ˆå¦‚ `convert-hf-to-gguf.py` å’Œ `llama-quantize`ï¼‰æ—¶è¿›è¡Œã€‚ îˆ€citeîˆ‚turn0search9îˆîˆ†

å› æ­¤ï¼Œä»£ç ä¸­çš„ `model.quantize()` æ–¹æ³•åœ¨ `export_compatible=True` å‚æ•°ä¸‹ï¼Œä¸»è¦æ˜¯é¢„å¤„ç†æ¨¡å‹æƒé‡ï¼Œä½¿å…¶åœ¨åç»­çš„é‡åŒ–è¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™åŸå§‹æ¨¡å‹çš„æ€§èƒ½å’Œç²¾åº¦ã€‚

**æ•°æ®ä¸¾ä¾‹ï¼š**

ä¸ºäº†æ›´æ¸…æ¥šåœ°ç†è§£æ¨¡å‹é‡åŒ–è¿‡ç¨‹ä¸­çš„é¢„å¤„ç†ï¼ˆå¦‚AWQï¼‰ä¸å®é™…é‡åŒ–ä¹‹é—´çš„åŒºåˆ«ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªå…·ä½“çš„ç¤ºä¾‹ï¼šîˆ†

**1. é¢„å¤„ç†é˜¶æ®µï¼ˆAWQè°ƒæ•´ï¼‰ï¼š**

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¨¡å‹å±‚çš„æƒé‡çŸ©é˜µï¼š

```plain
æƒé‡çŸ©é˜µ Wï¼š
[ 0.8,  2.5, -1.2]
[-0.5,  1.0,  3.3]
```

åœ¨ä¼ ç»Ÿçš„é‡åŒ–æ–¹æ³•ä¸­ï¼Œç›´æ¥å°†è¿™äº›æƒé‡ä»æµ®ç‚¹æ•°è½¬æ¢ä¸ºä½æ¯”ç‰¹æ•´æ•°å¯èƒ½å¯¼è‡´è¾ƒå¤§çš„é‡åŒ–è¯¯å·®ã€‚AWQæ–¹æ³•é€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿›è¡Œé¢„å¤„ç†ï¼š

+ åˆ†ææ¿€æ´»åˆ†å¸ƒï¼šåœ¨æ ¡å‡†æ•°æ®é›†ä¸Šè¿è¡Œæ¨¡å‹ï¼Œæ”¶é›†æ¯ä¸ªé€šé“çš„æ¿€æ´»å€¼åˆ†å¸ƒï¼Œç¡®å®šå“ªäº›é€šé“å¯¹æ¨¡å‹æ€§èƒ½å½±å“æ›´å¤§ã€‚
+ è°ƒæ•´æƒé‡ï¼ˆç¼©æ”¾ï¼‰ï¼šå¯¹äºé‡è¦çš„é€šé“ï¼Œåº”ç”¨ç¼©æ”¾å› å­ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç¬¬ä¸€åˆ—æƒé‡å¯¹åº”çš„æ¿€æ´»å€¼è¾ƒå¤§ï¼Œå¯èƒ½å¯¹è¯¥åˆ—æƒé‡åº”ç”¨ä¸€ä¸ªç¼©æ”¾å› å­ `s = 0.5`ï¼š

```plain
è°ƒæ•´åçš„æƒé‡çŸ©é˜µ W'ï¼š
  [ 0.4,  2.5, -1.2]   ï¼ˆ0.8 * 0.5 = 0.4ï¼‰
  [-0.25, 1.0,  3.3]   ï¼ˆ-0.5 * 0.5 = -0.25ï¼‰
```

è¿™ç§è°ƒæ•´ä½¿å¾—é‡è¦é€šé“çš„æƒé‡å€¼å‡å°ï¼Œä»è€Œåœ¨åç»­é‡åŒ–æ—¶å‡å°‘é‡åŒ–è¯¯å·®ã€‚

**2. é‡åŒ–é˜¶æ®µï¼š**

åœ¨é¢„å¤„ç†å®Œæˆåï¼Œè¿›è¡Œå®é™…çš„é‡åŒ–æ“ä½œï¼Œä¾‹å¦‚å°†è°ƒæ•´åçš„æƒé‡ä»æµ®ç‚¹æ•°è½¬æ¢ä¸º4ä½æ•´æ•°ï¼š

+ ç¡®å®šé‡åŒ–èŒƒå›´ï¼šæ‰¾åˆ°è°ƒæ•´åæƒé‡çŸ©é˜µä¸­çš„æœ€å¤§ç»å¯¹å€¼ï¼Œä¾‹å¦‚ `max_abs = 3.3`ã€‚
+ è®¡ç®—ç¼©æ”¾å› å­ï¼šå¯¹äº4ä½é‡åŒ–ï¼Œæ•´æ•°èŒƒå›´æ˜¯ [-8, 7]ï¼Œå› æ­¤ç¼©æ”¾å› å­ `scale = max_abs / 7 â‰ˆ 0.471`ã€‚
+ åº”ç”¨é‡åŒ–ï¼šå°†è°ƒæ•´åçš„æƒé‡çŸ©é˜µé‡åŒ–ä¸ºæ•´æ•°ï¼š

```plain
é‡åŒ–åçš„æƒé‡çŸ©é˜µ W_qï¼š
  [ 1,  5, -3]   ï¼ˆround(0.4 / 0.471) = 1ï¼‰
  [-1,  2,  7]   ï¼ˆround(-0.25 / 0.471) = -1ï¼‰
```

é€šè¿‡ä¸Šè¿°æ­¥éª¤ï¼Œé¢„å¤„ç†é˜¶æ®µçš„è°ƒæ•´ï¼ˆå¦‚AWQï¼‰ä½¿å¾—æƒé‡åœ¨é‡åŒ–æ—¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¡¨ç¤ºï¼Œä»è€Œå‡å°‘é‡åŒ–è¯¯å·®ï¼Œä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

**æ€»ç»“ï¼šé¢„å¤„ç†é˜¶æ®µï¼ˆå¦‚AWQï¼‰é€šè¿‡åˆ†ææ¿€æ´»åˆ†å¸ƒï¼Œå¯¹æƒé‡è¿›è¡Œé€‚å½“çš„ç¼©æ”¾è°ƒæ•´ï¼Œä½¿å…¶æ›´é€‚åˆé‡åŒ–ï¼›è€Œé‡åŒ–é˜¶æ®µåˆ™å°†è¿™äº›è°ƒæ•´åçš„æƒé‡è½¬æ¢ä¸ºä½æ¯”ç‰¹è¡¨ç¤ºã€‚ä¸¤è€…ç»“åˆï¼Œæœ‰åŠ©äºåœ¨é™ä½æ¨¡å‹å¤æ‚åº¦çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿æŒå…¶æ€§èƒ½ã€‚**

****

**P.S:è¿™é‡Œåœ¨ç”Ÿæˆ./models/qwen2.5-0.5b-instruct-awqæ–‡ä»¶åï¼Œçœ‹äº†ä¸€ä¸‹å…¶å†…éƒ¨æ–‡ä»¶ï¼Œå‘ç°ç»“æœå’Œqwençš„instructå·®ä¸å¤šï¼Œæ¨¡å‹çš„æ ¸å¿ƒæ–‡ä»¶éƒ½æœ‰ã€‚ç„¶åå°è¯•å»å°†è¿™ä¸ªæ¨¡å‹åŠ è½½è¿è¡Œèµ·æ¥ï¼Œå¼€å§‹ä½¿ç”¨transformersåº“çš„AutoModelForCausalLM.from_pretrained(ï¼‰æ–¹æ³•åŠ è½½æ¨¡å‹ï¼Œå‘ç°çŒ›çŒ›æŠ¥é”™ï¼Œè°ƒç ”åå‘ç°å¯èƒ½æ˜¯æƒé‡æ–‡ä»¶æ ¼å¼ä¸ç¬¦åˆopenaiæ ¼å¼ï¼Œåé¢æ”¹ç”¨AutoTokenizer.from_pretrainedï¼ˆï¼‰æ–¹æ³•åŠ è½½ï¼Œå‘ç°ä¹Ÿæ˜¯æŠ¥é”™ï¼Œåˆ†æåŸå› ï¼šå¯èƒ½ç¡®å®æƒé‡æ–‡ä»¶çš„ç»“æ„ä¸ç¬¦åˆè¿™ä¸¤ä¸ªæ–¹æ³•çš„æ ¼å¼ã€‚**

##### 5.ä¿å­˜ç¡®å®šå¥½çš„scaleæ¨¡å‹
```python
model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)
print(f'Model is quantized and saved at "{quant_path}"')
```

##### 6.å†æŒ‰ç…§ä¹‹å‰çš„æµç¨‹ä½¿ç”¨convert_hf_to_gguf.pyæ‰§è¡Œå…·ä½“çš„é‡åŒ–è¿‡ç¨‹
```python
!python convert_hf_to_gguf.py ./models/qwen2.5-0.5b-instruct-awq --outtype f32 --outfile ./models/qwen2.5-0.5b-instruct-f32-awq.gguf
```

```python
# AWQé‡åŒ–ï¼ˆå¸¦æ ¡å‡†æ•°æ®ï¼‰
import subprocess
command = [
    "./build/bin/llama-quantize",
    "./models/qwen2.5-0.5b-instruct-f32-awq.gguf",
    "./models/qwen2.5-0.5b-instruct-f16-awq.gguf",
    "F16"
]
subprocess.run(command)
```

#### --æ¨¡å‹å›°æƒ‘åº¦æµ‹è¯•
`<font style="color:rgb(0, 0, 0);">llama.cpp</font>`<font style="color:rgb(0, 0, 0);">ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç¤ºä¾‹ç¨‹åºæ¥è®¡ç®—å›°æƒ‘åº¦ï¼Œè¿™è¯„ä¼°äº†ç»™å®šæ–‡æœ¬å¯¹æ¨¡å‹è€Œè¨€çš„â€œä¸å¯èƒ½â€ç¨‹åº¦ã€‚å®ƒä¸»è¦ç”¨äºæ¯”è¾ƒï¼šå›°æƒ‘åº¦è¶Šä½ï¼Œæ¨¡å‹å¯¹ç»™å®šæ–‡æœ¬çš„è®°å¿†è¶Šå¥½ã€‚</font>

<font style="color:rgb(0, 0, 0);">é¦–å…ˆå‡†å¤‡ä¸€ä¸ªæ•°æ®é›†ï¼š</font>

```python
#ä¸‹è½½wikitextæ•°æ®é›†åˆ°æœ¬åœ°ï¼Œæµ‹è¯•æ¨¡å‹çš„å›°æƒ‘åº¦
from modelscope.msdatasets import MsDataset
ds =  MsDataset.load('modelscope/wikitext', subset_name='wikitext-2-raw-v1', split='test')
```

æ‰§è¡Œå›°æƒ‘åº¦æµ‹è¯•ï¼š

```python
command = [
    "./build/bin/llama-perplexity",
    "-m",
    "./models/qwen2.5-0.5b-instruct-f16-awq.gguf",
    "-f",
    "/home/zhangsh82/.cache/modelscope/hub/datasets/wikitext/wikitext-2-raw-v1/1.0.0/6280e5a53c82b20da4f99f484fa6f0ca9de738ff12f59efb0815fe7d8ae21478/wikitext-test.arrow"
]
subprocess.run(command)
```

æ‰§è¡Œç»“æœï¼š

```python
build: 5038 (193c3e03) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
llama_model_loader: loaded meta data with 26 key-value pairs and 290 tensors from ./models/qwen2.5-0.5b-instruct-f16-awq.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 0.5b Instruct Awq
llama_model_loader: - kv   3:                           general.finetune str              = instruct-awq
llama_model_loader: - kv   4:                           general.basename str              = qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 0.5B
llama_model_loader: - kv   6:                          qwen2.block_count u32              = 24
llama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 896
llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 4864
llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 14
llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 1
llama_model_loader: - type  f32:  121 tensors
llama_model_loader: - type  f16:  169 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 942.43 MiB (16.00 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 896
print_info: n_layer          = 24
print_info: n_head           = 14
print_info: n_head_kv        = 2
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 128
print_info: n_embd_v_gqa     = 128
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 4864
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 494.03 M
print_info: general.name     = Qwen2.5 0.5b Instruct Awq
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   942.43 MiB
..........................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init:        CPU KV buffer size =    24.00 MiB
llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
llama_context:        CPU compute buffer size =   298.50 MiB
llama_context: graph nodes  = 894
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 36 (n_threads_batch = 36) / 72 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 801.231 ms
perplexity: calculating perplexity over 617 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 4.20 seconds per pass - ETA 
10.78 minutes
[5]5.9615,[6]6.0265,[7]5.8214,[8]5.5404,[9]5.8321,[10]6.5176,[11]7.0585,[12]7.4067,[13]7.7750,[14]8.2667,[15]8.7040,[16]9.2678,[17]9.8532,[18]10.3352,[19]10.6466,[20]11.0134,[21]11.6023,[22]11.5693,[23]11.6952,[24]12.0265,[25]11.6275,[26]11.9358,[27]11.9598,[28]12.0548,[29]12.0115,[30]12.1281,[31]11.8333,[32]11.4872,[33]11.4241,[34]11.3175,[35]11.2055,[36]11.1704,[37]11.2962,[38]11.3603,[39]11.3144,[40]11.4045,[41]11.3009,[42]11.4031,[43]11.5521,[44]11.7364,[45]11.9746,[46]11.9983,[47]11.9372,[48]12.0063,[49]12.0339,[50]12.0964,[51]12.1538,[52]12.1331,[53]12.2117,[54]12.2104,[55]12.4167,[56]12.5660,[57]12.4693,[58]12.5439,[59]12.5879,[60]12.6847,[61]12.7410,[62]12.8092,[63]12.8234,[64]12.8911,[65]12.8894,[66]12.9349,[67]13.0027,[68]13.0784,[69]13.1273,[70]13.2364,[71]13.3259,[72]13.4420,[73]13.5751,[74]13.6522,[75]13.7962,[76]13.8127,[77]13.8353,[78]13.8191,[79]13.8744,[80]13.9482,[81]13.9984,[82]14.0143,[83]13.9725,[84]13.9247,[85]13.9665,[86]13.9858,[87]13.9166,[88]13.8996,[89]13.8656,[90]13.9366,[91]13.9381,[92]13.9159,[93]13.9616,[94]14.0552,[95]14.1829,[96]14.1854,[97]14.1802,[98]14.1737,[99]14.2364,[100]14.2166,[101]14.3062,[102]14.3175,[103]14.3026,[104]14.3419,[105]14.3304,[106]14.3507,[107]14.3487,[108]14.4207,[109]14.4448,[110]14.4862,[111]14.5425,[112]14.4938,[113]14.5259,[114]14.4861,[115]14.5348,[116]14.6044,[117]14.6440,[118]14.7402,[119]14.8606,[120]14.9137,[121]14.8772,[122]14.9484,[123]14.9910,[124]14.9310,[125]14.9285,[126]14.9199,[127]14.8495,[128]14.9046,[129]14.8711,[130]14.8763,[131]14.8160,[132]14.7558,[133]14.7399,[134]14.7327,[135]14.7373,[136]14.7011,[137]14.6998,[138]14.6581,[139]14.5846,[140]14.5372,[141]14.5281,[142]14.5350,[143]14.5247,[144]14.5130,[145]14.4881,[146]14.4245,[147]14.4459,[148]14.4495,[149]14.4660,[150]14.4530,[151]14.4758,[152]14.5308,[153]14.5064,[154]14.4155,[155]14.3555,[156]14.2879,[157]14.1557,[158]14.0633,[159]13.9905,[160]13.8855,[161]13.8157,[162]13.7800,[163]13.7305,[164]13.6583,[165]13.5972,[166]13.5305,[167]13.4992,[168]13.4736,[169]13.4459,[170]13.3967,[171]13.3535,[172]13.2901,[173]13.2423,[174]13.2231,[175]13.1599,[176]13.1340,[177]13.1179,[178]13.0915,[179]13.0499,[180]13.0111,[181]13.0027,[182]12.9994,[183]13.0540,[184]13.0694,[185]13.1022,[186]13.1422,[187]13.1911,[188]13.2346,[189]13.2843,[190]13.3210,[191]13.3837,[192]13.4314,[193]13.5077,[194]13.5666,[195]13.5784,[196]13.5802,[197]13.6393,[198]13.6801,[199]13.7119,[200]13.7323,[201]13.7391,[202]13.7858,[203]13.8009,[204]13.8015,[205]13.8341,[206]13.8770,[207]13.9072,[208]13.9132,[209]13.9428,[210]13.9931,[211]14.0337,[212]14.0435,[213]14.0560,[214]14.0319,[215]14.0333,[216]13.9928,[217]13.9682,[218]14.0218,[219]14.0466,[220]14.0566,[221]14.0346,[222]14.0181,[223]14.0061,[224]13.9530,[225]13.9670,[226]13.9680,[227]13.9469,[228]13.9123,[229]13.9220,[230]13.8854,[231]13.8887,[232]13.8514,[233]13.8326,[234]13.8084,[235]13.8011,[236]13.7929,[237]13.7833,[238]13.7785,[239]13.7772,[240]13.7824,[241]13.7660,[242]13.7429,[243]13.7557,[244]13.7658,[245]13.7383,[246]13.7085,[247]13.6944,[248]13.6889,[249]13.6997,[250]13.6959,[251]13.6959,[252]13.7131,[253]13.7091,[254]13.6948,[255]13.7130,[256]13.7270,[257]13.7081,[258]13.6975,[259]13.6966,[260]13.7257,[261]13.7370,[262]13.7494,[263]13.7923,[264]13.8268,[265]13.8484,[266]13.8691,[267]13.8885,[268]13.9075,[269]13.9301,[270]13.9693,[271]13.9944,[272]14.0002,[273]14.0214,[274]13.9976,[275]13.9720,[276]13.9614,[277]13.9585,[278]13.9576,[279]13.9716,[280]13.9853,[281]14.0052,[282]14.0215,[283]14.0103,[284]14.0157,[285]14.0409,[286]14.0469,[287]14.0381,[288]14.0616,[289]14.0661,[290]14.0694,[291]14.0622,[292]14.0683,[293]14.0558,[294]14.0514,[295]14.0469,[296]14.0379,[297]14.0206,[298]14.0256,[299]13.9825,[300]13.9509,[301]13.9016,[302]13.8524,[303]13.8128,[304]13.7916,[305]13.7362,[306]13.6980,[307]13.6590,[308]13.6865,[309]13.6807,[310]13.6614,[311]13.6785,[312]13.6970,[313]13.7411,[314]13.7498,[315]13.7637,[316]13.7767,[317]13.7933,[318]13.8337,[319]13.8584,[320]13.9024,[321]13.9090,[322]13.8962,[323]13.8911,[324]13.8850,[325]13.8892,[326]13.8750,[327]13.8901,[328]13.9072,[329]13.9088,[330]13.9124,[331]13.9040,[332]13.8922,[333]13.8996,[334]13.9167,[335]13.9098,[336]13.9082,[337]13.9200,[338]13.9118,[339]13.9115,[340]13.9356,[341]13.9481,[342]13.9439,[343]13.9658,[344]13.9393,[345]13.9729,[346]13.9866,[347]14.0113,[348]14.0306,[349]14.0444,[350]14.0326,[351]14.0261,[352]14.0076,[353]13.9956,[354]13.9924,[355]13.9817,[356]14.0009,[357]13.9861,[358]13.9732,[359]13.9911,[360]13.9826,[361]14.0026,[362]14.0008,[363]14.0010,[364]13.9904,[365]13.9730,[366]13.9801,[367]13.9783,[368]13.9800,[369]13.9706,[370]13.9592,[371]13.9594,[372]13.9452,[373]13.9476,[374]13.9400,[375]13.9427,[376]13.9379,[377]13.9135,[378]13.9415,[379]13.9540,[380]13.9589,[381]13.9514,[382]13.9523,[383]13.9509,[384]13.9570,[385]13.9667,[386]13.9552,[387]13.9742,[388]14.0030,[389]14.0501,[390]14.0838,[391]14.1272,[392]14.1594,[393]14.1785,[394]14.2106,[395]14.2481,[396]14.2657,[397]14.2816,[398]14.3156,[399]14.3488,[400]14.3686,[401]14.3971,[402]14.4179,[403]14.4399,[404]14.4629,[405]14.4866,[406]14.5081,[407]14.5421,[408]14.5809,[409]14.5948,[410]14.5839,[411]14.5628,[412]14.5723,[413]14.6088,[414]14.6347,[415]14.6425,[416]14.6552,[417]14.6413,[418]14.6479,[419]14.6598,[420]14.6723,[421]14.6835,[422]14.6857,[423]14.7002,[424]14.7042,[425]14.7044,[426]14.6859,[427]14.6837,[428]14.6688,[429]14.6556,[430]14.6553,[431]14.6636,[432]14.6686,[433]14.6656,[434]14.6884,[435]14.6938,[436]14.6997,[437]14.7014,[438]14.6983,[439]14.6931,[440]14.7063,[441]14.7167,[442]14.7197,[443]14.7134,[444]14.7170,[445]14.6975,[446]14.7126,[447]14.7323,[448]14.7366,[449]14.7458,[450]14.7331,[451]14.7059,[452]14.6523,[453]14.6072,[454]14.5715,[455]14.5366,[456]14.5064,[457]14.4780,[458]14.5160,[459]14.5329,[460]14.5487,[461]14.5382,[462]14.5379,[463]14.5365,[464]14.5133,[465]14.5237,[466]14.5322,[467]14.5276,[468]14.5442,[469]14.5498,[470]14.5429,[471]14.5624,[472]14.5814,[473]14.5851,[474]14.5719,[475]14.5751,[476]14.5668,[477]14.5828,[478]14.5962,[479]14.5979,[480]14.5828,[481]14.5949,[482]14.5866,[483]14.5828,[484]14.5765,[485]14.5652,[486]14.5717,[487]14.5794,[488]14.5840,[489]14.6042,[490]14.5891,[491]14.5849,[492]14.5915,[493]14.6061,[494]14.6222,[495]14.6317,[496]14.6421,[497]14.6459,[498]14.6599,[499]14.6698,[500]14.6779,[501]14.6807,[502]14.6708,[503]14.6641,[504]14.6444,[505]14.6563,[506]14.6678,[507]14.6514,[508]14.6570,[509]14.6468,[510]14.6414,[511]14.6665,[512]14.6744,[513]14.6617,[514]14.6534,[515]14.6591,[516]14.6584,[517]14.6410,[518]14.6323,[519]14.6222,[520]14.6217,[521]14.6166,[522]14.6033,[523]14.6048,[524]14.5858,[525]14.5748,[526]14.5697,[527]14.5626,[528]14.5640,[529]14.5781,[530]14.5732,[531]14.5797,[532]14.5739,[533]14.5881,[534]14.6143,[535]14.6201,[536]14.6303,[537]14.6344,[538]14.6300,[539]14.6360,[540]14.6384,[541]14.6340,[542]14.6432,[543]14.6369,[544]14.6321,[545]14.6336,[546]14.6358,[547]14.6371,[548]14.6134,[549]14.5999,[550]14.5975,[551]14.5924,[552]14.5858,[553]14.5750,[554]14.5769,[555]14.5877,[556]14.6018,[557]14.5909,[558]14.5870,[559]14.5832,[560]14.5790,[561]14.5708,[562]14.5772,[563]14.5622,[564]14.5627,[565]14.5613,[566]14.5586,[567]14.5453,[568]14.5656,[569]14.5725,[570]14.5780,[571]14.5936,[572]14.5891,[573]14.5751,[574]14.5511,[575]14.5312,[576]14.5030,[577]14.5135,[578]14.5165,[579]14.5273,[580]14.5171,[581]14.5029,[582]14.4779,[583]14.4908,[584]14.4693,[585]14.4508,[586]14.4314,[587]14.4012,[588]14.4076,[589]14.4136,[590]14.4150,[591]14.4167,[592]14.4277,[593]14.4424,[594]14.4568,[595]14.4577,[596]14.4811,[597]14.4948,[598]14.5026,[599]14.5135,[600]14.5188,[601]14.5047,[602]14.5064,[603]14.4983,[604]14.5069,[605]14.5084,[606]14.5251,[607]14.5310,[608]14.5301,[609]14.5321,[610]14.5287,[611]14.5274,[612]14.5359,[613]14.5557,[614]14.5695,[615]14.5899,[616]14.6012,[617]14.6109,

Final estimate: PPL = 14.6109 +/- 0.10800 #æœ€ç»ˆçš„å›°æƒ‘åº¦
llama_perf_context_print:        load time =     381.43 ms
llama_perf_context_print: prompt eval time =  464665.94 ms / 315904 tokens (    1.47 ms per token,   679.85 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =  471321.77 ms / 315905 tokens
```

## 6.éƒ¨ç½²
### 1.ç¦»çº¿æ‰¹é‡æ¨ç†
```python
#ç¦»çº¿æ‰¹é‡å¤„ç†
# æ³¨æ„ï¼šV100æ˜¾å¡ä¸æ”¯æŒä½¿ç”¨vllmç›´æ¥éƒ¨ç½²awqé‡åŒ–ç‰ˆæœ¬é¢
import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="7"
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct")

# Pass the default decoding hyperparameters of Qwen2.5-0.5B-Instruct
# max_tokens is for the maximum length for generation.
sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)

# Input the model name or path. Can be GPTQ or AWQ models.
llm = LLM(model="/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct",dtype = 'float16', quantization="awq")

# Prepare your prompts
prompt = "Tell me something about large language models."
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# generate outputs
outputs = llm.generate([text], sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

### 2.åŸºäºopenaiçš„å½¢å¼å¯åŠ¨ä¸€ä¸ªæœåŠ¡
```python
!CUDA_VISIBLE_DEVICES=2,3 vllm serve /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct --dtype 'float16'
```

#### --è¯·æ±‚è¿™ä¸ªæœåŠ¡ï¼š
```python
!curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{
  "model": "/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct",
  "messages": [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": "Tell me something about large language models."}
  ],
  "temperature": 0.7,
  "top_p": 0.8,
  "repetition_penalty": 1.05,
  "max_tokens": 512
}'
```

pythonä»£ç ï¼š

```python
from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

chat_response = client.chat.completions.create(
    model="/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct",
    messages=[
        {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
        {"role": "user", "content": "Tell me something about large language models."},
    ],
    temperature=0.7,
    top_p=0.8,
    max_tokens=512,
    extra_body={
        "repetition_penalty": 1.05,
    },
)
print("Chat response:", chat_response)
```

### 3.åˆ†éƒ¨ç½²éƒ¨ç½²
```python
#åˆ†å¸ƒå¼éƒ¨ç½²ï¼Œå¤šå¡å¹¶è¡Œï¼ˆå¼ é‡å¹¶è¡Œï¼‰ --tensor-parallel-sizeå‚æ•°
!CUDA_VISIBLE_DEVICES=2,3 vllm serve /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct --dtype 'float16' --tensor-parallel-size 2
```

## 7.æœ‰ç›‘ç£å¾®è°ƒ
æœ¬ç« èŠ‚è®­ç»ƒå¤§æ¨¡å‹æš‚æ—¶åŸºäºLLaMA-Factoryæ¡†æ¶

ä½¿ç”¨SFTåŠ loraæ–¹å¼è®­ç»ƒä¸€ä¸ªchat-å¬›å¬›ï¼Œä½¿ç”¨å…¬å¼€çš„ç”„å¬›æ•°æ®é›†

1.é…ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ

```python
!pip install deepspeed
!pip install flash-attn --no-build-isolation
```

2.ä¸‹è½½llama factoryä»£ç 

```python
!pip install -e "/home/zhangsh82/data/zsh/LLaMA-Factory"
```

```python
!llamafactory-cli version
```

3.è®­ç»ƒå¬›å¬›

æ•°æ®é›†æ¥æºï¼š[https://github.com/datawhalechina/self-llm.git](https://github.com/datawhalechina/self-llm.git)

```python
%%bash

#!/bin/bash
export CUDA_VISIBLE_DEVICES=7

# è®¾ç½®èŠ‚ç‚¹æ•°ä¸º1
export NNODES=1
# è®¾ç½®æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„GPUæ•°é‡ä¸º1
export GPUS_PER_NODE=1
# è®¾ç½®å½“å‰èŠ‚ç‚¹çš„ç­‰çº§ä¸º0
export NODE_RANK=0
# è®¾ç½®ä¸»èŠ‚ç‚¹çš„åœ°å€ä¸ºæœ¬åœ°IP
export MASTER_ADDR=localhost
# è®¾ç½®ä¸»èŠ‚ç‚¹çš„ç«¯å£ä¸º1234
export MASTER_PORT=5555

MODEL='/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/'
OUTPUT_PATH='/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan'
DATA_SET='zhenhuan'
DS_CONFIG_PATH='/home/zhangsh82/data/zsh/LLaMA-Factory/examples/deepspeed/ds_z3_config.json'


DISTRIBUTED_ARGS="
    --nproc_per_node $GPUS_PER_NODE \
    --nnodes $NNODES \
    --node_rank $NODE_RANK \
    --master_addr $MASTER_ADDR \
    --master_port $MASTER_PORT
"

torchrun $DISTRIBUTED_ARGS src/train.py \
    --deepspeed $DS_CONFIG_PATH \
    --stage sft \
    --do_train \
    --do_eval \
    --eval_strategy steps \
    --eval_steps 200 \
    --val_size 0.01 \
    --logging_dir /home/zhangsh82/data/zsh/Qwen-Learn \
    --use_fast_tokenizer \
    --flash_attn disabled \
    --model_name_or_path $MODEL \
    --dataset $DATA_SET \
    --template qwen \
    --finetuning_type lora \
    --lora_target q_proj,v_proj\
    --output_dir $OUTPUT_PATH \
    --overwrite_cache \
    --overwrite_output_dir \
    --warmup_ratio 0.01 \
    --weight_decay 0.1 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --ddp_timeout 9000 \
    --learning_rate 1e-5 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --cutoff_len 4096 \
    --save_steps 200 \
    --plot_loss True\
    --num_train_epochs 3
```

**å‚æ•°è®¾ç½®æ³¨æ„äº‹é¡¹ï¼š**

--deepspeed è®¾ç½®deepseekçš„é…ç½®æ–‡ä»¶ï¼Œexampleï¼š

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1744945417453-9fe46d0a-63a2-48c1-afa9-2c90641179ac.png)

--stage è®­ç»ƒæ–¹å¼æŒ‡å®šä¸ºsftæŒ‡ä»¤å¾®è°ƒ

---

ä¸‹é¢ä¸€ç»„å‚æ•°ç”¨äºè®¾ç½®è®­ç»ƒè¿‡ç¨‹ä¸­çš„éªŒè¯ä¿¡æ¯

 --do_eval \

 --eval_strategy steps \

 --eval_steps 200 \	#æ¯200æ­¥éªŒè¯ä¸€æ¬¡

 --val_size 0.01 \    #ä»è®­ç»ƒé›†ä¸­æ‹†å‡º0.01ä½œä¸ºéªŒè¯é›†

---

--dataset è®¾ç½®æ•°æ®é›†ï¼Œæ³¨æ„ä¸åŒçš„è®­ç»ƒé˜¶æ®µï¼Œæ•°æ®é›†çš„æ ¼å¼ä¸åŒï¼Œæœ¬æ¬¡åŸºäºsftå¾®è°ƒï¼Œè®­ç»ƒé›†ç¤ºä¾‹ï¼š

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1744945749468-9bd5c036-23a8-4ff3-bdf8-5d0d03b6b912.png)

æ•°æ®é›†å¤§å°3729æ¡ï¼Œæ­£å¸¸å¾®è°ƒå‚åŸŸå°åœºæ™¯ï¼Œè¿™ä¸ªæ•°æ®é‡å·®ä¸å¤šå¤Ÿäº†

---

--finetuning_type lora \

--lora_target q_proj,v_proj\

è®¾ç½®ä¸ºloraæ–¹å¼å¾®è°ƒï¼Œè¿˜å¯ä»¥ç»§ç»­è®¾ç½®lora-rankï¼ˆé»˜è®¤8ï¼‰å’Œalphaå¤§å°ï¼Œå¹³è¡¡è®­ç»ƒæ•ˆæœå’Œè®­ç»ƒæˆæœ¬

---

--warmup_ratio

 å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹   åœ¨æœ€å¼€å§‹çš„ 1% stepsï¼Œå­¦ä¹ ç‡çº¿æ€§ä» 0 â†’ å­¦ä¹ ç‡å³°å€¼ï¼Œé˜²æ­¢åˆšå¼€å§‹æ¢¯åº¦çˆ†ç‚¸ã€‚  

 --weight_decay  

 æƒé‡è¡°å‡   é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒæŠŠå‚æ•°é€æ¸å¾€ 0 æ‹‰ï¼ˆç±»ä¼¼L2æ­£åˆ™åŒ–ï¼‰ï¼Œè¿™é‡Œè®¾ç½® 0.1ï¼Œåé«˜ï¼Œé€‚åˆå¤§æ¨¡å‹ã€‚  

 --per_device_train_batch_size 4  

 å•å¡æ¯æ¬¡è®­ç»ƒ 4ä¸ªæ ·æœ¬ï¼Œå¦‚æœå¤šå¡ï¼Œè¿˜è¦ä¹˜è®¾å¤‡æ•°é‡ã€‚  

 --gradient_accumulation_steps 4  

 æ¯è®­ç»ƒ 4 ä¸ª batch æ‰çœŸæ­£åå‘ä¼ æ’­ä¸€æ¬¡ï¼Œç›¸å½“äº**æœ‰æ•ˆ batch_size = 4 Ã— 4 = 16**ï¼Œæ˜¾å­˜ä¸å¤Ÿæ—¶å¸¸ç”¨ã€‚  

 --ddp_timeout 9000  

åˆ†å¸ƒå¼è®­ç»ƒè¶…æ—¶æ—¶é—´

 --cutoff_len 4096  

 æ–‡æœ¬è¶…è¿‡ 4096 token ä¼šè¢«æˆªæ–­ï¼Œæ§åˆ¶æ˜¾å­˜æ¶ˆè€—ã€‚  

 --save_steps 200  

 æ¯ 200æ­¥ä¿å­˜ä¸€æ¬¡ checkpointï¼Œé˜²æ­¢ä¸­æ–­å¯¼è‡´å…¨æŒ‚ã€‚  

è®­ç»ƒlossï¼š

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1744948101858-d6529350-8501-4c32-a5ad-69d7f4c99b05.png)

---

è®­ç»ƒå®Œæ¯•åï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨å¾®è°ƒåçš„loraæ¨¡å‹ï¼Œä¹Ÿå¯ä»¥å¾®è°ƒåçš„æ¨¡å‹æƒé‡åˆå¹¶åˆ°åŸæ¨¡å‹å†…

åˆå¹¶æ“ä½œï¼š

```python
CUDA_VISIBLE_DEVICES=7 llamafactory-cli export \
    --model_name_or_path /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/ \
    --adapter_name_or_path /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan \
    --template qwen \
    --finetuning_type lora \
    --export_dir /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan-lora-merge \
    --export_size 2 \
    --export_legacy_format False
```

## 8.æ¨¡å‹æ•ˆæœæ¯”è¾ƒ
1.ç²¾åº¦æ¯”è¾ƒ

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1745225053048-14f9c1cc-b06b-4907-affd-06fb731d1abb.png)

2.é€Ÿåº¦æ¯”è¾ƒ

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1745225115873-33f978d5-6045-4f36-9a82-3861bcdb9e13.png)

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1745225129292-b531ce1c-5313-4939-8cda-d56c92ef5c0a.png)

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1745225141831-76e67636-51aa-45b5-82cf-59df098598d8.png)

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1745225153632-f41a3a7e-512d-440a-80fb-9bb76bf56ed7.png)

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1745225165746-01f04225-7ac2-410a-baae-f9d4757ae579.png)

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1745225176841-ec4295cc-6bbf-4988-b0b9-02a6caf4a131.png)

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1745225188510-d9a6d792-6279-451f-936b-c0a5a2a9a8ee.png)

![](https://cdn.nlark.com/yuque/0/2025/png/38450811/1745225219994-7557ad59-57e2-49f4-b45a-adc633efb6f9.png)

æ›´å¤šè¯·å‚è€ƒï¼š

[https://qwen.readthedocs.io/zh-cn/latest/benchmark/speed_benchmark.html](https://qwen.readthedocs.io/zh-cn/latest/benchmark/speed_benchmark.html)

